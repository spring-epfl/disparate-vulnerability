import warnings

import numpy as np
import pandas as pd


def get_num_classes(targets):
    """Infer the number of classes from the one-hot target vector.

    >>> get_num_classes(np.array([1, 0, 1]))
    2
    >>> get_num_classes(np.array([[0, 1, 0, 0], [0, 0, 1, 0]]))
    4
    """
    return 2 if len(targets.shape) == 1 else targets.shape[1]


def get_classes_from_confidence(y, threshold=0.5):
    """Get the predicted class from a confidence vector.

    >>> get_classes_from_confidence(np.array([0.4, 0.2, 0.6]))
    array([0, 0, 1])
    >>> get_classes_from_confidence(np.array([[0.2, 0.5, 0.3], [0.7, 0.2, 0.1]]))
    array([1, 0])
    """
    if get_num_classes(y) == 2:
        return (y > 0.5).astype(np.int64)
    else:
        return y.argmax(axis=1)


def get_one_hot_from_confidence(y, threshold=0.5):
    """Get the predicted class, one-hot encoded, from a confidence vector.

    >>> get_one_hot_from_confidence(np.array([0.4, 0.2, 0.6]))
    array([0, 0, 1])
    >>> get_one_hot_from_confidence(np.array([[0.2, 0.5, 0.3], [0.7, 0.2, 0.1]]))
    array([[0., 1., 0.],
           [1., 0., 0.]])
    """
    if get_num_classes(y) == 2:
        return (y > 0.5).astype(np.int64)
    else:
        result = np.zeros(y.shape)
        result[np.arange(len(y)), y.argmax(axis=1)] = 1.
        return result


def universal_slice(data, x_idxs):
    """
    Slice by index of either a pandas DataFrame, or a numpy array.

    >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})
    >>> universal_slice(df, [1]).values
    array([[2, 5]])
    """
    if isinstance(data, np.ndarray):
        if len(data.shape) == 2:
            return data[x_idxs, :]
        elif len(data.shape) == 1:
            return data[x_idxs]
    elif isinstance(data, pd.DataFrame):
        if len(data.shape) == 2:
            return data.iloc[x_idxs, :]
        elif len(data.shape) == 1:
            return data.iloc[x_idxs]
    elif isinstance(data, pd.Series):
        return data.iloc[x_idxs]


def universal_col_slice(data, y_idxs=None):
    """
    Slice by column index of either a pandas DataFrame, or a numpy array.

    >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})
    >>> universal_col_slice(df, 1).values
    array([4, 5, 6])
    """
    if isinstance(data, np.ndarray):
        if y_idxs is not None:
            return data[:, y_idxs]
    elif isinstance(data, pd.DataFrame):
        if y_idxs is not None:
            return data.iloc[:, y_idxs]


def attack_data_to_matrix(data):
    """
    Stack the true classes and target model predictions into one matrix.

    Args:
        data: A tuple or list ``[y_true, y_pred]``

    >>> y_true = np.array([0,     1,   0])
    >>> y_pred = np.array([0.1, 0.3, 0.4])
    >>> attack_data_to_matrix([y_true, y_pred])
    array([[0. , 0.1],
           [1. , 0.3],
           [0. , 0.4]])
    """
    y_true, y_pred = data
    if len(y_true.shape) == len(y_pred.shape) == 1:
        return np.column_stack([y_true, y_pred])
    else:
        return np.hstack([y_true, y_pred])


def subsample_data(X, y, size, seed=None):
    """
    Subsample a given part of a dataset.

    Returns:
        (X_subsample, y_subsample, X_rest, y_rest)
    """
    all_idxs = np.arange(len(X))
    random_state = np.random.RandomState(seed)
    subsample_idxs = random_state.choice(all_idxs, size=size, replace=False)
    rest_idxs = np.setdiff1d(all_idxs, subsample_idxs)
    X_subsample = universal_slice(X, subsample_idxs)
    y_subsample = universal_slice(y, subsample_idxs)

    X_rest = universal_slice(X, rest_idxs)
    y_rest = universal_slice(y, rest_idxs)
    return X_subsample, y_subsample, X_rest, y_rest


def make_attack_part(model, X, y, label=1):
    """
    Construct a part of the MIA attack data with a given in/out label.

    Args:
        model: Estimator-like object
        X: data array or DataFrame
        y: labels array or DataFrame, one-hot encoded
        label: in/out label
    """
    preds = model.predict_proba(X)
    if preds.shape[0] != 1:
        preds = preds.squeeze()
    if y.shape[0] != 1:
        y = y.squeeze()

    # Simplify sklearn-style binary confidence vectors.
    if len(preds.shape) == 2 and preds.shape[1] == 2:
        preds = preds[:, 1]

    labels = np.array([label] * len(preds))
    return [y, preds], labels


def make_attack_data(model, X_train, X_test, y_train, y_test):
    """
    Construct a MIA attack dataset.
    """
    [ins_y_true, ins_y_pred], in_labels = make_attack_part(
        model, X_train, y_train, label=1
    )
    [outs_y_true, outs_y_pred], out_labels = make_attack_part(
        model, X_test, y_test, label=0
    )
    if len(ins_y_true.shape) == len(outs_y_true.shape) == 1:
        stacked_y_true = np.concatenate([ins_y_true, outs_y_true])
        stacked_y_pred = np.concatenate([ins_y_pred, outs_y_pred])
    else:
        stacked_y_true = np.vstack([ins_y_true, outs_y_true])
        stacked_y_pred = np.vstack([ins_y_pred, outs_y_pred])
    attacker_y = np.concatenate([in_labels, out_labels]).squeeze()
    return [stacked_y_true, stacked_y_pred], attacker_y


def run_mia(target_model, attacker, X, y):
    """
    Get MIA predictions and some additional info on a dataset.

    Returns:
        dict, where
        - ``attack_pred`` holds the attacker's predictions
        - ``target_y_true`` the true classes, one-hot encoded
        - ``target_y_pred`` the classes as predicted by the target, one-hot encoded
        - ``target_y_proba`` the class probabilities as predicted by the target
    """
    attacker_X, _ = make_attack_part(target_model, X, y, label=None)
    [y_true, y_pred] = attacker_X
    preds = attacker.predict(attack_data_to_matrix(attacker_X))
    results = {
        "attack_pred": preds.astype(np.float32),
        "target_y_true": y_true.astype(np.float32),
        "target_y_pred": get_one_hot_from_confidence(y_pred).astype(np.float32),
        "target_y_proba": y_pred,
    }
    return results


def _get_max_conf_rate(y_pred, bins):
    """Get max confidence histogram bins."""
    if len(y_pred.shape) == 1:
        y_pred = np.expand_dims(y_pred, 1)

    num_cols = y_pred.shape[1]
    results = np.zeros((bins, num_cols))
    for j in range(num_cols):
        hist, _ = np.histogram(y_pred[:, j], range=(0, 1), density=False, bins=bins)
        props = hist / hist.sum()
        results[:, j] = props
    return results


def _nan_to_zero(arr):
    try:
        arr[np.isnan(arr)] = 0
        return arr
    except TypeError:
        # Assume arr is a single object, not array.
        if np.isnan(arr):
            return 0.
        else:
            return arr


def get_mia_global_loss(ins_y_pred, outs_y_pred, mode="avg", bins=100):
    """Compute MIA privacy loss.

    Let $M$ be a random variable denoting membership, $P$ the prediction, and $Y$
    the true class of a given sample.


    If mode is "avg", the loss is the following expression:
    \[
        \sup_{m, m'} \log \frac{E[P | Y=y, M=m]}{E[P | Y=y, M=m']}
    \]

    If mode is "prob", the loss, theoretically, should be the following expression:
    \[
        \sup_{p, m, m'} \log \frac{\Pr[P=p | Y=y, M=m]}{\Pr[P=p | Y=y, M=m']}
    \]
    Empirically, this probability is computed by binning the space of predictions into a number of
    bins uniformly from 0 to 1, computing the histograms of predictions for in and out respectively.
    This is numerically unstable and many values are likely to be infinite.

    Args:
        ins_y_pred: Predictions for some examples in training
        outs_y_pred: Predictions for some examples not in training
        mode: Mode, one of ["avg", "prob"]
        bins: If mode is "prob", the number of bins.

    Returns:
        Array with shape (num_classes,)
    """

    if mode == "prob":
        ins_hists = np.log(_get_max_conf_rate(ins_y_pred, bins=bins))
        outs_hists = np.log(_get_max_conf_rate(outs_y_pred, bins=bins))

        # Handle infs and NaNs.
        nan_mask = np.isfinite(ins_hists) & np.isfinite(outs_hists)
        ins_hists[~nan_mask] = 0.
        outs_hists[~nan_mask] = 0.
        log_ratio = np.abs(ins_hists - outs_hists)

        return log_ratio.max(axis=0).squeeze()

    elif mode == "avg":
        ins_avg = np.log(_nan_to_zero(ins_y_pred.mean(axis=0)))
        outs_avg = np.log(_nan_to_zero(outs_y_pred.mean(axis=0)))
        log_ratio1 = np.abs(_nan_to_zero(ins_avg - outs_avg))
        return log_ratio1.squeeze()


def get_mia_loss_by_class(
    ins_y_true, outs_y_true, ins_y_pred, outs_y_pred, mode="avg", bins=10, warn=False
):
    if len(ins_y_true.shape) == 1:
        ins_y_true = np.expand_dim(ins_y_true, 1)
    if len(outs_y_true.shape) == 1:
        outs_y_true = np.expand_dim(outs_y_true, 1)

    num_classes = max(get_num_classes(ins_y_true), get_num_classes(outs_y_true))
    losses_by_class_pair = np.zeros((num_classes, num_classes))
    for i in range(num_classes):
        for j in range(num_classes):
            # if i != j:
            #     continue
            ins_mask = universal_col_slice(ins_y_true, j) == 1
            outs_mask = universal_col_slice(outs_y_true, j) == 1
            if outs_mask.sum() == 0 or ins_mask.sum() == 0:
                if warn:
                    warnings.warn("One of the classes has no samples.")
                losses_by_class_pair[i, j] = 0.
            else:
                losses_by_class_pair[i, j] = get_mia_global_loss(
                    ins_y_pred[ins_mask, i],
                    outs_y_pred[outs_mask, i],
                    mode=mode,
                    bins=bins,
                )
    return np.array(losses_by_class_pair)
