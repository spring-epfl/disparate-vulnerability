import numpy as np
import pandas as pd
import warnings

from sklearn import datasets as sklearn_datasets
from sklearn.preprocessing import StandardScaler


def create_adult_age_df(
        train_data, test_data, num_classes=10, quantiles=5, binarize=True
):
    df = pd.concat([train_data, test_data], ignore_index=True)
    df["income"] = df["income"].map({"<=50K": 0, ">50K": 1})

    num_cols = ["hours-per-week", "capital-loss", "capital-gain"]

    cat_cols = [
        "work-class",
        "marital-status",
        "occupation",
        "relationship",
        "race",
        "sex",
        "native-country",
    ]
    quant_cols = []
    target_col = "age"

    if binarize:
        cat_cols.append("education")
        num_cols.remove("capital-loss")
        num_cols.remove("capital-gain")
        quant_cols.extend(num_cols)
        num_cols = []
    else:
        num_cols.append("education-num")
        scaler = StandardScaler().fit(df[num_cols])
        scaled = pd.DataFrame(scaler.transform(df[num_cols]), columns=num_cols)

    one_hot_encoded = pd.get_dummies(df[cat_cols])

    quant_encoded_parts = []
    for quant_col in quant_cols:
        try:
            bins = pd.qcut(df[quant_col], quantiles)

        except ValueError:
            bins = pd.qcut(df[quant_col], quantiles, duplicates="drop")
            num_bins = len(bins.unique())
            warnings.warn(
                "%s will have fewer bins than specified: %d" % (quant_col, num_bins)
            )

        quant_encoded_parts.append(pd.get_dummies(bins, prefix=quant_col))

    quant_encoded_parts.append(
        pd.get_dummies(pd.qcut(df[target_col], num_classes), prefix=target_col)
    )

    parts = [one_hot_encoded] + quant_encoded_parts
    if not binarize:
        parts.append(scaled)

    return pd.concat(parts, axis=1)


def create_adult_income_df(
        train_data, test_data, binarize=False):
    if type(train_data) == str:
        train_data = pd.read_csv(train_data)
        test_data = pd.read_csv(test_data)

    df = pd.concat([train_data, test_data], ignore_index=True)
    df["income"] = df["income"].map({"<=50K": 0, ">50K": 1})

    num_cols = ["hours-per-week", "capital-loss", "capital-gain"]

    cat_cols = [
        "work-class",
        "marital-status",
        "occupation",
        "relationship",
        "race",
        "sex",
        "native-country",
    ]

    if binarize:
        cat_cols.append("education")
        num_cols.remove("capital-loss")
        num_cols.remove("capital-gain")
    else:
        num_cols.append("education-num")
        scaler = StandardScaler().fit(df[num_cols])
        df[num_cols] = pd.DataFrame(scaler.transform(df[num_cols]), columns=num_cols)

    # one_hot_encoded = pd.get_dummies(df[cat_cols])
    #
    # quant_encoded_parts = []
    # for quant_col in quant_cols:
    #     try:
    #         bins = pd.qcut(df[quant_col], quantiles)
    #
    #     except ValueError:
    #         bins = pd.qcut(df[quant_col], quantiles, duplicates="drop")
    #         num_bins = len(bins.unique())
    #         warnings.warn(
    #             "%s will have fewer bins than specified: %d" % (quant_col, num_bins)
    #         )
    #
    #     quant_encoded_parts.append(pd.get_dummies(bins, prefix=quant_col))
    #
    # quant_encoded_parts.append(
    #     pd.get_dummies(pd.qcut(df[target_col], num_classes), prefix=target_col)
    # )
    #
    # parts = [one_hot_encoded] + quant_encoded_parts
    # if not binarize:a
    #     parts.append(scaled)

    return df[cat_cols + num_cols + ["income"]]


def customize_adult_dataset(
        train_data, test_data, target="age", binarize=True, quantiles=5, num_classes=None
):
    """
    Create a version of the ADULT dataset with modified targets and pre-processing.

    The following options are supported.
    For the targets:
    - Income. Use the standard binary target (income is over 50K or not)
    - Age. Use quantized age as the target. By default, 10 classes are used.

    For feature types:
    - Binary. All features are one-hot encoded.
    - Mixed. Categorical features are one-hot encoded, numerical features are scaled.

    Args:
        train_data
        test_data
        target: Target.
            One of ["income", "age"]
        binarize: Whether to use all-binarized features.
        quantiles: Number of buckets for quantizing numberical features.
            Used if ``features`` option is "binary"
        num_classes: Number of classes.
            Used if target is "age"

    Returns:
        Numpy arrays ``(X, y)``
    """
    if target == "age":
        df = create_adult_age_df(
            train_data,
            test_data,
            num_classes=num_classes,
            binarize=binarize,
            quantiles=quantiles,
        )

    elif target == "income":
        df = create_adult_income_df(
            train_data, test_data, binarize=binarize, quantiles=quantiles
        )

    data_cols = [col for col in df.columns if not col.startswith(target)]
    target_cols = np.array(
        [col for col in df.columns if col.startswith(target)]
    ).squeeze()

    X, y = df[data_cols], df[target_cols].squeeze()
    X = X.astype(np.float32)
    y = y.astype(np.float32)
    return X, y


def generate_sensitive_data(
        sensitive_group_dict, n_features, n_samples_per_sensitive_group, n_classes, **kwargs
):
    groups = sensitive_group_dict.keys()
    mus = [sensitive_group_dict[group]["mu"] for group in groups]
    sigmas = [sensitive_group_dict[group]["sigma"] for group in groups]

    if np.array([len(mu) != n_features for mu in mus]).astype(int).sum() != 0:
        raise Exception("mu's should be the same size as n_features.")

    X, z = sklearn_datasets.make_blobs(
        n_samples=n_samples_per_sensitive_group * len(groups),
        n_features=n_features,
        centers=np.array(mus),
        cluster_std=tuple(sigmas),
        **kwargs
    )

    last_column = n_features
    data = pd.DataFrame(np.c_[X, z]).rename({last_column: "z"}, axis=1).copy(deep=True)
    print(data.columns)
    data["z"] = data["z"].astype(int)
    z2p1_dict = {group: sensitive_group_dict[group]["p1"] for group in groups}
    print(z2p1_dict)
    z2m_dict = {group: sensitive_group_dict[group]["m"] for group in groups}
    print(z2m_dict)

    data["y"] = data["z"].apply(lambda z: np.random.choice(n_classes, p=z2p1_dict[z]))
    data["m"] = data["z"].apply(lambda z: np.random.choice(2, p=z2m_dict[z]))
    return data


def create_non_disparate_df(num_classes):
    """
    Generate Non-Disparate synthetic dataset.
    """
    p_m1 = 0.6
    return generate_sensitive_data(
        sensitive_group_dict={
            0: dict(sigma=2, mu=[0] * 10, p1=None, m=[1 - p_m1, p_m1]),
            1: dict(sigma=2, mu=[0] * 10, p1=None, m=[1 - p_m1, p_m1]),
            2: dict(sigma=2, mu=[0] * 10, p1=None, m=[1 - p_m1, p_m1]),
            3: dict(sigma=2, mu=[0] * 10, p1=None, m=[1 - p_m1, p_m1]),
            4: dict(sigma=2, mu=[0] * 10, p1=None, m=[1 - p_m1, p_m1]),
        },
        n_features=10,
        n_samples_per_sensitive_group=10000,
        n_classes=num_classes,
        shuffle=False,
    )


def create_disparate_df(num_classes):
    """
    Generate Disparate synthetic dataset.
    """
    p_m1 = 0.6
    return generate_sensitive_data(
        sensitive_group_dict={
            0: dict(sigma=2, mu=[0] * 10, p1=None, m=[1 - p_m1, p_m1]),
            1: dict(sigma=2, mu=[0] * 10, p1=None, m=[1 - p_m1, p_m1]),
            2: dict(sigma=2, mu=[0] * 10, p1=None, m=[0.3, 0.7]),
            3: dict(sigma=2, mu=[0] * 10, p1=None, m=[1 - p_m1, p_m1]),
            4: dict(sigma=2, mu=[0] * 10, p1=None, m=[1 - p_m1, p_m1]),
        },
        n_features=10,
        n_samples_per_sensitive_group=10000,
        n_classes=num_classes,
        shuffle=False,
    )


def generate_synthetic_dataset(num_classes=10, kind="non-disparate"):
    """
    Generate a synthetic dataset.

    Args:
        kind: one of ["non-disparate", "disparate"]

    Numpy arrays ``(X_train, y_train, X_test, y_test)``
    """
    if kind == "non-disparate":
        data = create_non_disparate_df(num_classes=num_classes)

    elif kind == "disparate":
        data = create_disparate_df(num_classes=num_classes)

    X = data.drop(["y"], axis=1)
    X.columns = X.columns.astype(str)
    X = X.join(pd.get_dummies(X["z"], prefix="z")).drop("z", axis=1)

    y = pd.get_dummies(data["y"])
    y.columns = y.columns.astype(str)

    X_train = X[X.m == 1]
    X_test = X[X.m == 0]
    y_train = y[X.m == 1]
    y_test = y[X.m == 0]

    return X_train, y_train, X_test, y_test
