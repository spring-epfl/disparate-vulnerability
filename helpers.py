from functools import reduce
from operator import xor
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics.ranking import _binary_clf_curve
from sklearn.model_selection import StratifiedKFold
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.utils import resample
from tqdm import tqdm_notebook


def encode_dataframe(df, output_columns=None, initially_drop_columns=None, finally_drop_columns=None, drop_first=True,
                     add_non_dummified_columns=None, return_output_columns=False):
    if initially_drop_columns is not None:
        df = df.drop(initially_drop_columns, axis=1)

    df_ = pd.get_dummies(df, columns=[col for col in df.columns if col != "strat"
                                      and not pd.api.types.is_numeric_dtype(df.dtypes[col])],
                         drop_first=drop_first)

    if add_non_dummified_columns is not None:
        keep_cols = df[add_non_dummified_columns]
        df_ = pd.concat([df_, keep_cols], axis=1)

    if output_columns is not None:
        df_ = df_.reindex(columns=output_columns, fill_value=0)

    if finally_drop_columns is not None:
        df_ = df_.drop(finally_drop_columns, axis=1)

    if return_output_columns:
        return df_, df_.columns.tolist()
    else:
        return df_


def assign_y(p_ser, thr):
    return (p_ser > thr).astype(int)


def calculate_measures(trn_df, tst_df, evl_df):
    thr2dfs = {thr: {} for thr in np.arange(0.01, 0.99, 0.01)}
    for thr in thr2dfs.keys():
        train_df = trn_df.copy()
        test_df = tst_df.copy()
        eval_df = evl_df.copy()
        train_df.loc[:, "yhat"] = assign_y(train_df["p"], thr)
        test_df.loc[:, "yhat"] = assign_y(test_df["p"], thr)
        eval_df.loc[:, "yhat"] = assign_y(eval_df["p"], thr)
        thr2dfs[thr]["train_df"] = train_df
        thr2dfs[thr]["test_df"] = test_df
        thr2dfs[thr]["eval_df"] = eval_df

    return thr2dfs


def prob_operator(y_label, y_value, yhat_value, z_label, z_value, df, event, condition=None, m_value=None):
    def count(df, predicates, ):
        p_list = predicates.split(" ")
        df_ = df.copy()
        for pred in p_list:
            if pred == "yhat":
                df_ = df_[df_["yhat"] == yhat_value].copy()
            elif pred == 'y':
                df_ = df_[df_[y_label] == y_value].copy()
            elif pred == "z":
                df_ = df_[df_[z_label] == z_value].copy()

            elif pred == "m":
                if m_value is not None:
                    df_ = df_[df_['m'] == m_value].copy()
                else:
                    raise Exception("m_value is None.")
            else:
                print(f"`{pred}` is not understood.")
                raise NotImplementedError
        return len(df_)

    if condition is not None:
        count_numerator = count(df, event + " " + condition)
        count_denum = float(count(df, condition))
        # if count_numerator == 0:
        #     print("WARN: Empty Event+Condition!")
        # elif count_denum == 0:
        #     print("WARN: Empty Condition!")
        try:
            out_ = count_numerator / count_denum
        except ZeroDivisionError:
            out_ = np.NaN
        return out_
    else:
        count_numerator = count(df, event)
        count_denum = float(len(df))
        if count_numerator == 0:
            print("WARN: Empty Event!")
        elif count_denum == 0:
            print("WARN: Empty Set!")
        return count(df, event) / float(len(df))


def nor(a, b): return not xor(a, b)


def joint_prob(df, **kwargs):
    query = ["%s == %f" % (k, v) for (k, v) in kwargs.items()]
    return len(df.query(" and ".join(query))) / len(df)


# def vuln_regular(learn_df, eval_df, y_label, y_value, yhat_value, z_label=None, z_value=None):
#     terms = []
#     for p, y in [(1, 1), (1, 0), (0, 1), (0, 0)]:
#         rate_in = joint_prob(learn_df, yhat=p, income=y, m=1)
#         rate_out = joint_prob(learn_df, yhat=p, income=y, m=0)
#         rate_avg_learn = joint_prob(learn_df, yhat=p, income=y)
#
#         rate_avg_eval = joint_prob(eval_df, yhat=p, income=y)
#         if z_label is not None and z_value is not None:
#             rate_avg_eval = rate_avg_eval / joint_prob(eval_df, **{z_label: z_value})
#         elif not (z_label is not None) ^ (z_value is not None):
#             raise Exception("Both z_label and z_value should be defined.")
#
#         terms.append(max(rate_in, rate_out) / rate_avg_learn * rate_avg_eval)
#     #     return sum(terms)
#     return dict(zip(['tp', 'fp', 'fn', 'tn'], terms))
#
#
# def vuln_discriminating(learn_df, eval_df, y_label, z_label, z_values):
#     terms = []
#     for p, y, z in [(1, 1), (1, 0), (0, 1), (0, 0)]:
#         rate_in = joint_prob(learn_df, **{"yhat": p, y_label:y, "m": 1})
#         rate_out = joint_prob(learn_df, yhat=p, income=y, m=0)
#         rate_avg_learn = joint_prob(learn_df, yhat=p, income=y)
#         rate_avg_eval = joint_prob(eval_df, yhat=p, income=y)
#         terms.append(max(rate_in, rate_out) / rate_avg_learn * rate_avg_eval)
#     #     return sum(terms)
#     return dict(zip(['tp', 'fp', 'fn', 'tn'], terms))


def metrics_dict_to_dataframe(metrics):
    return reduce(lambda df1, df2: pd.DataFrame.append(df1, df2),
                  [df.assign(model=model_name) for model_name, df in metrics.items()])


def balance_dataset(data_undummified, z_label, y_label, random_state=None):
    subgroups = data_undummified[z_label].value_counts().index.to_list()
    regroup_list = []

    for sg in subgroups:
        df_ = data_undummified.loc[data_undummified[z_label] == sg]
        counts = df_[y_label].value_counts().sort_values(ascending=True)
        min_count, min_group_label, maj_group_label = counts.iloc[0], counts.index[0], counts.index[1]
        # min_group = resample(df_.loc[df_[y_label] == min_group_label], n_samples=min_count, replace=False)
        min_group = df_.loc[df_[y_label] == min_group_label]
        maj_group = resample(df_.loc[df_[y_label] == maj_group_label], n_samples=min_count, replace=False,
                             **({"random_state": random_state} if random_state is not None else {}))
        _balanced_df = min_group.append(maj_group)
        regroup_list.append(_balanced_df)

    balanced_df = reduce(lambda df1, df2: pd.DataFrame.append(df1, df2), regroup_list)
    return balanced_df


def compute_p_matrices(metrics_by_model, attacker, num_reps):
    """
    Compute pairwise t-tests.
    """
    p_matrices_by_model = {}
    for model, metrics in tqdm(metrics_by_model.items()):
        p_matrices = []
        for batch in range(num_reps):
            p_matrix = pd.DataFrame(np.zeros((len(subgroups), len(subgroups))),
                                    columns=subgroups, index=subgroups)
            for i, z1 in enumerate(subgroups):
                for j, z2 in enumerate(subgroups):
                    vuln_df = metrics.query(
                        f"vuln_method == 'cf' and attacker == '{attacker}' and batch_no == {batch}"
                    )
                    a = vuln_df.query(f"subgroup == {repr(z1)}").vuln.values.astype(np.float32)
                    b = vuln_df.query(f"subgroup == {repr(z2)}").vuln.values.astype(np.float32)
                    _, p = ttest_ind(a, b, equal_var=False)
                    p_matrix.loc[z1][z2] = p
            p_matrices.append(p_matrix)

        p_matrices_by_model[model] = p_matrices
    return p_matrices_by_model
