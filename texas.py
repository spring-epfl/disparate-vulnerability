# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.11.1
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %%
DATA_FILE_FORMAT = "results/texas_{}.csv"
NUM_SHUFFLES = 200
FIGWIDTH = 8
ALPHA = 0.01
LOAD_FROM_PICKLE = False

# %%
# %load_ext autoreload
# %autoreload 2

import itertools

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from tqdm import notebook as tqdm
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from joblib import Parallel, delayed
from statsmodels.stats.anova import AnovaRM
from statsmodels.stats.multicomp import MultiComparison

from model_zoo import model_zoo, renaming_dict, DpClassifierFactory
from mia.vuln import run_threshold_estimator
from utils import infer_from, score, normalize_vuln_mean, normalize_vuln_std
# from plotting import plot_stat_heatmaps, plot_vuln_dists
from loaders.texas import load_texas

import plot_params

# %% [markdown]
# ## Data prep

# %%
raw_data = load_texas("data/texas/PUDF_base1_%dq2013_tab.txt").dropna()
subgroup_label = "RACE"

# %%
subsampled_index, _ = train_test_split(raw_data.index, train_size=50000, random_state=0)
data = raw_data.loc[subsampled_index]

# %%
main_diagnoses = set(raw_data.PRINC_DIAG_CODE.value_counts()[:1000].index)
assert 0 not in main_diagnoses
other_diagnoses = set(raw_data.PRINC_DIAG_CODE.value_counts()[1000:].index)

data = data.replace({code: 0 for code in other_diagnoses})

# %%
print(data.RACE.value_counts().to_latex())

# %%
cat_cols = [                                                                                                                            
    "TYPE_OF_ADMISSION",
    "SEX_CODE",
    "RACE",
    "ETHNICITY",
    "ILLNESS_SEVERITY",
    "RISK_MORTALITY",
    "PRINC_DIAG_CODE",
]

num_cols = [
    "LENGTH_OF_STAY",
    "PAT_AGE",
    "TOTAL_CHARGES"
]

def process_for_fit(data):
    cat_vals = pd.get_dummies(
        data[[col for col in cat_cols]],
        drop_first=True).astype(int)
    
    num_vals = data[num_cols].apply(lambda x: x/x.max(), axis=0)
    X = pd.concat([cat_vals, num_vals], axis=1)
    y = data["TOTAL_CHARGES"] > data["TOTAL_CHARGES"].median()
    return X, y


# %%
X, y = process_for_fit(data)

# %%
X.shape, y.shape

# %%
dp_model_zoo = {
    f"eps{eps}": DpClassifierFactory(eps, data_norm=np.sqrt(11))
    for eps in [1, 2, 10]
}
model_zoo = dict(model_zoo, **dp_model_zoo)

# %%
renaming_dict = dict(renaming_dict, **{
    "threshold": "Control",
    "eps0.1": r"$\varepsilon=0.1$",
    "eps0.5": r"$\varepsilon=0.5$",
    "eps1": r"$\varepsilon=1$",
    "eps2": r"$\varepsilon=2$",
    "eps2.5": r"$\varepsilon=2.5$",
    "eps5": r"$\varepsilon=5$",
    "eps7.5": r"$\varepsilon=7.5$",
    "eps10": r"$\varepsilon=10$",
    "eps15": r"$\varepsilon=15$",
})


# %%
class ControlClassifier:
    """
    Task-specific control (data-independent) classifier.
    
    Outputs 1 if Prior Counts is greater than 0.
    """
    def __init__(self):
        pass
        
    def fit(self, *args, **kwargs):
        pass
    
    def predict(self, xs, *args, **kwargs):
        return xs[:, 0] > 0
    
    def predict_proba(self, xs, *args, **kwargs):
        p = self.predict(xs)
        p = np.expand_dims(p, 1)
        return np.hstack([1-p, p])
    
ControlClassifier().predict_proba(np.array([[0, 1], [1, 0]]))
model_zoo["control"] = lambda: ControlClassifier()
renaming_dict["control"] = "Control"

# %%
model_zoo.keys()

# %%
subgroups = [1, 2, 3, 4, 5]

# %% [markdown]
# ## Run experiments

# %%
experiment_results = {}

# %%
models = ["control", "lr", "nn_8", "nn_32"] + list(dp_model_zoo.keys()) + ["lr_dm_threshold", "lr_eo_threshold"]
parallel = True

is_fair_model = lambda model_name: any(keyword in model_name for keyword in ["eo", "dm", "erp"])

for model_name in models:
    print(model_name)
    model_fn = model_zoo[model_name]
    
    def run_iter(rep, rng):
        # Train model.
        train_indices, test_indices = train_test_split(data.index, test_size=0.5, random_state=rng)
        X_train, y_train = X.loc[train_indices], y.loc[train_indices]
        X_test, y_test = X.loc[test_indices], y.loc[test_indices]
        
        fit_args = {}
        train_sensitive_features = None
        test_sensitive_features = None
        if is_fair_model(model_name):
            train_sensitive_features = data.loc[train_indices, subgroup_label]
            test_sensitive_features = data.loc[test_indices, subgroup_label]
            fit_args = dict(sensitive_features=train_sensitive_features)
            
        clf = model_fn()
        clf.fit(X_train.values, y_train.values, **fit_args)

        results = []
        for subgroup in ["All"] + subgroups:
            # Model info.            
            if subgroup == "All":
                group_train_indices = train_indices
                group_test_indices = test_indices
            else:
                group_train_indices = data.loc[train_indices].query(f"{subgroup_label} == '{subgroup}'").index
                group_test_indices = data.loc[test_indices].query(f"{subgroup_label} == '{subgroup}'").index
            
            X_group_train, y_group_train = X_train.loc[group_train_indices], y_train.loc[group_train_indices]
            X_group_test, y_group_test = X_test.loc[group_test_indices], y_test.loc[group_test_indices]
            
            group_train_sensitive_features = None
            group_test_sensitive_features = None
            if is_fair_model(model_name):
                group_train_sensitive_features = data.loc[group_train_indices, subgroup_label]
                group_test_sensitive_features = data.loc[group_test_indices, subgroup_label]
                
            # Model info.
            target_train_acc = score(clf, X_group_train, y_group_train, group_train_sensitive_features)
            target_test_acc = score(clf, X_group_test, y_group_test, group_test_sensitive_features)
            overfitting_gap = target_train_acc - target_test_acc
            
            # Vulnerability.                  
            vuln = run_threshold_estimator(
                y_group_train, infer_from(clf, X_group_train, group_train_sensitive_features),
                y_group_test, infer_from(clf, X_group_test, group_test_sensitive_features),
                method="average_loss_threshold",
            )
            
            results.append(dict(rep=rep,
                               vuln=vuln,
                               subgroup=subgroup,
                               target_train_acc=target_train_acc,
                               target_test_acc=target_test_acc,
                               overfitting_gap=overfitting_gap))
            
        return results
    
    reps = range(NUM_SHUFFLES)
    rngs = [np.random.RandomState(i) for i in enumerate(reps)]
    it = tqdm.tqdm(list(zip(reps, rngs)))
    
    # Run experiments and collect results.
    if parallel:
        collected_subgroup_vulns = Parallel(n_jobs=8, verbose=0)(delayed(run_iter)(rep, rng)
                                              for rep, rng in it)
    else:
        collected_subgroup_vulns = []
        for rep, rng in it:
            collected_subgroup_vulns.append(run_iter(rep, rng))

    model_results = pd.DataFrame(itertools.chain(*collected_subgroup_vulns))
    
    # Compute ANOVA p-value right away.
    anova = AnovaRM(model_results.query("subgroup != 'All'"),
        depvar="vuln", subject="rep", within=["subgroup"])
    res = anova.fit()
    f, p = (
        res.anova_table.loc["subgroup", "F Value"],
        res.anova_table.loc["subgroup", "Pr > F"]
    )
    print(f"p={p:.4f}\n")
    
    # Save experimental results for summaries and followup tests.
    experiment_results[model_name] = model_results.assign(model=model_name, f=f, p=p)

# %%
# pd.concat(experiment_results.values()).to_csv(DATA_FILE_FORMAT.format("pets22_camready"))

# %%
renaming_dict["control"] = "Control"

# %%
summary_data = pd.concat(experiment_results.values()).replace(renaming_dict)
summary_table = summary_data.query("subgroup == 'All'").groupby(["model"]).agg({
    "p": "mean",
    "target_test_acc": ["mean", "std"],
    "overfitting_gap": ["mean", "std"],
    "vuln": [normalize_vuln_mean, normalize_vuln_std],
}).reindex([renaming_dict[model_name] for model_name in experiment_results.keys()]).rename(columns={
    "model": "Model",
    "target_test_acc": "Test acc.",
    "overfitting_gap": "Gen. gap",
    "vuln": "Overall vuln.",
})
summary_table

# %%
print(summary_table.to_latex(float_format="%.4f"))

# %%
summary_data = pd.concat(experiment_results.values()).replace(renaming_dict)
disaggregated_table = summary_data.query(f"subgroup != 'All' and p < {ALPHA}").groupby(["model", "subgroup"]).agg({
    "target_test_acc": ["mean", "std"],
    "overfitting_gap": ["mean", "std"],
    "vuln": [normalize_vuln_mean, normalize_vuln_std],
}).rename(columns={
    "model": "Model",
    "target_test_acc": "Test acc.",
    "overfitting_gap": "Gen. gap",
    "vuln": "Subgroup vuln.",
})
disaggregated_table

# %%
print(disaggregated_table.to_latex(float_format="%.4f"))

# %%
for model_name, df in experiment_results.items():
    p = df.p.mean()
    if p < ALPHA:
        _, _ , cmp = MultiComparison(
            data=df.query("subgroup != 'All'").vuln,\
            groups=df.query("subgroup != 'All'").subgroup,
        ).allpairtest(stats.ttest_rel, alpha=ALPHA, method="fdr_bh")
        print(f"{model_name} {p=:.4f}")
        print(pd.DataFrame(cmp).iloc[:, :-1].to_latex(float_format="%.4f"))
