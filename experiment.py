import joblib
import pandas as pd
import numpy as np
import itertools
from multiprocess.pool import Pool
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from tqdm import tqdm_notebook as tqdm
from sklearn.linear_model import LogisticRegression
import sys
from functools import reduce
from operator import add
from fbleau.fbleau import calc_vulnerability as fbleau_vuln

from compas import prepare_compas
from helpers import encode_dataframe, prob_operator
from fixes import model_zoo
from fbleau.fbleau import calc_vulnerability

from mia.vuln import calc_vulnerability_closed_form


def run_compas_experiment(data, z_label, z_values, clf, fit_args=None, num_batches=1):
    """
    :: experiment settings

    """

    # setup
    # if classifier is None:
    #     clf = LogisticRegression(solver="lbfgs")
    # else:
    #     clf = classifier

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + '-' + x["Ground Truth"], axis=1)

    y_label = "Ground Truth_Will not reoffend"
    encode = lambda df, cols: encode_dataframe(df, drop_first=False,
                                               output_columns=cols,
                                               initially_drop_columns=None,
                                               add_non_dummified_columns=[z_label],
                                               finally_drop_columns=['Ground Truth_Will reoffend', 'Gender_Female',
                                                                     'Felony or Misdemeanor_Misdemeanor'] if cols == None else None,
                                               return_output_columns=cols == None)

    metrics = []
    for batch_no in tqdm(range(num_batches)):

        # train/test split
        train_df, test_df = train_test_split(data, test_size=0.5, stratify=data["strat"])

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)

        # separate features and labels
        X_train, y_train = train_df.drop([y_label, z_label, "strat"], axis=1), train_df[y_label]
        X_test, y_test = test_df.drop([y_label, z_label, "strat"], axis=1), test_df[y_label]

        # print(X_train.columns)

        fit_args_ = dict(sensitive_features=train_df[z_label],
                         **{k: v for k, v in fit_args.items() if k != "sensitive"}) if fit_args.get("sensitive",
                                                                                                    False) else dict()
        clf.fit(X_train, y_train, **fit_args_)

        # save confidences to train/test dataframes
        predict_args_ = lambda df: dict(sensitive_features=df[z_label]) if fit_args.get("sensitive", False) else dict()

        train_df.loc[:, "p"] = clf.predict_proba(X_train, **predict_args_(train_df))[:, 1]
        test_df.loc[:, "p"] = clf.predict_proba(X_test, **predict_args_(test_df))[:, 1]

        # save predicted labels to train/test dataframes
        train_df.loc[:, "yhat"] = clf.predict(X_train, **predict_args_(train_df))
        test_df.loc[:, "yhat"] = clf.predict(X_test, **predict_args_(test_df))

        # measure target model accuracy
        target_train_acc = accuracy_score(train_df[y_label], train_df["yhat"])
        target_test_acc = accuracy_score(test_df[y_label], test_df["yhat"])

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(train_df, test_size=0.3, stratify=train_df["strat"])

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(test_df, test_size=0.3, stratify=test_df["strat"])

        for z_val in z_values + [None]:
            # Counts.
            disc_vuln = calc_vulnerability_closed_form(train_learn_df, train_eval_df, test_learn_df,
                    test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                    y_label=y_label, attacker="discriminating")
            reg_vuln = calc_vulnerability_closed_form(train_learn_df, train_eval_df, test_learn_df,
                    test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                    y_label=y_label, attacker="regular")
            metrics.append((z_val or "Overall", "cf", batch_no, 'discriminating', target_train_acc, target_test_acc,
                    disc_vuln))
            metrics.append((z_val or "Overall", "cf", batch_no, 'regular', target_train_acc, target_test_acc,
                    reg_vuln))

            # fbleau.
            disc_vuln = calc_vulnerability(train_learn_df, train_eval_df, test_learn_df,
                    test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                    y_label=y_label, attacker="discriminating")
            reg_vuln = calc_vulnerability(train_learn_df, train_eval_df, test_learn_df,
                    test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                    y_label=y_label, attacker="regular")
            metrics.append((z_val or "Overall", "fbleau", batch_no, 'discriminating', target_train_acc, target_test_acc,
                    disc_vuln))
            metrics.append((z_val or "Overall", "fbleau", batch_no, 'regular', target_train_acc, target_test_acc,
                    reg_vuln))

    return pd.DataFrame(
            metrics, columns=["subgroup", "type", "batch_no", "attacker", "target_train_acc", "target_test_acc", "vuln"])



def run_adult_experiment(data, z_label, z_values, clf, fit_args=None, n_processes=4, num_batches=1, diagnostic=0):
    """
    :: experiment settings

    """

    # setup
    # if classifier is sNone:
    #     clf = LogisticRegression(solver="lbfgs")
    # else:
    #     clf = classifier

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + '-' + str(x["income"]), axis=1)

    y_label = "income"
    encode = lambda df, cols: encode_dataframe(df, drop_first=False,
                                               output_columns=cols,
                                               initially_drop_columns=None,
                                               add_non_dummified_columns=[z_label],
                                               finally_drop_columns=[] if cols == None else None,
                                               return_output_columns=cols == None)

    def one_iter(batch_no):

        metrics = []
        # train/test split
        train_df, test_df = train_test_split(data, test_size=0.5, stratify=data["strat"], random_state=batch_no)

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)

        # separate features and labels
        X_train, y_train = train_df.drop([y_label, z_label, "strat"], axis=1), train_df[y_label]
        X_test, y_test = test_df.drop([y_label, z_label, "strat"], axis=1), test_df[y_label]

        fit_args_ = dict(sensitive_features=train_df[z_label],
                         **{k: v for k, v in fit_args.items() if k != "sensitive"}) if fit_args.get("sensitive",
                                                                                                    False) else dict()
        clf.fit(X_train, y_train, **fit_args_)

        # save confidences to train/test dataframes
        predict_args_ = lambda df: dict(sensitive_features=df[z_label]) if fit_args.get("sensitive", False) else dict()

        train_df.loc[:, "p"] = clf.predict_proba(X_train, **predict_args_(train_df))[:, 1]
        test_df.loc[:, "p"] = clf.predict_proba(X_test, **predict_args_(test_df))[:, 1]

        # save predicted labels to train/test dataframes
        train_df.loc[:, "yhat"] = clf.predict(X_train, **predict_args_(train_df))
        test_df.loc[:, "yhat"] = clf.predict(X_test, **predict_args_(test_df))

        # measure target model accuracy
        target_train_acc = accuracy_score(train_df[y_label], train_df["yhat"])
        target_test_acc = accuracy_score(test_df[y_label], test_df["yhat"])

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(train_df, test_size=0.3, stratify=train_df["strat"],
                                                         random_state=batch_no)

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(test_df, test_size=0.3, stratify=test_df["strat"],
                                                       random_state=batch_no)

        if diagnostic == 1:
            return train_learn_df, train_eval_df, test_learn_df, test_eval_df

        for z_val in z_values + [None]:
            # Counts.
            disc_vuln = calc_vulnerability_closed_form(train_learn_df, train_eval_df, test_learn_df,
                    test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                    y_label=y_label, attacker="discriminating")
            reg_vuln = calc_vulnerability_closed_form(train_learn_df, train_eval_df, test_learn_df,
                    test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                    y_label=y_label, attacker="regular")
            metrics.append((z_val or "Overall", "cf", batch_no, 'discriminating', target_train_acc, target_test_acc,
                    disc_vuln))
            metrics.append((z_val or "Overall", "cf", batch_no, 'regular', target_train_acc, target_test_acc,
                    reg_vuln))

            # fbleau.
            disc_vuln = calc_vulnerability(train_learn_df, train_eval_df, test_learn_df,
                    test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                    y_label=y_label, attacker="discriminating")
            reg_vuln = calc_vulnerability(train_learn_df, train_eval_df, test_learn_df,
                    test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                    y_label=y_label, attacker="regular")
            metrics.append((z_val or "Overall", "fbleau", batch_no, 'discriminating', target_train_acc, target_test_acc,
                    disc_vuln))
            metrics.append((z_val or "Overall", "fbleau", batch_no, 'regular', target_train_acc, target_test_acc,
                    reg_vuln))

        return pd.DataFrame(
            metrics, columns=["subgroup", "type", "batch_no", "attacker", "target_train_acc", "target_test_acc", "vuln"])

    if diagnostic == 1:
        return one_iter(num_batches)
    elif diagnostic == 2:
        results = one_iter(num_batches)
    else:
        with joblib.parallel_backend("dask"):
            results = joblib.Parallel(n_jobs=n_processes, verbose=100)(
                joblib.delayed(one_iter)(i) for i in range(num_batches))

    # # flatten list of lists
    # # print(results)
    # results = reduce(lambda list1, list2: [*list1, *list2], results)

    return results

if __name__ == "__main__":
    data_undummified = prepare_compas("data/compas/compas-scores-two-years.csv", dummified=False, drop_first=True,
                                      drop_duplicates=False)

    fit_args = {"fair_lr": dict(sensitive_col_idx=[10], correlation_tolerance=0.2)}

    metrics_gender = {
        model_name: run_compas_experiment(data_undummified, z_label="Gender", z_values=["Male", "Female"],
                                          num_batches=1,
                                          clf=clf, fit_args=fit_args.get(model_name, {})) for model_name, clf in
        model_zoo.items()}
