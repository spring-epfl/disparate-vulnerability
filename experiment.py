from itertools import product
import sys
import joblib
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

import tensorflow as tf
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from tqdm import tqdm_notebook as tqdm

from functools import reduce

from compas import prepare_compas
from helpers import encode_dataframe, balance_dataset, reset_weights
from fixes import model_zoo

from copy import deepcopy

from fbleau_wrapper import estimate_vulnerability as est_vuln_fbleau
from mia.vuln import estimate_vulnerability as est_vuln_counts

import pdb


def run_compas_experiment(data, z_label, z_values, clf, fit_args=None, num_batches=1, continuous=False,
                          estimation_method="frequentist",
                          diagnose_calibration=False, synthetic_bins=None):
    """
    :: experiment settings

    """

    # setup
    # if classifier is None:
    #     clf = LogisticRegression(solver="lbfgs")
    # else:
    #     clf = classifier

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + '-' + x["Ground Truth"], axis=1)

    y_label = "Ground Truth_Will not reoffend"
    encode = lambda df, cols: encode_dataframe(df, drop_first=False,
                                               output_columns=cols,
                                               initially_drop_columns=None,
                                               add_non_dummified_columns=[z_label],
                                               finally_drop_columns=['Ground Truth_Will reoffend', 'Gender_Female',
                                                                     'Felony or Misdemeanor_Misdemeanor'] if cols == None else None,
                                               return_output_columns=cols == None)

    metrics = []
    for batch_no in tqdm(range(num_batches)):

        # train/test split
        train_df, test_df = train_test_split(data, test_size=0.5, stratify=data["strat"])

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)

        # separate features and labels
        X_train, y_train = train_df.drop([y_label, z_label, "strat"], axis=1), train_df[y_label]
        X_test, y_test = test_df.drop([y_label, z_label, "strat"], axis=1), test_df[y_label]

        # print(X_train.columns)

        fit_args_ = dict(sensitive_features=train_df[z_label],
                         **{k: v for k, v in fit_args.items() if k != "sensitive"}) if fit_args.get("sensitive",
                                                                                                    False) else dict()
        clf.fit(X_train, y_train, **fit_args_)

        # save confidences to train/test dataframes
        predict_args_ = lambda df: dict(sensitive_features=df[z_label]) if fit_args.get("sensitive", False) else dict()

        train_df.loc[:, "p"] = clf.predict_proba(X_train, **predict_args_(train_df))[:, 1]
        test_df.loc[:, "p"] = clf.predict_proba(X_test, **predict_args_(test_df))[:, 1]

        decisions_train = clf.predict(X_train, **predict_args_(train_df))
        decisions_test = clf.predict(X_test, **predict_args_(test_df))
        if synthetic_bins is not None:
            bins = np.histogram_bin_edges(train_df.p, bins=np.linspace(0, 1, synthetic_bins + 1))
            # print(bins)
            train_df.loc[:, "yhat"] = np.digitize(train_df.p.values, bins)
            test_df.loc[:, "yhat"] = np.digitize(test_df.p.values, bins)
        else:
            train_df.loc[:, "yhat"] = decisions_train
            test_df.loc[:, "yhat"] = decisions_test

        if diagnose_calibration:
            print(bins)
            train_df.yhat.hist()
            plt.show()

        # measure target model accuracy
        target_train_acc = accuracy_score(train_df[y_label], decisions_train)
        target_test_acc = accuracy_score(test_df[y_label], decisions_test)

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(train_df, test_size=0.3, stratify=train_df["strat"])

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(test_df, test_size=0.3, stratify=test_df["strat"])

        eval_df = train_eval_df.append(test_eval_df)

        for z_val in z_values + [None]:
            if z_val is not None:
                support = len(eval_df.loc[eval_df[z_label] == z_val])
            else:
                support = len(eval_df)

            disc_vuln = est_vuln_counts(train_learn_df, train_eval_df, test_learn_df,
                                        test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                                        y_label=y_label, attacker="discriminating", y_range=[0, 1],
                                        p_range=sorted(train_df.yhat.unique()),
                                        full_set_eval=True)
            reg_vuln = est_vuln_counts(train_learn_df, train_eval_df, test_learn_df,
                                       test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                                       y_label=y_label, attacker="regular", y_range=[0, 1],
                                       p_range=sorted(train_df.yhat.unique()),
                                       full_set_eval=True)

            # fbleau.
            # disc_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                             test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
            #                             y_label=y_label, attacker="discriminating", estimate_method=estimation_method,
            #                             full_set_eval=True)
            # reg_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                            test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
            #                            y_label=y_label, attacker="regular", estimate_method=estimation_method,
            #                            full_set_eval=True)

            for vuln in disc_vuln:
                metrics.append(
                    (z_val or "Overall", "counts", batch_no, 'discriminating', target_train_acc, target_test_acc,
                     vuln, support))
            for vuln in reg_vuln:
                metrics.append((z_val or "Overall", "counts", batch_no, 'regular', target_train_acc, target_test_acc,
                                vuln, support))

    return pd.DataFrame(
        metrics,
        columns=["subgroup", "vuln_method", "batch_no", "attacker", "target_train_acc", "target_test_acc", "vuln",
                 "support"])


def run_adult_experiment(data, z_label, z_values, clf, clf_name=None, fit_args=None, n_processes=4, num_batches=1,
                         diagnostic=0, balanced=False, parallelize=False, synthetic_bins=None,
                         diagnose_calibration=False):
    """
    :: experiment settings

    """

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + '-' + str(x["income"]), axis=1)

    y_label = "income"
    encode = lambda df, cols: encode_dataframe(df, drop_first=False,
                                               output_columns=cols,
                                               initially_drop_columns=None,
                                               add_non_dummified_columns=[z_label],
                                               finally_drop_columns=[] if cols == None else None,
                                               return_output_columns=cols == None)

    def one_iter(batch_no):

        if balanced:
            _data = balance_dataset(data, z_label, y_label, random_state=batch_no)
        else:
            _data = data

        metrics = []
        # train/test split
        train_df, test_df = train_test_split(_data, test_size=0.5, stratify=_data["strat"], random_state=batch_no)

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)

        # separate features and labels
        X_train, y_train = train_df.drop([y_label, z_label, "strat"], axis=1), train_df[y_label]
        X_test, y_test = test_df.drop([y_label, z_label, "strat"], axis=1), test_df[y_label]

        fit_args_ = dict(sensitive_features=train_df[z_label],
                         **{k: v for k, v in fit_args.items() if k != "sensitive"}) if fit_args.get("sensitive",
                                                                                                    False) else dict()
        clf.fit(X_train, y_train, **fit_args_)

        # save confidences to train/test dataframes
        predict_args_ = lambda df: dict(sensitive_features=df[z_label]) if fit_args.get("sensitive", False) else dict()

        train_df.loc[:, "p"] = clf.predict_proba(X_train, **predict_args_(train_df))[:, 1]
        test_df.loc[:, "p"] = clf.predict_proba(X_test, **predict_args_(test_df))[:, 1]

        # save predicted labels to train/test dataframes
        decisions_train = clf.predict(X_train, **predict_args_(train_df))
        decisions_test = clf.predict(X_test, **predict_args_(test_df))

        # save decisions for subgroup overfitting
        train_df.loc[:, "_decision"] = decisions_train
        test_df.loc[:, "_decision"] = decisions_test

        if synthetic_bins is not None:
            bins = np.histogram_bin_edges(train_df.p, bins=np.linspace(0, 1, synthetic_bins + 1))
            # print(bins)
            train_df.loc[:, "yhat"] = np.digitize(train_df.p.values, bins)
            test_df.loc[:, "yhat"] = np.digitize(test_df.p.values, bins)
        else:
            train_df.loc[:, "yhat"] = decisions_train
            test_df.loc[:, "yhat"] = decisions_test

        if diagnose_calibration:
            print(bins)
            print(train_df.loc[:, "yhat"])
            train_df.yhat.hist()
            plt.show()

        #         # measure target model accuracy
        #         target_train_acc = accuracy_score(train_df[y_label], decisions_train)
        #         target_test_acc = accuracy_score(test_df[y_label], decisions_test)

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(train_df, test_size=0.3, stratify=train_df["strat"],
                                                         random_state=batch_no)

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(test_df, test_size=0.3, stratify=test_df["strat"],
                                                       random_state=batch_no)

        if diagnostic == 1:
            return train_learn_df, train_eval_df, test_learn_df, test_eval_df

        eval_df = train_eval_df.append(test_eval_df)
        for z_val in z_values + [None]:
            if z_val is not None:
                ## Subgroup metrics
                support = len(eval_df.loc[eval_df[z_label] == z_val])
                subgroup_train_df = train_df.loc[train_df[z_label] == z_val]
                subgroup_test_df = test_df.loc[test_df[z_label] == z_val]

                # measure subgroup target model accuracy
                target_train_acc = accuracy_score(subgroup_train_df[y_label], subgroup_train_df["_decision"])
                target_test_acc = accuracy_score(subgroup_test_df[y_label], subgroup_test_df["_decision"])

                # collect confusion matrix measures
                tn_train, fp_train, fn_train, tp_train = confusion_matrix(subgroup_train_df[y_label],
                                                                          subgroup_train_df["_decision"]).ravel()
                tn_test, fp_test, fn_test, tp_test = confusion_matrix(subgroup_test_df[y_label],
                                                                      subgroup_test_df["_decision"]).ravel()

            else:
                ## Overall metrics
                support = len(eval_df)

                # measure overall target model accuracy
                target_train_acc = accuracy_score(train_df[y_label], decisions_train)
                target_test_acc = accuracy_score(test_df[y_label], decisions_test)

                # collect conufsion matrix measures
                tn_train, fp_train, fn_train, tp_train = confusion_matrix(train_df[y_label], decisions_train).ravel()
                tn_test, fp_test, fn_test, tp_test = confusion_matrix(train_df[y_label], decisions_train).ravel()

            # Counts.
            disc_vuln = est_vuln_counts(train_learn_df, train_eval_df, test_learn_df,
                                        test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                                        y_label=y_label, attacker="discriminating", y_range=[0, 1],
                                        p_range=sorted(train_df.yhat.unique()),
                                        full_set_eval=True)
            reg_vuln = est_vuln_counts(train_learn_df, train_eval_df, test_learn_df,
                                       test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values, z_value=z_val,
                                       y_label=y_label, attacker="regular", y_range=[0, 1],
                                       p_range=sorted(train_df.yhat.unique()),
                                       full_set_eval=True)

            # fbleau.
            # disc_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                             test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values,
            #                             z_value=z_val,
            #                             y_label=y_label, attacker="discriminating")
            # reg_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                            test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values,
            #                            z_value=z_val,
            #                            y_label=y_label, attacker="regular")

            # import pdb; pdb.set_trace()

            for vuln in disc_vuln:
                metrics.append(
                    (z_val or "Overall", "counts", batch_no, 'discriminating', target_train_acc, target_test_acc,
                     vuln, support, tn_train, fp_train, fn_train, tp_train, tn_test, fp_test, fn_test, tp_test))
            for vuln in reg_vuln:
                metrics.append((z_val or "Overall", "counts", batch_no, 'regular', target_train_acc, target_test_acc,
                                vuln, support, tn_train, fp_train, fn_train, tp_train, tn_test, fp_test, fn_test,
                                tp_test))

        return metrics

    if diagnostic == 1:
        return one_iter(num_batches)
    elif diagnostic == 2:
        results = one_iter(num_batches)
    elif diagnostic == 3:
        return
    else:
        if clf_name is not None:
            if "nn" in clf_name:
                n_jobs = int(n_processes / 3)
            else:
                n_jobs = n_processes
        else:
            n_jobs = n_processes

        if parallelize:
            with joblib.parallel_backend("loky", inner_max_num_threads=4):
                results = joblib.Parallel(n_jobs=n_jobs, verbose=51, prefer="threads")(
                    joblib.delayed(one_iter)(i) for i in range(num_batches))
                results = reduce(lambda list1, list2: [*list1, *list2], results)
        else:
            results = []
            for i in tqdm(range(num_batches), leave=False):
                results += one_iter(i)

    return pd.DataFrame(
        results,
        columns=["subgroup", "vuln_method", "batch_no", "attacker", "target_train_acc", "target_test_acc", "vuln",
                 "support", "tn_train", "fp_train", "fn_train", "tp_train", "tn_test", "fp_test", "fn_test", "tp_test"])


def run_generic_experiment(binary_data, y_label, z_label, z_values, clf, clf_name=None, fit_args=None, n_processes=4,
                           num_batches=1, synthetic_bins=None, diagnose_calibration=False,
                           diagnostic=0, balanced=False, estimation_method="frequentist", continuous=False,
                           parallelize=False,
                           random_state=None,
                           meta_data=None,
                           validation=True,
                           compile_parameters=None,
                           attacker_clf="counts",
                           full_set_eval=True):
    """
    :: experiment settings

    """

    # get model configuration
    # model_initial_weights = clf.get_weights()

    # set compile parameters
    if compile_parameters is None:
        compile_parameters = dict(loss='sparse_categorical_crossentropy', optimizer='adam',
                                  metrics=['accuracy'])

    if meta_data is not None:
        if len(meta_data) != len(binary_data):
            raise Exception(
                "Length mismatch 'len(meta_data) != len(binary_data)'.`meta_data` and `binary_data` should use the same indexer.")
        data = meta_data
    else:
        data = binary_data

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + '-' + str(x[y_label]), axis=1)
    p_label = "yhat" if not continuous else "p"

    encode = lambda df, cols: encode_dataframe(df,
                                               output_columns=cols,
                                               initially_drop_columns=["age"] if meta_data is not None else None,
                                               drop_first=False,
                                               add_non_dummified_columns=[z_label],
                                               ignore_cols=[y_label],
                                               finally_drop_columns=[] if cols == None else None,
                                               return_output_columns=cols == None)

    def one_iter(batch_no):

        if meta_data is not None:
            # clone and compile model
            clf_cloned = tf.keras.models.clone_model(clf)
            # clf_cloned.set_weights(model_initial_weights)
            clf_cloned.compile(**compile_parameters)
        else:
            clf_cloned = clf

        if random_state is None:
            batch_random_state = batch_no
        else:
            batch_random_state = random_state

        if balanced:
            if meta_data is not None:
                raise NotImplementedError("Cannot have both `balanced` and `meta_data`.")
            _data = balance_dataset(data, z_label, y_label, random_state=batch_random_state)
        else:
            _data = data

        metrics = []
        # train/test split

        train_idx, test_idx = train_test_split(_data.index, test_size=0.5, stratify=_data["strat"],
                                               random_state=batch_random_state)

        if validation:
            # import pdb; pdb.set_trace();
            valid_idx, test_idx = train_test_split(test_idx, test_size=0.5, stratify=_data.loc[test_idx]["strat"],
                                                   random_state=batch_random_state)
        else:
            valid_idx = []

        train_df = _data.loc[train_idx]
        test_df = _data.loc[test_idx]
        valid_df = _data.loc[valid_idx]

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)
        valid_df = encode(valid_df, cols)

        # separate features and labels
        if meta_data is None:
            X_train, y_train = train_df.drop([y_label, z_label, "strat"], axis=1), train_df[y_label]
            X_test, y_test = test_df.drop([y_label, z_label, "strat"], axis=1), test_df[y_label]
        else:
            X_train = binary_data[train_idx]
            y_train = train_df[y_label]
            X_test = binary_data[test_idx]
            y_test = test_df[y_label]

        fit_args_ = dict(sensitive_features=train_df[z_label],
                         **{k: v for k, v in fit_args.items() if k != "sensitive"}) if fit_args.get("sensitive",
                                                                                                    False) else fit_args
        if validation:
            fit_args_["validation_data"] = (binary_data[valid_idx], valid_df[y_label])

        # pdb.set_trace()
        tf.random.set_seed(batch_random_state)
        clf_cloned.fit(X_train, y_train, **fit_args_)

        if diagnostic == 4:
            return clf_cloned

        # save confidences to train/test dataframes
        predict_args_ = lambda df: dict(sensitive_features=df[z_label]) if fit_args.get("sensitive", False) else dict()

        train_df.loc[:, "p"] = clf_cloned.predict_proba(X_train, **predict_args_(train_df))[:, 1]
        test_df.loc[:, "p"] = clf_cloned.predict_proba(X_test, **predict_args_(test_df))[:, 1]

        # save predicted labels to train/test dataframes
        decisions_train = clf_cloned.predict(X_train, **predict_args_(train_df))
        decisions_test = clf_cloned.predict(X_test, **predict_args_(test_df))

        # save decisions for subgroup overfitting
        train_df.loc[:, "_decision"] = decisions_train
        test_df.loc[:, "_decision"] = decisions_test

        if synthetic_bins is not None:
            bins = np.histogram_bin_edges(train_df.p, bins=np.linspace(0, 1, synthetic_bins + 1))
            # print(bins)
            train_df.loc[:, "yhat"] = np.digitize(train_df.p.values, bins)
            test_df.loc[:, "yhat"] = np.digitize(test_df.p.values, bins)
        else:
            train_df.loc[:, "yhat"] = decisions_train
            test_df.loc[:, "yhat"] = decisions_test

        if diagnose_calibration:
            print(bins)
            train_df.yhat.hist()
            plt.show()

        # measure target model accuracy
        if meta_data is not None:
            target_train_acc = accuracy_score(train_df[y_label].values, np.argmax(decisions_train, axis=1))
            target_test_acc = accuracy_score(test_df[y_label].values, np.argmax(decisions_test, axis=1))
        else:
            target_train_acc = accuracy_score(train_df[y_label], decisions_train)
            target_test_acc = accuracy_score(test_df[y_label], decisions_test)

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(train_df, test_size=0.3, stratify=train_df["strat"],
                                                         random_state=batch_random_state)

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(test_df, test_size=0.3, stratify=test_df["strat"],
                                                       random_state=batch_random_state)

        if diagnostic == 1:
            return train_learn_df, train_eval_df, test_learn_df, test_eval_df

        eval_df = train_eval_df.append(test_eval_df)
        for z_val in z_values + [None]:
            if z_val is not None:
                ## Subgroup metrics
                support = len(eval_df.loc[eval_df[z_label] == z_val])
                subgroup_train_df = train_df.loc[train_df[z_label] == z_val]
                subgroup_test_df = test_df.loc[test_df[z_label] == z_val]

                # measure subgroup target model accuracy
                target_train_acc = accuracy_score(subgroup_train_df[y_label], subgroup_train_df["_decision"])
                target_test_acc = accuracy_score(subgroup_test_df[y_label], subgroup_test_df["_decision"])

                # collect confusion matrix measures
                tn_train, fp_train, fn_train, tp_train = confusion_matrix(subgroup_train_df[y_label],
                                                                          subgroup_train_df["_decision"]).ravel()
                tn_test, fp_test, fn_test, tp_test = confusion_matrix(subgroup_test_df[y_label],
                                                                      subgroup_test_df["_decision"]).ravel()

            else:
                ## Overall metrics
                support = len(eval_df)

                # measure overall target model accuracy
                target_train_acc = accuracy_score(train_df[y_label], decisions_train)
                target_test_acc = accuracy_score(test_df[y_label], decisions_test)

                # collect conufsion matrix measures
                tn_train, fp_train, fn_train, tp_train = confusion_matrix(train_df[y_label], decisions_train).ravel()
                tn_test, fp_test, fn_test, tp_test = confusion_matrix(train_df[y_label], decisions_train).ravel()

            # Counts.
            disc_vuln = est_vuln_counts(train_learn_df, train_eval_df, test_learn_df,
                                        test_eval_df, p_label=p_label, z_label=z_label, z_values=z_values,
                                        z_value=z_val,
                                        y_label=y_label, attacker="discriminating", y_range=[0, 1],
                                        p_range=sorted(train_df.yhat.unique()),
                                        full_set_eval=full_set_eval,
                                        attacker_clf=attacker_clf)
            reg_vuln = est_vuln_counts(train_learn_df, train_eval_df, test_learn_df,
                                       test_eval_df, p_label=p_label, z_label=z_label, z_values=z_values, z_value=z_val,
                                       y_label=y_label, attacker="regular", y_range=[0, 1],
                                       p_range=sorted(train_df.yhat.unique()),
                                       full_set_eval=full_set_eval,
                                       attacker_clf=attacker_clf)

            # fbleau.
            # disc_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                             test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values,
            #                             z_value=z_val,
            #                             y_label=y_label, attacker="discriminating")
            # reg_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                            test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values,
            #                            z_value=z_val,
            #                            y_label=y_label, attacker="regular")

            for vuln in disc_vuln:
                metrics.append(
                    (z_val or "Overall", "counts", batch_no, 'discriminating', target_train_acc, target_test_acc,
                     vuln, support, tn_train, fp_train, fn_train, tp_train, tn_test, fp_test, fn_test, tp_test))
            for vuln in reg_vuln:
                metrics.append((z_val or "Overall", "counts", batch_no, 'regular', target_train_acc, target_test_acc,
                                vuln, support, tn_train, fp_train, fn_train, tp_train, tn_test, fp_test, fn_test,
                                tp_test))
        # Clean up
        tf.keras.backend.clear_session()
        del clf_cloned

        return metrics

    if diagnostic == 1:
        return one_iter(num_batches)
    elif diagnostic == 2:
        results = one_iter(num_batches)
    elif diagnostic == 3:
        return
    elif diagnostic == 4:
        return one_iter(num_batches)
    else:
        if clf_name is not None:
            if "nn" in clf_name:
                n_jobs = int(n_processes / 2) if n_processes < 10 else int(n_processes / 3)
            else:
                n_jobs = n_processes
        else:
            n_jobs = n_processes

        if parallelize:
            with joblib.parallel_backend("loky", inner_max_num_threads=4):
                results = joblib.Parallel(n_jobs=n_jobs, verbose=51, prefer="threads")(
                    joblib.delayed(one_iter)(i) for i in range(num_batches))
                results = reduce(lambda list1, list2: [*list1, *list2], results)
        else:
            results = []
            for i in tqdm(range(num_batches), leave=False):
                results += one_iter(i)

    return pd.DataFrame(
        results,
        columns=["subgroup", "vuln_method", "batch_no", "attacker", "target_train_acc", "target_test_acc", "vuln",
                 "support", "tn_train", "fp_train", "fn_train", "tp_train", "tn_test", "fp_test", "fn_test", "tp_test"])

def run_generic_keras_experiment(
    binary_data,
    y_label,
    z_label,
    z_values,
    clf,
    clf_name=None,
    fit_args=None,
    num_batches=1,
    synthetic_bins=None,
    diagnose_calibration=False,
    diagnostic=0,
    balanced=False,
    estimation_method="frequentist",
    continuous=False,
    checkpoint=None,
    y_range=None,
    random_state=None,
    meta_data=None,
    validation=False,
    save_best_only=True,
    compile_parameters=None,
):
    """
    :: experiment settings

    """
    import tensorflow as tf
    import keras

    if y_range is None:
        y_range = [0, 1]

    # get model configuration
    # model_initial_weights = clf.get_weights()

    # set compile parameters
    if compile_parameters is None:
        compile_parameters = dict(
            loss="sparse_categorical_crossentropy",
            optimizer="adam",
            metrics=["accuracy"],
        )

    if meta_data is not None:
        if len(meta_data) != len(binary_data):
            raise Exception(
                "Length mismatch 'len(meta_data) != len(binary_data)'. `meta_data` and `binary_data` should use the same indexer."
            )
        data = meta_data
    else:
        data = binary_data

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + "-" + str(x[y_label]), axis=1)
    p_label = "yhat" if not continuous else "p"

    encode = lambda df, cols: encode_dataframe(
        df,
        output_columns=cols,
        initially_drop_columns=["age"] if meta_data is not None else None,
        drop_first=False,
        add_non_dummified_columns=[z_label],
        ignore_cols=[y_label],
        finally_drop_columns=[] if cols == None else None,
        return_output_columns=cols == None,
    )

    def one_iter(batch_no):

        if meta_data is not None:
            # clone and compile model
            clf_cloned = tf.keras.models.clone_model(clf)
            # clf_cloned.set_weights(model_initial_weights)
            clf_cloned.compile(**compile_parameters)
        else:
            clf_cloned = clf

        if random_state is None:
            batch_random_state = batch_no
        else:
            batch_random_state = random_state

        if balanced:
            if meta_data is not None:
                raise NotImplementedError(
                    "Cannot have both `balanced` and `meta_data`."
                )
            _data = balance_dataset(
                data, z_label, y_label, random_state=batch_random_state
            )
        else:
            _data = data

        metrics = []

        # validation
        if validation:
            main_indices, holdout_indices = train_test_split(
                _data.index, test_size=validation, random_state=batch_random_state
            )
        else:
            main_indices = _data.index

        # train/test split
        train_idx, test_idx = train_test_split(
            main_indices,
            test_size=0.5,
            stratify=_data.loc[main_indices]["strat"],
            random_state=batch_random_state,
        )

        train_df = _data.loc[train_idx]
        test_df = _data.loc[test_idx]
        valid_df = _data.loc[holdout_indices]

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)

        # separate features and labels
        if meta_data is None:
            X_train, y_train = (
                train_df.drop([y_label, z_label, "strat"], axis=1),
                train_df[y_label],
            )
            X_test, y_test = (
                test_df.drop([y_label, z_label, "strat"], axis=1),
                test_df[y_label],
            )
        else:
            X_train = binary_data[train_idx]
            y_train = train_df[y_label]
            X_test = binary_data[test_idx]
            y_test = test_df[y_label]

        fit_args_ = (
            dict(
                shuffle=True,
                sensitive_features=train_df[z_label],
                **{k: v for k, v in fit_args.items() if k != "sensitive"}
            )
            if fit_args.get("sensitive", False)
            else fit_args
        )
        if validation:
            fit_args_["validation_data"] = (
                binary_data[holdout_indices],
                valid_df[y_label],
            )
        if checkpoint:
            if not fit_args_.get("callbacks"):
                fit_args_["callbacks"] = []
            fit_args_["callbacks"].append(
                keras.callbacks.ModelCheckpoint(
                    filepath=checkpoint.format(batch_no),
                    monitor="val_loss",
                    save_best_only=save_best_only,
                )
            )

        tf.random.set_seed(batch_random_state)
        clf_cloned.fit(X_train, y_train, **fit_args_)
        if checkpoint:
            clf_cloned.load_weights(checkpoint.format(batch_no))

        if diagnostic == 4:
            return clf_cloned

        # save confidences to train/test dataframes
        predict_args_ = (
            lambda df: dict(sensitive_features=df[z_label])
            if fit_args.get("sensitive", False)
            else dict()
        )

        train_df.loc[:, "p"] = clf_cloned.predict_proba(
            X_train, **predict_args_(train_df)
        )[:, 1]
        test_df.loc[:, "p"] = clf_cloned.predict_proba(
            X_test, **predict_args_(test_df)
        )[:, 1]

        # save predicted labels to train/test dataframes
        decisions_train = clf_cloned.predict(X_train, **predict_args_(train_df))
        decisions_test = clf_cloned.predict(X_test, **predict_args_(test_df))
        if synthetic_bins is not None:
            bins = np.histogram_bin_edges(
                train_df.p, bins=np.linspace(0, 1, synthetic_bins + 1)
            )
            # print(bins)
            train_df.loc[:, "yhat"] = np.digitize(train_df.p.values, bins)
            test_df.loc[:, "yhat"] = np.digitize(test_df.p.values, bins)
        else:
            train_df.loc[:, "yhat"] = decisions_train
            test_df.loc[:, "yhat"] = decisions_test

        if diagnose_calibration:
            print(bins)
            train_df.yhat.hist()
            plt.show()

        # measure target model accuracy
        if meta_data is not None:
            target_train_acc = accuracy_score(
                train_df[y_label].values, np.argmax(decisions_train, axis=1)
            )
            target_test_acc = accuracy_score(
                test_df[y_label].values, np.argmax(decisions_test, axis=1)
            )
        else:
            target_train_acc = accuracy_score(train_df[y_label], decisions_train)
            target_test_acc = accuracy_score(test_df[y_label], decisions_test)

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(
            train_df,
            test_size=0.3,
            stratify=train_df["strat"],
            random_state=batch_random_state,
        )

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(
            test_df,
            test_size=0.3,
            stratify=test_df["strat"],
            random_state=batch_random_state,
        )

        if diagnostic == 1:
            return train_learn_df, train_eval_df, test_learn_df, test_eval_df

        learn_df = train_learn_df.append(test_learn_df)
        eval_df = train_eval_df.append(test_eval_df)
        full_df = learn_df.append(eval_df)
        for z_val in z_values + [None]:
            if z_val is not None:
                support = len(full_df.loc[full_df[z_label] == z_val])
            else:
                support = len(full_df)

            disc_vuln = est_vuln_counts(
                train_learn_df,
                train_eval_df,
                test_learn_df,
                test_eval_df,
                p_label="yhat",
                z_label=z_label,
                z_values=z_values,
                z_value=z_val,
                y_label=y_label,
                attacker="discriminating",
                y_range=y_range,
                p_range=sorted(train_df.yhat.unique()),
                full_set_eval=True,
            )
            reg_vuln = est_vuln_counts(
                train_learn_df,
                train_eval_df,
                test_learn_df,
                test_eval_df,
                p_label="yhat",
                z_label=z_label,
                z_values=z_values,
                z_value=z_val,
                y_label=y_label,
                attacker="regular",
                y_range=y_range,
                p_range=sorted(train_df.yhat.unique()),
                full_set_eval=True,
            )

            # fbleau.
            # disc_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                             test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values,
            #                             z_value=z_val,
            #                             y_label=y_label, attacker="discriminating")
            # reg_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                            test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values,
            #                            z_value=z_val,
            #                            y_label=y_label, attacker="regular")

            for vuln in disc_vuln:
                metrics.append(
                    (
                        z_val or "Overall",
                        "counts",
                        batch_no,
                        "discriminating",
                        target_train_acc,
                        target_test_acc,
                        vuln,
                        support,
                    )
                )
            for vuln in reg_vuln:
                metrics.append(
                    (
                        z_val or "Overall",
                        "counts",
                        batch_no,
                        "regular",
                        target_train_acc,
                        target_test_acc,
                        vuln,
                        support,
                    )
                )
        # Clean up
        tf.keras.backend.clear_session()
        del clf_cloned

        return metrics

    if diagnostic == 1:
        return one_iter(num_batches)
    elif diagnostic == 2:
        results = one_iter(num_batches)
    elif diagnostic == 3:
        return
    elif diagnostic == 4:
        return one_iter(num_batches)
    else:
        results = []
        for i in tqdm(range(num_batches), leave=False):
            results += one_iter(i)

    return pd.DataFrame(
        results,
        columns=[
            "subgroup",
            "vuln_method",
            "batch_no",
            "attacker",
            "target_train_acc",
            "target_test_acc",
            "vuln",
            "support",
        ],
    )

if __name__ == "__main__":
    data_undummified = prepare_compas("data/compas/compas-scores-two-years.csv", dummified=False, drop_first=True,
                                      drop_duplicates=False)

    fit_args = {"fair_lr": dict(sensitive_col_idx=[10], correlation_tolerance=0.2)}

    metrics_gender = {
        model_name: run_compas_experiment(data_undummified, z_label="Gender", z_values=["Male", "Female"],
                                          num_batches=1,
                                          clf=clf, fit_args=fit_args.get(model_name, {})) for model_name, clf in
        model_zoo.items()}
