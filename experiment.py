import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from tqdm import tqdm_notebook

from tqdm import tqdm_notebook as tqdm
import sys

sys.path.append("../mia")

from compas import prepare_compas
from helpers import *
from fixes import model_zoo


def calculate_discriminating(train_df, test_df, eval_df, z_label, z_value, y_label, y_value, yhat_value):
    trn_df = train_df.loc[
        (train_df[z_label] == z_value) & (train_df[y_label] == y_value) & (train_df["yhat"] == yhat_value)]
    tst_df = test_df.loc[
        (test_df[z_label] == z_value) & (test_df[y_label] == y_value) & (test_df["yhat"] == yhat_value)]
    evl_z_df = eval_df.loc[(eval_df[z_label] == z_value)]
    evl_df = eval_df.loc[
        (eval_df[z_label] == z_value) & (eval_df[y_label] == y_value) & (eval_df["yhat"] == yhat_value)]

    try:
        vul = max([len(tst_df), len(trn_df)]) / float(len(trn_df) + len(tst_df))
    except ZeroDivisionError:
        vul = 1

    # vulnerability
    prob = len(evl_df) / float(len(evl_z_df))
    subgroup_vuln = vul * prob

    # # overfitting factor \epsilon
    # evl_z_y = eval_df.loc[(eval_df[z_label] == z_value) & (eval_df[y_label] == y_value)]
    # eps = vul - (len(evl_df)) / float(len(evl_z_y))
    #
    # # unfairness factor \phi
    # evl_p_y = eval_df.loc[(eval_df["yhat"] == yhat_value) & (eval_df[y_label] == y_value)]
    # phi = len(trn_df) / float(len(evl_p_y))

    return subgroup_vuln


def calculate_regular(train_df, test_df, eval_df, z_label, z_value, y_label, y_value, yhat_value):
    trn_df = train_df.loc[(train_df[y_label] == y_value) & (train_df["yhat"] == yhat_value)]
    test_df = test_df.loc[(test_df[y_label] == y_value) & (test_df["yhat"] == yhat_value)]
    eval_z_df = eval_df.loc[(eval_df[z_label] == z_value)]
    eval_z_y_yhat_df = eval_df.loc[
        (eval_df[z_label] == z_value) & (eval_df[y_label] == y_value) & (eval_df["yhat"] == yhat_value)]

    # vulnerability
    try:
        vul = max([len(test_df), len(train_df)]) / float(len(train_df) + len(test_df))
    except ZeroDivisionError:
        vul = 1

    prob = len(eval_z_y_yhat_df) / float(len(eval_z_df))
    subgroup_vuln = vul * prob

    return subgroup_vuln


def run_paper_experiment(data, z_label, z_values, num_batches=1, classifier=None, fit_args=None):
    """
    :: experimnet settings

    """

    data["strat"] = data.apply(lambda x: x[z_label] + '-' + x["Ground Truth"], axis=1)

    metrics = []
    for batch_no in tqdm(range(num_batches)):
        learning_df, evaluation_df = train_test_split(data, test_size=0.3, stratify=data["strat"])

        train_df, test_df = train_test_split(learning_df, test_size=0.5, stratify=learning_df["strat"])
        train_df = train_df.copy()
        test_df = test_df.copy()

        # Learning
        y_label = "Ground Truth_Will not reoffend"
        encode = lambda df, cols: encode_dataframe(df, drop_first=False,
                                                   output_columns=cols,
                                                   initially_drop_columns=["strat"],
                                                   add_non_dummified_columns=[z_label],
                                                   finally_drop_columns=["Ground Truth_Will reoffend", 'Gender_Female',
                                                                         'Felony or Misdemeanor_Misdemeanor'] if cols == None else None,
                                                   return_output_columns=cols == None)

        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)
        eval_df = encode(evaluation_df, cols)

        X_train, y_train = train_df.drop([y_label, z_label], axis=1), train_df[y_label]
        X_test, y_test = test_df.drop([y_label, z_label], axis=1), test_df[y_label]
        X_eval = eval_df.drop([y_label, z_label], axis=1)

        if classifier is None:
            clf = LogisticRegression(solver="lbfgs")
        else:
            clf = classifier

        clf.fit(X_train, y_train, **fit_args)

        train_df.loc[:, "p"] = clf.predict_proba(X_train)[:, 0]
        test_df.loc[:, "p"] = clf.predict_proba(X_test)[:, 0]
        eval_df.loc[:, "p"] = clf.predict_proba(X_eval)[:, 0]

        train_df.loc[:, "yhat"] = clf.predict(X_train)
        test_df.loc[:, "yhat"] = clf.predict(X_test)
        eval_df.loc[:, "yhat"] = clf.predict(X_eval)

        # measures
        target_train_acc = accuracy_score(train_df[y_label], train_df["yhat"])
        target_test_acc = accuracy_score(test_df[y_label], test_df["yhat"])

        for z_val in z_values:
            # fp metrics
            metrics += [(z_val, 'fp', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         calculate_discriminating(train_df, test_df, eval_df,
                                                  z_label=z_label,
                                                  z_value=z_val,
                                                  y_label=y_label,
                                                  y_value=0,
                                                  yhat_value=1)),
                        (z_val, 'fp', batch_no, 'regular', target_train_acc, target_test_acc,
                         calculate_regular(train_df, test_df, eval_df,
                                           z_label=z_label,
                                           z_value=z_val,
                                           y_label=y_label,
                                           y_value=0,
                                           yhat_value=1))
                        ]

            # tp metrics
            metrics += [(z_val, 'tp', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         calculate_discriminating(train_df, test_df, eval_df,
                                                  z_label=z_label,
                                                  z_value=z_val,
                                                  y_label=y_label,
                                                  y_value=1,
                                                  yhat_value=1)),
                        (z_val, 'tp', batch_no, 'regular', target_train_acc, target_test_acc,
                         calculate_regular(train_df, test_df, eval_df,
                                           z_label=z_label,
                                           z_value=z_val,
                                           y_label=y_label,
                                           y_value=1,
                                           yhat_value=1))]

            # fn metrics
            metrics += [(z_val, 'fn', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         calculate_discriminating(train_df, test_df, eval_df,
                                                  z_label=z_label,
                                                  z_value=z_val,
                                                  y_label=y_label,
                                                  y_value=1,
                                                  yhat_value=0)),
                        (z_val, 'fn', batch_no, 'regular', target_train_acc, target_test_acc,
                         calculate_regular(train_df, test_df, eval_df,
                                           z_label=z_label,
                                           z_value=z_val,
                                           y_label=y_label,
                                           y_value=1,
                                           yhat_value=0))]

            # tn metrics
            metrics += [(z_val, 'tn', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         calculate_discriminating(train_df, test_df, eval_df,
                                                  z_label=z_label,
                                                  z_value=z_val,
                                                  y_label=y_label,
                                                  y_value=0,
                                                  yhat_value=0)),
                        (z_val, 'tn', batch_no, 'regular', target_train_acc, target_test_acc,
                         calculate_regular(train_df, test_df, eval_df,
                                           z_label=z_label,
                                           z_value=z_val,
                                           y_label=y_label,
                                           y_value=0,
                                           yhat_value=0))]

    return pd.DataFrame(metrics,
                        columns=["subgroup", "metric", "batch_no", "attacker", "target_train_acc", "target_test_acc",
                                 "subgroup_vuln"])


if __name__ == "__main__":
    data_undummified = prepare_compas("data/compas/compas-scores-two-years.csv", dummified=False, drop_first=True,
                                      drop_duplicates=False)

    fit_args = {"fair_lr": dict(sensitive_col_idx=[10], correlation_tolerance=0.2)}

    metrics_gender = {
        model_name: run_paper_experiment(data_undummified, z_label="Gender", z_values=["Male", "Female"], num_batches=1,
                                         classifier=clf, fit_args=fit_args.get(model_name, {})) for model_name, clf in
        model_zoo.items()}
