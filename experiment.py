import joblib
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from tqdm import tqdm_notebook as tqdm
import sys
from functools import reduce
from fbleau.fbleau import calc_vulnerability as fbleau_vuln

sys.path.append("../mia")

from compas import prepare_compas
from helpers import encode_dataframe, prob_operator, joint_prob
from fixes import model_zoo


def calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df, z_label, z_value, y_label,
                             y_value, yhat_value):
    prob = lambda *args, **kwargs: prob_operator(y_label, y_value, yhat_value, z_label, z_value, *args, **kwargs)

    learn_df_ = train_learn_df.assign(m=1).append(test_learn_df.assign(m=0))

    try:
        vuln = np.max([prob(learn_df_, "m", condition="yhat y z", m_value=0),
                       prob(learn_df_, "m", condition="yhat y z", m_value=1)])
    except ZeroDivisionError:
        vuln = 0.5

    # vulnerability
    eval_df = train_eval_df.append(test_eval_df)
    subgroup_vuln = vuln * prob(eval_df, "yhat y", condition="z")

    # overfitting factor \epsilon
    p_train_learn = prob(train_learn_df, "yhat", condition="y z")
    p_test_learn = prob(test_learn_df, "yhat", condition="y z")
    eps_learn = max(p_train_learn, p_test_learn) - np.mean([p_train_learn, p_test_learn])

    p_train_eval = prob(train_eval_df, "yhat", condition="y z")
    p_test_eval = prob(test_eval_df, "yhat", condition="y z")

    try:
        eps_eval = max(p_train_eval, p_test_eval) - np.mean([p_train_eval, p_test_eval])
    except TypeError:
        eps_eval = None

    # unfairness factor \phi
    learn_df = train_learn_df.append(test_learn_df)

    try:
        phi_learn = prob(learn_df, "yhat", condition="y z") / prob(learn_df, "yhat", condition="y")
    except ZeroDivisionError:
        phi_learn = np.inf

    try:
        phi_eval = prob(eval_df, "yhat", condition="y z") / prob(eval_df, "yhat", condition="y")
    except ZeroDivisionError:
        phi_eval = np.inf

    return subgroup_vuln, eps_learn, eps_eval, phi_learn, phi_eval


def calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df, z_label, z_value, y_label, y_value,
                      yhat_value):
    prob = lambda *args, **kwargs: prob_operator(y_label, y_value, yhat_value, z_label, z_value, *args, **kwargs)

    try:
        vuln = np.max([prob(train_learn_df, "yhat", condition="y"), prob(test_learn_df, "yhat", condition="y")])
    except ZeroDivisionError:
        vuln = 0.5  # it's a probability

    # vulnerability
    eval_df = train_eval_df.append(test_eval_df)

    # print(eval_df.query(f"yhat == {yhat_value} & {y_label} == {y_value}")[z_label].value_counts())
    # print(eval_df.query(f"yhat == {yhat_value} & {y_label} == {y_value} & {z_label} == '{z_value}'"))
    # print(prob(eval_df, "yhat y", "z"))

    subgroup_vuln = vuln * prob(eval_df, "yhat y", condition="z")

    # overfitting factor \epsilon
    p_train_learn = prob(train_learn_df, "yhat", condition="y")
    p_test_learn = prob(test_learn_df, "yhat", condition="y")
    eps_learn = max(p_train_learn, p_test_learn) - np.mean([p_train_learn, p_test_learn])

    p_train_eval = prob(train_eval_df, "yhat", condition="y")
    p_test_eval = prob(test_eval_df, "yhat", condition="y")
    eps_eval = max(p_train_eval, p_test_eval) - np.mean([p_train_eval, p_test_eval])

    # unfairness factor \phi
    learn_df = train_learn_df.append(test_learn_df)
    try:
        phi_learn = prob(learn_df, "yhat", condition="y z") / prob(learn_df, "yhat", condition="y")
    except ZeroDivisionError:
        phi_learn = np.inf

    try:
        phi_eval = prob(eval_df, "yhat", condition="y z") / prob(eval_df, "yhat", condition="y")
    except ZeroDivisionError:
        phi_eval = np.inf

    return subgroup_vuln, eps_learn, eps_eval, phi_learn, phi_eval


def run_compas_experiment(data, z_label, z_values, clf, fit_args=None, num_batches=1):
    """
    :: experiment settings

    """

    # setup
    # if classifier is None:
    #     clf = LogisticRegression(solver="lbfgs")
    # else:
    #     clf = classifier

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + '-' + x["Ground Truth"], axis=1)

    y_label = "Ground Truth_Will not reoffend"
    encode = lambda df, cols: encode_dataframe(df, drop_first=False,
                                               output_columns=cols,
                                               initially_drop_columns=None,
                                               add_non_dummified_columns=[z_label],
                                               finally_drop_columns=['Ground Truth_Will reoffend', 'Gender_Female',
                                                                     'Felony or Misdemeanor_Misdemeanor'] if cols == None else None,
                                               return_output_columns=cols == None)

    metrics = []
    for batch_no in tqdm(range(num_batches)):

        # train/test split
        train_df, test_df = train_test_split(data, test_size=0.5, stratify=data["strat"])

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)

        # separate features and labels
        X_train, y_train = train_df.drop([y_label, z_label, "strat"], axis=1), train_df[y_label]
        X_test, y_test = test_df.drop([y_label, z_label, "strat"], axis=1), test_df[y_label]

        # print(X_train.columns)

        fit_args_ = dict(sensitive_features=train_df[z_label],
                         **{k: v for k, v in fit_args.items() if k != "sensitive"}) if fit_args.get("sensitive",
                                                                                                    False) else dict()
        clf.fit(X_train, y_train, **fit_args_)

        # save confidences to train/test dataframes
        predict_args_ = lambda df: dict(sensitive_features=df[z_label]) if fit_args.get("sensitive", False) else dict()

        train_df.loc[:, "p"] = clf.predict_proba(X_train, **predict_args_(train_df))[:, 1]
        test_df.loc[:, "p"] = clf.predict_proba(X_test, **predict_args_(test_df))[:, 1]

        # save predicted labels to train/test dataframes
        train_df.loc[:, "yhat"] = clf.predict(X_train, **predict_args_(train_df))
        test_df.loc[:, "yhat"] = clf.predict(X_test, **predict_args_(test_df))

        # measure target model accuracy
        target_train_acc = accuracy_score(train_df[y_label], train_df["yhat"])
        target_test_acc = accuracy_score(test_df[y_label], test_df["yhat"])

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(train_df, test_size=0.3, stratify=train_df["strat"])

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(test_df, test_size=0.3, stratify=test_df["strat"])

        for z_val in z_values:
            # fp metrics
            metrics += [(z_val, 'fp', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=0,
                                                   yhat_value=1)),
                        (z_val, 'fp', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=0,
                                            yhat_value=1))
                        ]

            # tp metrics
            metrics += [(z_val, 'tp', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=1,
                                                   yhat_value=1)),
                        (z_val, 'tp', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=1,
                                            yhat_value=1))]

            # fn metrics
            metrics += [(z_val, 'fn', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=1,
                                                   yhat_value=0)),
                        (z_val, 'fn', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=1,
                                            yhat_value=0))]

            # tn metrics
            metrics += [(z_val, 'tn', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=0,
                                                   yhat_value=0)),
                        (z_val, 'tn', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=0,
                                            yhat_value=0))]

    return pd.DataFrame(metrics,
                        columns=["subgroup", "metric", "batch_no", "attacker", "target_train_acc", "target_test_acc",
                                 "subgroup_vuln", "eps_learn", "eps_eval", "phi_learn", "phi_eval"])


def run_adult_experiment(data, z_label, z_values, clf, clf_name=None, fit_args=None, n_processes=4, num_batches=1, diagnostic=0):
    """
    :: experiment settings

    """

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + '-' + str(x["income"]), axis=1)

    y_label = "income"
    encode = lambda df, cols: encode_dataframe(df, drop_first=False,
                                               output_columns=cols,
                                               initially_drop_columns=None,
                                               add_non_dummified_columns=[z_label],
                                               finally_drop_columns=[] if cols == None else None,
                                               return_output_columns=cols == None)

    def one_iter(batch_no):

        metrics = []
        # train/test split
        train_df, test_df = train_test_split(data, test_size=0.5, stratify=data["strat"], random_state=batch_no)

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)

        # separate features and labels
        X_train, y_train = train_df.drop([y_label, z_label, "strat"], axis=1), train_df[y_label]
        X_test, y_test = test_df.drop([y_label, z_label, "strat"], axis=1), test_df[y_label]

        fit_args_ = dict(sensitive_features=train_df[z_label],
                         **{k: v for k, v in fit_args.items() if k != "sensitive"}) if fit_args.get("sensitive",
                                                                                                    False) else dict()
        clf.fit(X_train, y_train, **fit_args_)

        # save confidences to train/test dataframes
        predict_args_ = lambda df: dict(sensitive_features=df[z_label]) if fit_args.get("sensitive", False) else dict()

        train_df.loc[:, "p"] = clf.predict_proba(X_train, **predict_args_(train_df))[:, 1]
        test_df.loc[:, "p"] = clf.predict_proba(X_test, **predict_args_(test_df))[:, 1]

        # save predicted labels to train/test dataframes
        train_df.loc[:, "yhat"] = clf.predict(X_train, **predict_args_(train_df))
        test_df.loc[:, "yhat"] = clf.predict(X_test, **predict_args_(test_df))

        # measure target model accuracy
        target_train_acc = accuracy_score(train_df[y_label], train_df["yhat"])
        target_test_acc = accuracy_score(test_df[y_label], test_df["yhat"])

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(train_df, test_size=0.3, stratify=train_df["strat"],
                                                         random_state=batch_no)

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(test_df, test_size=0.3, stratify=test_df["strat"],
                                                       random_state=batch_no)

        if diagnostic == 1:
            return train_learn_df, train_eval_df, test_learn_df, test_eval_df
        elif diagnostic == 3:
            learn_df = train_learn_df.assign(m=0).append(test_learn_df.assign(m=1))
            return [
                (joint_prob(learn_df, m=1, yhat=1, **{y_label: 1, z_label: z_values[0]}) /
                 joint_prob(learn_df, yhat=1, **{y_label: 1, z_label: z_values[0]})
                 ),
                joint_prob(learn_df, m=1, yhat=1, **{y_label: 1}) / joint_prob(learn_df, yhat=1, **{y_label: 1})]

        for z_val in z_values:
            # calculate fbleau vulnerability
            fbleau_vuln_discriminating = fbleau_vuln(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                     "discriminating",
                                                     z_values=z_values,
                                                     z_label=z_label,
                                                     y_label=y_label,
                                                     p_label="yhat",
                                                     # estimate_method='--knn=1',
                                                     z_value=z_val)

            fbleau_vuln_regular = fbleau_vuln(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                              "regular",
                                              z_values=z_values,
                                              z_label=z_label,
                                              y_label=y_label,
                                              p_label="yhat",
                                              # estimate_method='--knn=1',
                                              z_value=z_val)

            # metrics
            metrics += [
                (z_val, batch_no, 'discriminating', target_train_acc, target_test_acc, fbleau_vuln_discriminating),
                (z_val, batch_no, 'regular', target_train_acc, target_test_acc, fbleau_vuln_regular)
            ]

        # print(metrics[0])
        return metrics

    if diagnostic == 1:
        return one_iter(num_batches)
    elif diagnostic == 2:
        results = one_iter(num_batches)
    elif diagnostic == 3:
        return
    else:
        if clf_name is not None:
            if "nn" in clf_name:
                n_jobs = 1
            else:
                n_jobs = n_processes
        else:
            n_jobs = n_processes

        with joblib.parallel_backend('dask'):
            results = joblib.Parallel(n_jobs=n_jobs, verbose=100)(
                joblib.delayed(one_iter)(i) for i in range(num_batches))
            results = reduce(lambda list1, list2: [*list1, *list2], results)

    # flatten list of lists
    print(results)

    return pd.DataFrame(results,
                        columns=["subgroup", "batch_no", "attacker",
                                 "target_train_acc", "target_test_acc",
                                 "fbleau_vuln"])


if __name__ == "__main__":
    data_undummified = prepare_compas("data/compas/compas-scores-two-years.csv", dummified=False, drop_first=True,
                                      drop_duplicates=False)

    fit_args = {"fair_lr": dict(sensitive_col_idx=[10], correlation_tolerance=0.2)}

    metrics_gender = {
        model_name: run_compas_experiment(data_undummified, z_label="Gender", z_values=["Male", "Female"],
                                          num_batches=1,
                                          clf=clf, fit_args=fit_args.get(model_name, {})) for model_name, clf in
        model_zoo.items()}