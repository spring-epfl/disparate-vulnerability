import sys
import joblib
import pandas as pd
import numpy as np
import tensorflow as tf

from functools import reduce
from itertools import product
from warnings import warn

from matplotlib import pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from tqdm.notebook import tqdm

from utils import encode_dataframe, balance_dataset, reset_weights
from mia.vuln import estimate_vulnerability as est_vuln_counts


def run_generic_keras_experiment(
    binary_data,
    y_label,
    z_label,
    z_values,
    clf,
    validation=None,
    train_valid_test_idx=None,
    clf_name=None,
    fit_args=None,
    num_batches=1,
    synthetic_bins=10,
    diagnose_calibration=False,
    diagnostic=0,
    balanced=False,
    estimation_method="frequentist",
    continuous=False,
    checkpoint=None,
    y_range=None,
    random_state=None,
    meta_data=None,
    save_best_only=True,
    compile_parameters=None,
    output_extended_subgroup_metrics=False,
):
    import tensorflow as tf

    if y_range is None:
        y_range = [0, 1]

    # get model configuration
    # model_initial_weights = clf.get_weights()

    # set compile parameters
    if compile_parameters is None:
        compile_parameters = dict(
            loss="sparse_categorical_crossentropy",
            optimizer="adam",
            metrics=["accuracy"],
        )

    if meta_data is not None:
        if len(meta_data) != len(binary_data):
            raise Exception(
                "Length mismatch 'len(meta_data) != len(binary_data)'. `meta_data` and `binary_data` should use the same indexer."
            )
        data = meta_data
    else:
        data = binary_data

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + "-" + str(x[y_label]), axis=1)
    p_label = "yhat" if not continuous else "p"

    encode = lambda df, cols: encode_dataframe(
        df,
        output_columns=cols,
        initially_drop_columns=["age"] if meta_data is not None else None,
        drop_first=False,
        add_non_dummified_columns=[z_label],
        ignore_cols=[y_label],
        finally_drop_columns=[] if cols == None else None,
        return_output_columns=cols == None,
    )

    def one_iter(batch_no):

        if meta_data is not None:
            # clone and compile model
            clf_cloned = tf.keras.models.clone_model(clf)
            # clf_cloned.set_weights(model_initial_weights)
            clf_cloned.compile(**compile_parameters)
        else:
            clf_cloned = clf

        if random_state is None:
            batch_random_state = batch_no
        else:
            batch_random_state = random_state

        if balanced:
            if meta_data is not None:
                raise NotImplementedError(
                    "Cannot have both `balanced` and `meta_data`."
                )
            _data = balance_dataset(
                data, z_label, y_label, random_state=batch_random_state
            )
        else:
            _data = data

        metrics = []

        if validation is not None and train_valid_test_idx is not None:
            raise Exception(
                "Either `validation` (ratio/number of validation samples) "
                "or `train_valid_test_idx` should be specified, not both."
            )

        if validation is not None:
            main_indices, holdout_indices = train_test_split(
                _data.index, test_size=validation, random_state=batch_random_state
            )
            valid_idx = holdout_indices

            # train/test split
            train_idx, test_idx = train_test_split(
                main_indices,
                test_size=0.5,
                stratify=_data.loc[main_indices]["strat"],
                random_state=batch_random_state,
            )
        elif train_valid_test_idx is not None:
            train_idx, valid_idx, test_idx = train_valid_test_idx
        else:
            main_indices = _data.index
            holdout_indices = []

        train_df = _data.loc[train_idx]
        test_df = _data.loc[test_idx]
        valid_df = _data.loc[valid_idx]

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)
        valid_df = encode(valid_df, cols)

        # separate features and labels
        if meta_data is None:
            X_train, y_train = (
                train_df.drop([y_label, z_label, "strat"], axis=1),
                train_df[y_label],
            )
            X_test, y_test = (
                test_df.drop([y_label, z_label, "strat"], axis=1),
                test_df[y_label],
            )
        else:
            # pdb.set_trace()
            X_train = binary_data[train_idx]
            y_train = train_df[y_label].values.codes
            X_test = binary_data[test_idx]
            y_test = test_df[y_label].values.codes

        fit_args_ = (
            dict(
                shuffle=True,
                sensitive_features=train_df[z_label],
                **{k: v for k, v in fit_args.items() if k != "sensitive"},
            )
            if fit_args.get("sensitive", False)
            else fit_args
        )

        X_valid = binary_data[valid_idx]
        y_valid = valid_df[y_label].values.codes
        fit_args_["validation_data"] = (X_valid, y_valid)

        if checkpoint:
            if not fit_args_.get("callbacks"):
                fit_args_["callbacks"] = []
            fit_args_["callbacks"].append(
                tf.keras.callbacks.ModelCheckpoint(
                    filepath=checkpoint.format(batch_no),
                    monitor="val_loss",
                    save_best_only=save_best_only,
                )
            )

        # fit_args_["callbacks"].append(tqdm_callback)

        try:
            tf.random.set_seed(batch_random_state)
        except AttributeError:
            print("WARN: Using Tensorflow < 2.0")
            tf.compat.v1.set_random_seed(1)

        # Learn
        clf_cloned.fit(X_train, y_train, verbose=0, **fit_args_)
        if checkpoint:
            clf_cloned.load_weights(checkpoint.format(batch_no))

        if diagnostic == 4:
            return clf_cloned

        # save confidences to train/test dataframes
        predict_args_ = (
            lambda df: dict(sensitive_features=df[z_label])
            if fit_args.get("sensitive", False)
            else dict()
        )

        # Inference
        # Combining train and validation
        # pdb.set_trace()
        train_df = pd.concat([train_df, valid_df])
        X_train = np.r_[X_train, X_valid]

        train_df.loc[:, "p"] = clf_cloned.predict_proba(
            X_train, **predict_args_(train_df)
        )[:, 1]
        test_df.loc[:, "p"] = clf_cloned.predict_proba(
            X_test, **predict_args_(test_df)
        )[:, 1]

        # save predicted labels to train/test dataframes
        decisions_train = clf_cloned.predict(X_train, **predict_args_(train_df))
        decisions_test = clf_cloned.predict(X_test, **predict_args_(test_df))

        # save decisions for subgroup overfitting
        # pdb.set_trace()
        train_df.loc[:, "_decision"] = np.argmax(decisions_train, axis=1)
        test_df.loc[:, "_decision"] = np.argmax(decisions_test, axis=1)

        if synthetic_bins is not None:
            bins = np.histogram_bin_edges(
                train_df.p, bins=np.linspace(0, 1, synthetic_bins + 1)
            )
            # print(bins)
            train_df.loc[:, "yhat"] = np.digitize(train_df.p.values, bins)
            test_df.loc[:, "yhat"] = np.digitize(test_df.p.values, bins)
        else:
            train_df.loc[:, "yhat"] = decisions_train
            test_df.loc[:, "yhat"] = decisions_test

        if diagnose_calibration:
            print(bins)
            train_df.yhat.hist()
            plt.show()

        # measure target model accuracy
        if meta_data is not None:
            target_train_acc = accuracy_score(
                y_train, np.argmax(clf_cloned.predict(binary_data[train_idx]), axis=1)
            )
            target_test_acc = accuracy_score(y_test, np.argmax(decisions_test, axis=1))
            # print("Accuracy (train/test):", target_train_acc, target_test_acc)
        else:
            target_train_acc = accuracy_score(train_df[y_label], decisions_train)
            target_test_acc = accuracy_score(test_df[y_label], decisions_test)

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(
            train_df,
            test_size=0.3,
            stratify=train_df["strat"],
            random_state=batch_random_state,
        )

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(
            test_df,
            test_size=0.3,
            stratify=test_df["strat"],
            random_state=batch_random_state,
        )

        if diagnostic == 1:
            return train_learn_df, train_eval_df, test_learn_df, test_eval_df

        learn_df = train_learn_df.append(test_learn_df)
        eval_df = train_eval_df.append(test_eval_df)
        full_df = learn_df.append(eval_df)

        for z_val in z_values + [None]:
            if z_val is not None:
                # Subgroup metrics
                support = len(full_df.loc[full_df[z_label] == z_val])

                subgroup_train_df = train_df.loc[train_df[z_label] == z_val]
                subgroup_test_df = test_df.loc[test_df[z_label] == z_val]

                # measure subgroup target model accuracy
                target_train_acc = accuracy_score(
                    subgroup_train_df[y_label], subgroup_train_df["_decision"]
                )
                target_test_acc = accuracy_score(
                    subgroup_test_df[y_label], subgroup_test_df["_decision"]
                )

                if output_extended_subgroup_metrics:
                    # collect confusion matrix measures
                    # pdb.set_trace()
                    train_confusion_mat = confusion_matrix(
                        subgroup_train_df[y_label], subgroup_train_df["_decision"]
                    )
                    test_confusion_mat = confusion_matrix(
                        subgroup_test_df[y_label], subgroup_test_df["_decision"]
                    )

            else:
                # Overall metrics
                support = len(full_df)

                # measure overall target model accuracy
                target_train_acc = accuracy_score(
                    train_df[y_label].values, np.argmax(decisions_train, axis=1)
                )
                target_test_acc = accuracy_score(
                    test_df[y_label].values, np.argmax(decisions_test, axis=1)
                )

                if output_extended_subgroup_metrics:
                    # collect confusion matrix measures
                    train_confusion_mat = confusion_matrix(
                        train_df[y_label], np.argmax(decisions_train, axis=1)
                    )
                    test_confusion_mat = confusion_matrix(
                        test_df[y_label], np.argmax(decisions_test, axis=1)
                    )

            disc_vuln = est_vuln_counts(
                train_learn_df,
                train_eval_df,
                test_learn_df,
                test_eval_df,
                p_label="yhat",
                z_label=z_label,
                z_values=z_values,
                z_value=z_val,
                y_label=y_label,
                attacker="discriminating",
                y_range=y_range,
                p_range=sorted(train_df.yhat.unique()),
                full_set_eval=True,
            )
            reg_vuln = est_vuln_counts(
                train_learn_df,
                train_eval_df,
                test_learn_df,
                test_eval_df,
                p_label="yhat",
                z_label=z_label,
                z_values=z_values,
                z_value=z_val,
                y_label=y_label,
                attacker="regular",
                y_range=y_range,
                p_range=sorted(train_df.yhat.unique()),
                full_set_eval=True,
            )

            # fbleau.
            # disc_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                             test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values,
            #                             z_value=z_val,
            #                             y_label=y_label, attacker="discriminating")
            # reg_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                            test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values,
            #                            z_value=z_val,
            #                            y_label=y_label, attacker="regular")

            if not output_extended_subgroup_metrics:
                for vuln in disc_vuln:
                    metrics.append(
                        (
                            z_val or "Overall",
                            "counts",
                            batch_no,
                            "discriminating",
                            target_train_acc,
                            target_test_acc,
                            vuln,
                            support,
                        )
                    )
                for vuln in reg_vuln:
                    metrics.append(
                        (
                            z_val or "Overall",
                            "counts",
                            batch_no,
                            "regular",
                            target_train_acc,
                            target_test_acc,
                            vuln,
                            support,
                        )
                    )
            else:
                for vuln in disc_vuln:
                    metrics.append(
                        (
                            z_val or "Overall",
                            "counts",
                            batch_no,
                            "discriminating",
                            target_train_acc,
                            target_test_acc,
                            vuln,
                            support,
                            train_confusion_mat,
                            test_confusion_mat,
                        )
                    )
                for vuln in reg_vuln:
                    metrics.append(
                        (
                            z_val or "Overall",
                            "counts",
                            batch_no,
                            "regular",
                            target_train_acc,
                            target_test_acc,
                            vuln,
                            support,
                            train_confusion_mat,
                            test_confusion_mat,
                        )
                    )
        # Clean up
        tf.keras.backend.clear_session()
        del clf_cloned

        return metrics

    if diagnostic == 1:
        return one_iter(num_batches)
    elif diagnostic == 2:
        results = one_iter(num_batches)
    elif diagnostic == 3:
        return
    elif diagnostic == 4:
        return one_iter(num_batches)
    else:
        results = []
        for i in tqdm(range(num_batches), leave=False):
            results += one_iter(i)

    if not output_extended_subgroup_metrics:
        return pd.DataFrame(
            results,
            columns=[
                "subgroup",
                "vuln_method",
                "batch_no",
                "attacker",
                "target_train_acc",
                "target_test_acc",
                "vuln",
                "support",
            ],
        )
    else:
        return pd.DataFrame(
            results,
            columns=[
                "subgroup",
                "vuln_method",
                "batch_no",
                "attacker",
                "target_train_acc",
                "target_test_acc",
                "vuln",
                "support",
                "train_confusion_mat",
                "test_confusion_mat",
            ],
        )


def run_generic_experiment(
    data,
    y_label,
    z_label,
    z_values,
    clf,
    clf_name=None,
    fit_args=None,
    n_processes=4,
    num_batches=1,
    diagnostic=0,
    full_set_eval=True,
    balanced=False,
    parallelize=False,
    synthetic_bins=10,
    diagnose_calibration=False,
    seed=0,
):

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + "-" + str(x[y_label]), axis=1)
    p_label = "yhat"

    encode = lambda df, cols: encode_dataframe(
        df,
        output_columns=cols,
        initially_drop_columns=None,
        drop_first=False,
        add_non_dummified_columns=[z_label],
        ignore_cols=[y_label],
        finally_drop_columns=[] if cols is None else None,
        return_output_columns=cols is None,
    )

    def one_iter(batch_no):
        # batch_seed = batch_no + seed
        batch_seed = batch_no + seed
        if balanced:
            _data = balance_dataset(data, z_label, y_label, random_state=batch_seed)
        else:
            _data = data

        metrics = []
        # train/test split
        train_df, test_df = train_test_split(
            _data, test_size=0.5, stratify=_data["strat"], random_state=batch_seed
        )

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)

        # separate features and labels
        X_train, y_train = (
            train_df.drop([y_label, z_label, "strat"], axis=1),
            train_df[y_label],
        )
        X_test, y_test = (
            test_df.drop([y_label, z_label, "strat"], axis=1),
            test_df[y_label],
        )

        fit_args_ = (
            dict(
                sensitive_features=train_df[z_label],
                **{k: v for k, v in fit_args.items() if k != "sensitive"},
            )
            if fit_args.get("sensitive", False)
            else dict()
        )
        clf.fit(X_train, y_train, **fit_args_)

        # save confidences to train/test dataframes
        predict_args_ = (
            lambda df: dict(sensitive_features=df[z_label])
            if fit_args.get("sensitive", False)
            else dict()
        )

        train_df.loc[:, "p"] = clf.predict_proba(X_train, **predict_args_(train_df))[
            :, 1
        ]
        test_df.loc[:, "p"] = clf.predict_proba(X_test, **predict_args_(test_df))[:, 1]

        # save predicted labels to train/test dataframes
        decisions_train = clf.predict(X_train, **predict_args_(train_df))
        decisions_test = clf.predict(X_test, **predict_args_(test_df))

        # save decisions for subgroup overfitting
        train_df.loc[:, "_decision"] = decisions_train
        test_df.loc[:, "_decision"] = decisions_test

        if synthetic_bins is not None:
            bins = np.histogram_bin_edges(
                train_df.p, bins=np.linspace(0, 1, synthetic_bins + 1)
            )
            # print(bins)
            train_df.loc[:, "yhat"] = np.digitize(train_df.p.values, bins)
            test_df.loc[:, "yhat"] = np.digitize(test_df.p.values, bins)
        else:
            train_df.loc[:, "yhat"] = decisions_train
            test_df.loc[:, "yhat"] = decisions_test

        if diagnose_calibration:
            print(bins)
            print(train_df.loc[:, "yhat"])
            train_df.yhat.hist()
            plt.show()

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(
            train_df, test_size=0.3, stratify=train_df["strat"], random_state=batch_seed
        )

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(
            test_df, test_size=0.3, stratify=test_df["strat"], random_state=batch_seed
        )

        if diagnostic == 1:
            return train_learn_df, train_eval_df, test_learn_df, test_eval_df

        eval_df = train_eval_df.append(test_eval_df)
        for z_val in z_values + [None]:
            if z_val is not None:
                ## Subgroup metrics
                support = len(eval_df.loc[eval_df[z_label] == z_val])
                subgroup_train_df = train_df.loc[train_df[z_label] == z_val]
                subgroup_test_df = test_df.loc[test_df[z_label] == z_val]

                # measure subgroup target model accuracy
                target_train_acc = accuracy_score(
                    subgroup_train_df[y_label], subgroup_train_df["_decision"]
                )
                target_test_acc = accuracy_score(
                    subgroup_test_df[y_label], subgroup_test_df["_decision"]
                )

                # collect confusion matrix measures
                tn_train, fp_train, fn_train, tp_train = confusion_matrix(
                    subgroup_train_df[y_label], subgroup_train_df["_decision"]
                ).ravel()
                tn_test, fp_test, fn_test, tp_test = confusion_matrix(
                    subgroup_test_df[y_label], subgroup_test_df["_decision"]
                ).ravel()

            else:
                ## Overall metrics
                support = len(eval_df)

                # measure overall target model accuracy
                target_train_acc = accuracy_score(train_df[y_label], decisions_train)
                target_test_acc = accuracy_score(test_df[y_label], decisions_test)

                # collect conufsion matrix measures
                tn_train, fp_train, fn_train, tp_train = confusion_matrix(
                    train_df[y_label], decisions_train
                ).ravel()
                tn_test, fp_test, fn_test, tp_test = confusion_matrix(
                    train_df[y_label], decisions_train
                ).ravel()

            # Counts.
            disc_vuln = est_vuln_counts(
                train_learn_df,
                train_eval_df,
                test_learn_df,
                test_eval_df,
                p_label=p_label,
                z_label=z_label,
                z_values=z_values,
                z_value=z_val,
                y_label=y_label,
                attacker="discriminating",
                y_range=[0, 1],
                p_range=sorted(train_df.yhat.unique()),
                full_set_eval=full_set_eval,
            )
            reg_vuln = est_vuln_counts(
                train_learn_df,
                train_eval_df,
                test_learn_df,
                test_eval_df,
                p_label=p_label,
                z_label=z_label,
                z_values=z_values,
                z_value=z_val,
                y_label=y_label,
                attacker="regular",
                y_range=[0, 1],
                p_range=sorted(train_df.yhat.unique()),
                full_set_eval=full_set_eval,
            )

            ## fbleau.
            # disc_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                             test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values,
            #                             z_value=z_val,
            #                             y_label=y_label, attacker="discriminating")
            # reg_vuln = est_vuln_fbleau(train_learn_df, train_eval_df, test_learn_df,
            #                            test_eval_df, p_label="yhat", z_label=z_label, z_values=z_values,
            #                            z_value=z_val,
            #                            y_label=y_label, attacker="regular")

            for vuln in disc_vuln:
                metrics.append(
                    (
                        z_val or "Overall",
                        "counts",
                        batch_no,
                        "discriminating",
                        target_train_acc,
                        target_test_acc,
                        vuln,
                        support,
                        tn_train,
                        fp_train,
                        fn_train,
                        tp_train,
                        tn_test,
                        fp_test,
                        fn_test,
                        tp_test,
                    )
                )
            for vuln in reg_vuln:
                metrics.append(
                    (
                        z_val or "Overall",
                        "counts",
                        batch_no,
                        "regular",
                        target_train_acc,
                        target_test_acc,
                        vuln,
                        support,
                        tn_train,
                        fp_train,
                        fn_train,
                        tp_train,
                        tn_test,
                        fp_test,
                        fn_test,
                        tp_test,
                    )
                )

        return metrics

    if clf_name is not None:
        if "nn" in clf_name:
            n_jobs = int(n_processes / 3)
        else:
            n_jobs = n_processes
    else:
        n_jobs = n_processes

    if parallelize:
        warn("Parallelized operation might result in non-properly randomized outcomes.")

        with joblib.parallel_backend("loky", inner_max_num_threads=4):
            results = joblib.Parallel(n_jobs=n_jobs, verbose=51, prefer="threads")(
                joblib.delayed(one_iter)(i) for i in range(num_batches)
            )
            results = reduce(lambda list1, list2: [*list1, *list2], results)
    else:
        results = []
        for i in tqdm(range(num_batches), leave=False):
            results += one_iter(i)

    return pd.DataFrame(
        results,
        columns=[
            "subgroup",
            "vuln_method",
            "batch_no",
            "attacker",
            "target_train_acc",
            "target_test_acc",
            "vuln",
            "support",
            "tn_train",
            "fp_train",
            "fn_train",
            "tp_train",
            "tn_test",
            "fp_test",
            "fn_test",
            "tp_test",
        ],
    )
