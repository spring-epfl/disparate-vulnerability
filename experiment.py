import joblib
import pandas as pd
import numpy as np
from multiprocess.pool import Pool
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from tqdm import tqdm_notebook as tqdm
from sklearn.linear_model import LogisticRegression
import sys
from functools import reduce
from operator import add

sys.path.append("../mia")

from compas import prepare_compas
from helpers import encode_dataframe, prob_operator
from fixes import model_zoo


def calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df, z_label, z_value, y_label,
                             y_value, yhat_value):
    prob = lambda *args, **kwargs: prob_operator(y_label, y_value, yhat_value, z_label, z_value, *args, **kwargs)

    try:
        vuln = np.max([prob(train_learn_df, "yhat", "y z"), prob(test_learn_df, "yhat", "y z")])
    except ZeroDivisionError:
        vuln = 1  # it's a probability

    # vulnerability
    eval_df = train_eval_df.append(test_eval_df)
    subgroup_vuln = vuln * prob(eval_df, "yhat y", "z")

    # overfitting factor \epsilon
    p_train_learn = prob(train_learn_df, "yhat", "y z")
    p_test_learn = prob(test_learn_df, "yhat", "y z")
    eps_learn = max(p_train_learn, p_test_learn) - np.mean([p_train_learn, p_test_learn])

    p_train_eval = prob(train_eval_df, "yhat", "y z")
    p_test_eval = prob(test_eval_df, "yhat", "y z")

    try:
        eps_eval = max(p_train_eval, p_test_eval) - np.mean([p_train_eval, p_test_eval])
    except TypeError:
        eps_eval = None

    # unfairness factor \phi
    learn_df = train_learn_df.append(test_learn_df)

    try:
        phi_learn = prob(learn_df, "yhat", "y z") / prob(learn_df, "yhat", "y")
    except ZeroDivisionError:
        phi_learn = np.inf

    try:
        phi_eval = prob(eval_df, "yhat", "y z") / prob(eval_df, "yhat", "y")
    except ZeroDivisionError:
        phi_eval = np.inf

    return subgroup_vuln, eps_learn, eps_eval, phi_learn, phi_eval


def calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df, z_label, z_value, y_label, y_value,
                      yhat_value):
    prob = lambda *args, **kwargs: prob_operator(y_label, y_value, yhat_value, z_label, z_value, *args, **kwargs)

    try:
        vuln = np.max([prob(train_learn_df, "yhat", "y"), prob(test_learn_df, "yhat", "y")])
    except ZeroDivisionError:
        vuln = 1  # it's a probability

    # vulnerability
    eval_df = train_eval_df.append(test_eval_df)
    subgroup_vuln = vuln * prob(eval_df, "yhat y", "z")

    # overfitting factor \epsilon
    p_train_learn = prob(train_learn_df, "yhat", "y")
    p_test_learn = prob(test_learn_df, "yhat", "y")
    eps_learn = max(p_train_learn, p_test_learn) - np.mean([p_train_learn, p_test_learn])

    p_train_eval = prob(train_eval_df, "yhat", "y")
    p_test_eval = prob(test_eval_df, "yhat", "y")
    eps_eval = max(p_train_eval, p_test_eval) - np.mean([p_train_eval, p_test_eval])

    # unfairness factor \phi
    learn_df = train_learn_df.append(test_learn_df)
    try:
        phi_learn = prob(learn_df, "yhat", "y z") / prob(learn_df, "yhat", "y")
    except ZeroDivisionError:
        phi_learn = np.inf

    try:
        phi_eval = prob(eval_df, "yhat", "y z") / prob(eval_df, "yhat", "y")
    except ZeroDivisionError:
        phi_eval = np.inf

    return subgroup_vuln, eps_learn, eps_eval, phi_learn, phi_eval


def run_compas_experiment(data, z_label, z_values, clf, fit_args=None, num_batches=1):
    """
    :: experiment settings

    """

    # setup
    # if classifier is None:
    #     clf = LogisticRegression(solver="lbfgs")
    # else:
    #     clf = classifier

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + '-' + x["Ground Truth"], axis=1)

    y_label = "Ground Truth_Will not reoffend"
    encode = lambda df, cols: encode_dataframe(df, drop_first=False,
                                               output_columns=cols,
                                               initially_drop_columns=None,
                                               add_non_dummified_columns=[z_label],
                                               finally_drop_columns=['Ground Truth_Will reoffend', 'Gender_Female',
                                                                     'Felony or Misdemeanor_Misdemeanor'] if cols == None else None,
                                               return_output_columns=cols == None)

    metrics = []
    for batch_no in tqdm(range(num_batches)):

        # train/test split
        train_df, test_df = train_test_split(data, test_size=0.5, stratify=data["strat"])

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)

        # separate features and labels
        X_train, y_train = train_df.drop([y_label, z_label, "strat"], axis=1), train_df[y_label]
        X_test, y_test = test_df.drop([y_label, z_label, "strat"], axis=1), test_df[y_label]

        # print(X_train.columns)

        fit_args_ = dict(sensitive_features=train_df[z_label],
                         **{k: v for k, v in fit_args.items() if k != "sensitive"}) if fit_args.get("sensitive",
                                                                                                    False) else dict()
        clf.fit(X_train, y_train, **fit_args_)

        # save confidences to train/test dataframes
        predict_args_ = lambda df: dict(sensitive_features=df[z_label]) if fit_args.get("sensitive", False) else dict()

        train_df.loc[:, "p"] = clf.predict_proba(X_train, **predict_args_(train_df))[:, 1]
        test_df.loc[:, "p"] = clf.predict_proba(X_test, **predict_args_(test_df))[:, 1]

        # save predicted labels to train/test dataframes
        train_df.loc[:, "yhat"] = clf.predict(X_train, **predict_args_(train_df))
        test_df.loc[:, "yhat"] = clf.predict(X_test, **predict_args_(test_df))

        # measure target model accuracy
        target_train_acc = accuracy_score(train_df[y_label], train_df["yhat"])
        target_test_acc = accuracy_score(test_df[y_label], test_df["yhat"])

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(train_df, test_size=0.3, stratify=train_df["strat"])

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(test_df, test_size=0.3, stratify=test_df["strat"])

        for z_val in z_values:
            # fp metrics
            metrics += [(z_val, 'fp', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=0,
                                                   yhat_value=1)),
                        (z_val, 'fp', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=0,
                                            yhat_value=1))
                        ]

            # tp metrics
            metrics += [(z_val, 'tp', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=1,
                                                   yhat_value=1)),
                        (z_val, 'tp', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=1,
                                            yhat_value=1))]

            # fn metrics
            metrics += [(z_val, 'fn', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=1,
                                                   yhat_value=0)),
                        (z_val, 'fn', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=1,
                                            yhat_value=0))]

            # tn metrics
            metrics += [(z_val, 'tn', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=0,
                                                   yhat_value=0)),
                        (z_val, 'tn', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=0,
                                            yhat_value=0))]

    return pd.DataFrame(metrics,
                        columns=["subgroup", "metric", "batch_no", "attacker", "target_train_acc", "target_test_acc",
                                 "subgroup_vuln", "eps_learn", "eps_eval", "phi_learn", "phi_eval"])


def run_adult_experiment(data, z_label, z_values, clf, fit_args=None, n_processes=4, num_batches=1):
    """
    :: experiment settings

    """

    # setup
    # if classifier is sNone:
    #     clf = LogisticRegression(solver="lbfgs")
    # else:
    #     clf = classifier

    # adding the joint column for stratification
    data["strat"] = data.apply(lambda x: x[z_label] + '-' + str(x["income"]), axis=1)

    y_label = "income"
    encode = lambda df, cols: encode_dataframe(df, drop_first=False,
                                               output_columns=cols,
                                               initially_drop_columns=None,
                                               add_non_dummified_columns=[z_label],
                                               finally_drop_columns=[] if cols == None else None,
                                               return_output_columns=cols == None)

    def one_iter(batch_no):

        metrics = []
        # train/test split
        train_df, test_df = train_test_split(data, test_size=0.5, stratify=data["strat"], random_state=batch_no)

        # encode dataframes for learning and prediction
        train_df, cols = encode(train_df, None)
        test_df = encode(test_df, cols)

        # separate features and labels
        X_train, y_train = train_df.drop([y_label, z_label, "strat"], axis=1), train_df[y_label]
        X_test, y_test = test_df.drop([y_label, z_label, "strat"], axis=1), test_df[y_label]

        fit_args_ = dict(sensitive_features=train_df[z_label],
                         **{k: v for k, v in fit_args.items() if k != "sensitive"}) if fit_args.get("sensitive",
                                                                                                    False) else dict()
        clf.fit(X_train, y_train, **fit_args_)

        # save confidences to train/test dataframes
        predict_args_ = lambda df: dict(sensitive_features=df[z_label]) if fit_args.get("sensitive", False) else dict()

        train_df.loc[:, "p"] = clf.predict_proba(X_train, **predict_args_(train_df))[:, 1]
        test_df.loc[:, "p"] = clf.predict_proba(X_test, **predict_args_(test_df))[:, 1]

        # save predicted labels to train/test dataframes
        train_df.loc[:, "yhat"] = clf.predict(X_train, **predict_args_(train_df))
        test_df.loc[:, "yhat"] = clf.predict(X_test, **predict_args_(test_df))

        # measure target model accuracy
        target_train_acc = accuracy_score(train_df[y_label], train_df["yhat"])
        target_test_acc = accuracy_score(test_df[y_label], test_df["yhat"])

        # split train into learn/eval
        train_learn_df, train_eval_df = train_test_split(train_df, test_size=0.3, stratify=train_df["strat"],
                                                         random_state=batch_no)

        # split test into learn/eval
        test_learn_df, test_eval_df = train_test_split(test_df, test_size=0.3, stratify=test_df["strat"],
                                                       random_state=batch_no)

        for z_val in z_values:
            # fp metrics
            metrics += [(z_val, 'fp', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=0,
                                                   yhat_value=1)),
                        (z_val, 'fp', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=0,
                                            yhat_value=1))
                        ]

            # tp metrics
            metrics += [(z_val, 'tp', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=1,
                                                   yhat_value=1)),
                        (z_val, 'tp', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=1,
                                            yhat_value=1))]

            # fn metrics
            metrics += [(z_val, 'fn', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=1,
                                                   yhat_value=0)),
                        (z_val, 'fn', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=1,
                                            yhat_value=0))]

            # tn metrics
            metrics += [(z_val, 'tn', batch_no, 'discriminating', target_train_acc, target_test_acc,
                         *calculate_discriminating(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                                   z_label=z_label,
                                                   z_value=z_val,
                                                   y_label=y_label,
                                                   y_value=0,
                                                   yhat_value=0)),
                        (z_val, 'tn', batch_no, 'regular', target_train_acc, target_test_acc,
                         *calculate_regular(train_learn_df, train_eval_df, test_learn_df, test_eval_df,
                                            z_label=z_label,
                                            z_value=z_val,
                                            y_label=y_label,
                                            y_value=0,
                                            yhat_value=0))]

        # print(metrics[0])
        return metrics

    # results = []
    # with Pool(processes=n_processes) as p:
    #     pbar = tqdm(p.map(one_iter, range(num_batches)), total=num_batches)
    #     for _res in pbar:
    #         results += _res
    #         pbar.update()


    with joblib.parallel_backend('dask'):
        results = joblib.Parallel(n_jobs=n_processes, verbose=100)(
            joblib.delayed(one_iter)(i) for i in range(num_batches))

    # results = one_iter(10)

    # flatten list of lists
    results = reduce(lambda list1, list2: [*list1, *list2], results)

    return pd.DataFrame(results,
                        columns=["subgroup", "metric", "batch_no", "attacker", "target_train_acc", "target_test_acc",
                                 "subgroup_vuln", "eps_learn", "eps_eval", "phi_learn", "phi_eval"])


if __name__ == "__main__":
    data_undummified = prepare_compas("data/compas/compas-scores-two-years.csv", dummified=False, drop_first=True,
                                      drop_duplicates=False)

    fit_args = {"fair_lr": dict(sensitive_col_idx=[10], correlation_tolerance=0.2)}

    metrics_gender = {
        model_name: run_compas_experiment(data_undummified, z_label="Gender", z_values=["Male", "Female"],
                                          num_batches=1,
                                          clf=clf, fit_args=fit_args.get(model_name, {})) for model_name, clf in
        model_zoo.items()}
