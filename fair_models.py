try:
    from fairlearn.post_processing import ThresholdOptimizer
except ImportError:
    from fairlearn.postprocessing import ThresholdOptimizer

import itertools

import attr
import numpy as np
import pandas as pd
import cvxopt as cvx

from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.linear_model import LogisticRegression


class LogisticRegressionAsRegression:
    # This wrapper around the unconstrained estimator serves the purpose of mapping the predict
    # method to predict_proba so that we can use real values to get more accurate estimates.
    def __init__(self, logistic_regression_estimator):
        self.logistic_regression_estimator = logistic_regression_estimator

    def fit(self, X, y):
        self.logistic_regression_estimator.fit(X, y)

    def predict(self, X):
        # use predict_proba to get real values instead of 0/1, select only prob for 1
        scores = self.logistic_regression_estimator.predict_proba(X)[:, 1]
        return scores


class LogisticRegressionThresholdOptimized(BaseEstimator, ClassifierMixin):
    def __init__(self, **init_params):
        self.unconstrained_model = LogisticRegression(**init_params)
        self.fair_model_ = None
        self.sensitive_col_idx_ = None

    def fit(self, X, y, sensitive_features, parity_criteria="EqualizedOdds"):
        self.unconstrained_model.fit(X, y)
        unconstrained_model_wrapper = LogisticRegressionAsRegression(
            self.unconstrained_model
        )
        self.fair_model_ = ThresholdOptimizer(
            unconstrained_model=unconstrained_model_wrapper,
            parity_criteria=parity_criteria,
        )
        self.fair_model_.fit(X, y, sensitive_features=sensitive_features)
        return self.fair_model_

    def predict(self, X, sensitive_features):
        return self.fair_model_.predict(X, sensitive_features=sensitive_features)

    def predict_proba(self, X, sensitive_features):
        return self.fair_model_.predict_proba(X, sensitive_features=sensitive_features)

    def score(self, X, y, sample_weight=None):
        return self.fair_model_.score(X, y, sample_weight)


@attr.s
class Rates:
    fpr = attr.ib()
    tpr = attr.ib()


class LogisticRegressionFairDP(BaseEstimator, ClassifierMixin):
    class PostProcessingFail(Exception):
        pass

    def __init__(self, epsilon=1, gamma=0.2, beta=0.05, seed=1, **lr_params):
        self.lr_params = lr_params
        self.epsilon = epsilon
        self.gamma = gamma
        self.beta = beta

        self._random_state = np.random.RandomState(seed=seed)

    def fit(self, X, y, sensitive_features):
        self.unconstrained_model_ = LogisticRegression(**self.lr_params).fit(X, y)
        yhat = self.unconstrained_model_.predict(X)
        self.qhat_by_subgroup_ = self._compute_qhat(yhat, y, sensitive_features)
        self.p_by_subgroup_ = self._post_process_dp_eo(yhat)
        if self.p_by_subgroup_ is None:
            raise LogisticRegressionFairDP.PostProcessingFail("Post-processing failed.")
        return self

    @staticmethod
    def _compute_rates(qhat_by_subgroup):
        """
        Compute TPR/FPR rates for each subgroup.

        Based on code: https://github.com/SaeedSharifiMa/FairDP/blob/dev_branch/dp_postproc.py
        """
        rates_by_subgroup = {}
        for subgroup, qhat in qhat_by_subgroup.items():
            rates_by_subgroup[subgroup] = Rates(
                fpr=qhat[1, 0] / (qhat[1, 0] + qhat[0, 0]),
                tpr=qhat[1, 1] / (qhat[1, 1] + qhat[0, 1]),
            )

        return rates_by_subgroup

    def _compute_qhat(self, yhat, y, z, subgroups=None, y_range=None, p_range=None):
        """
        Compute the joint probability P[yhat, y, z].

        Based on code: https://github.com/SaeedSharifiMa/FairDP/blob/dev_branch/dp_postproc.py
        """
        if y_range is None:
            y_range = [0, 1]
        if p_range is None:
            p_range = [0, 1]
        if subgroups is None:
            subgroups = z.unique().tolist()

        df = pd.DataFrame(
            np.column_stack((np.array(yhat), np.array(z), np.array(y))),
            columns=["yhat", "z", "y"],
        )
        qhat_by_subgroup = {}
        for z_value in subgroups:
            qhat_by_subgroup[z_value] = np.zeros((2, 2))
            for yhat_value, y_value in itertools.product(p_range, y_range):
                qhat_by_subgroup[z_value][yhat_value, y_value] = len(
                    df.query(
                        f"yhat == {yhat_value} & y == {y_value} & z == '{z_value}'"
                    )
                ) / len(df)
        return qhat_by_subgroup

    def _add_dp_noise(self, qhat, num_examples):
        noise = self._random_state.laplace(
            scale=2 / (num_examples * self.epsilon), size=4
        ).reshape((2, 2))
        return qhat + noise

    def _post_process_dp_eo(self, yhat):
        """
        DP Fair post-processing by Jagielski et al.

        Based on code: https://github.com/SaeedSharifiMa/FairDP/blob/dev_branch/dp_postproc.py
        """
        num_examples = len(yhat)
        num_subgroups = len(self.qhat_by_subgroup_)
        rates_by_subgroup = LogisticRegressionFairDP._compute_rates(
            self.qhat_by_subgroup_
        )
        subgroups = list(self.qhat_by_subgroup_.keys())
        ref_subgroup = subgroups[0]

        # Setup and noisy joint probabilities.
        q_dp_by_subgroup = {}
        rates_dp_by_subgroup = {}
        fpr_bounds_by_subgroup = {}
        tpr_bounds_by_subgroup = {}
        ref_q_dp = q_dp_by_subgroup[ref_subgroup] = self._add_dp_noise(
            self.qhat_by_subgroup_[ref_subgroup], num_examples
        )
        for i, subgroup in enumerate(subgroups[1:]):
            q_dp_by_subgroup[subgroup] = self._add_dp_noise(
                self.qhat_by_subgroup_[subgroup], num_examples
            )
            q_dp = q_dp_by_subgroup[subgroup]
            minq_dp_fpr = np.min(
                [q_dp[1, 0] + q_dp[0, 0], ref_q_dp[1, 0] + ref_q_dp[0, 0]]
            )
            minq_dp_tpr = np.min(
                [q_dp[1, 1] + q_dp[0, 1], ref_q_dp[1, 1] + ref_q_dp[0, 1]]
            )
            fpr_bounds_by_subgroup[subgroup] = fpr_bound = (
                4 * np.log(4 * num_subgroups / self.beta)
            ) / (minq_dp_fpr * num_examples * self.epsilon)
            tpr_bounds_by_subgroup[subgroup] = tpr_bound = (
                4 * np.log(4 * num_subgroups / self.beta)
            ) / (minq_dp_tpr * num_examples * self.epsilon)
            if (fpr_bound < 0) or (tpr_bound < 0):
                return None

        rates_dp_by_subgroup = LogisticRegressionFairDP._compute_rates(q_dp_by_subgroup)
        ref_rates_dp = rates_dp_by_subgroup[ref_subgroup]

        # LP objective.
        c_values = []
        for q_dp in q_dp_by_subgroup.values():
            c_values.extend([q_dp[0, 0] - q_dp[0, 1], q_dp[1, 0] - q_dp[1, 1]])
        c = cvx.matrix(c_values, (len(c_values), 1), "d")
        upper_bounds = []

        # FPR constraints.
        fpr_constraints = []
        for i in subgroups[1:]:
            rates_dp = rates_dp_by_subgroup[subgroup]
            constraint_row = []
            for j in subgroups:
                if j == ref_subgroup:
                    constraint_row.extend([ref_rates_dp.fpr - 1, -ref_rates_dp.fpr])
                elif j == i:
                    constraint_row.extend([1 - rates_dp.fpr, rates_dp.fpr])
                else:
                    constraint_row.extend([0, 0])
            assert len(constraint_row) == 2 * num_subgroups
            assert (np.array(constraint_row) != 0).sum() == 4
            fpr_constraints.append(constraint_row)
            upper_bounds.append(self.gamma + fpr_bounds_by_subgroup[subgroup])

            constraint_row = []
            for j in subgroups:
                if j == ref_subgroup:
                    constraint_row.extend([-ref_rates_dp.fpr + 1, ref_rates_dp.fpr])
                elif j == i:
                    constraint_row.extend([-1 + rates_dp.fpr, -rates_dp.fpr])
                else:
                    constraint_row.extend([0, 0])
            assert len(constraint_row) == 2 * num_subgroups
            assert (np.array(constraint_row) != 0).sum() == 4
            fpr_constraints.append(constraint_row)
            upper_bounds.append(self.gamma + fpr_bounds_by_subgroup[subgroup])

        # TPR constraints.
        tpr_constraints = []
        for i in subgroups[1:]:
            rates_dp = rates_dp_by_subgroup[subgroup]
            constraint_row = []
            for j in subgroups:
                if j == ref_subgroup:
                    constraint_row.extend([ref_rates_dp.tpr - 1, -ref_rates_dp.tpr])
                elif i == j:
                    constraint_row.extend([1 - rates_dp.tpr, rates_dp.tpr])
                else:
                    constraint_row.extend([0, 0])
            assert len(constraint_row) == 2 * num_subgroups
            assert (np.array(constraint_row) != 0).sum() == 4
            tpr_constraints.append(constraint_row)
            upper_bounds.append(self.gamma + tpr_bounds_by_subgroup[subgroup])

            constraint_row = []
            for j in subgroups:
                if j == ref_subgroup:
                    constraint_row.extend([-ref_rates_dp.tpr + 1, ref_rates_dp.tpr])
                elif i == j:
                    constraint_row.extend([-1 + rates_dp.tpr, -rates_dp.tpr])
                else:
                    constraint_row.extend([0, 0])
            assert len(constraint_row) == 2 * num_subgroups
            assert (np.array(constraint_row) != 0).sum() == 4
            tpr_constraints.append(constraint_row)
            upper_bounds.append(self.gamma + tpr_bounds_by_subgroup[subgroup])

        # Probability constraints.
        probability_constraints = []
        for i in range(len(c_values)):
            constraint_row = [1 if i == j else 0 for j in range(len(c_values))]
            probability_constraints.append(constraint_row)
            upper_bounds.append(1)

            constraint_row = [-1 if i == j else 0 for j in range(len(c_values))]
            probability_constraints.append(constraint_row)
            upper_bounds.append(0)

        # Build constraint matrices.
        constraint_rows = []
        constraint_rows.extend(fpr_constraints)
        constraint_rows.extend(tpr_constraints)
        constraint_rows.extend(probability_constraints)
        A = cvx.matrix(constraint_rows, (8 * num_subgroups - 4, num_subgroups * 2))
        b = cvx.matrix(upper_bounds, (8 * num_subgroups - 4, 1), "d")
        cvx.solvers.options["show_progress"] = False
        sol = cvx.solvers.lp(c, A, b)
        p_vec = np.array(sol["x"]).squeeze()
        p_vec[p_vec < 0] = 0
        p_vec[p_vec > 1] = 1

        p_by_subgroup = {}
        for i, subgroup in enumerate(subgroups):
            p_by_subgroup[subgroup] = p_vec[[i, i + 1]]

        # err_tilde = 0
        # for subgroup, qhat in q_dp_by_subgroup.items():
        #     p = p_by_subgroup[subgroup]
        #     err_tilde += (qhat[0, 0] - qhat[0, 1]) * p[0]
        #     err_tilde += (qhat[1, 0] - qhat[1, 1]) * p[1]
        #     err_tilde += qhat[0, 1] + qhat[1, 1]
        # print("Error on noisy data", err_tilde, 1 - err_tilde)

        # err_tilde = 0
        # for subgroup, qhat in self.qhat_by_subgroup_.items():
        #     p = p_by_subgroup[subgroup]
        #     err_tilde += (qhat[0, 0] - qhat[0, 1]) * p[0]
        #     err_tilde += (qhat[1, 0] - qhat[1, 1]) * p[1]
        #     err_tilde += qhat[0, 1] + qhat[1, 1]
        # print("Error on real data", err_tilde, 1 - err_tilde)
        return p_by_subgroup

    def predict(self, X, sensitive_features):
        yhat = self.unconstrained_model_.predict(X)
        corrected_values = np.zeros_like(yhat)
        for i, (yhat_value, subgroup) in enumerate(zip(yhat, sensitive_features)):
            p = self.p_by_subgroup_[subgroup][yhat_value]
            corrected_values[i] = self._random_state.choice([0, 1], p=[1 - p, p])
            # corrected_values[i] = p >= 0.5
        return corrected_values

    def get_params(self, deep=True):
        return {
            "epsilon": self.epsilon,
            "gamma": self.gamma,
            "beta": self.beta,
            "lr_params": self.lr_params,
        }

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self

    def score(self, X, y, sensitive_features):
        yhat = self.predict(X, sensitive_features)
        return (yhat == y).mean()
