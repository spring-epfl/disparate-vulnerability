# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.11.1
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %%
NUM_SHUFFLES = 100
FIGWIDTH = 8
ALPHA = 0.01
LOAD_FROM_PICKLE = False
PICKLE = "results/compas_race_may3.pkl"

# %%
# %load_ext autoreload
# %autoreload 2

import itertools

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from tqdm import notebook as tqdm
from scipy import stats
from sklearn.model_selection import train_test_split
from joblib import Parallel, delayed
from statsmodels.stats.anova import AnovaRM
from statsmodels.stats.multicomp import pairwise_tukeyhsd

from model_zoo import model_zoo, renaming_dict, DpClassifierFactory
from mia.vuln import run_threshold_estimator
from utils import infer_from, normalize_vuln_mean, normalize_vuln_std
# from plotting import plot_stat_heatmaps, plot_vuln_dists
from loaders.compas import prepare_compas

import plot_params

# %% [markdown]
# ## Data prep

# %%
df = pd.read_csv("data/compas/compas-scores-two-years.csv")

# %%
data = prepare_compas("data/compas/compas-scores-two-years.csv",
                      dummified=False,
                      drop_first=True,
                      drop_duplicates=True)

def process_for_fit(data):
    X = pd.get_dummies(
        data[[col for col in data.columns if col != "Ground Truth"]],
        drop_first=True).astype(int)
    y = data["Ground Truth"]
    return X, y


# %%
dp_model_zoo = {
    f"eps{eps}": DpClassifierFactory(eps, data_norm=np.sqrt(7))
    for eps in [1, 2, 5, 15]
}
model_zoo = dict(model_zoo, **dp_model_zoo)

# %%
renaming_dict = dict(renaming_dict, **{
    "threshold": "Control",
    "eps0.1": r"$\varepsilon=0.1$",
    "eps0.5": r"$\varepsilon=0.5$",
    "eps1": r"$\varepsilon=1$",
    "eps2": r"$\varepsilon=2$",
    "eps2.5": r"$\varepsilon=2.5$",
    "eps5": r"$\varepsilon=5$",
    "eps7.5": r"$\varepsilon=7.5$",
    "eps15": r"$\varepsilon=15$",
})


# %%
class ControlClassifier:
    """
    Task-specific control (data-independent) classifier.
    
    Outputs 1 if Prior Counts is greater than 0.
    """
    def __init__(self):
        pass
        
    def fit(self, *args, **kwargs):
        pass
    
    def predict(self, xs, *args, **kwargs):
        return xs[:, 0] > 0
    
    def predict_proba(self, xs, *args, **kwargs):
        p = self.predict(xs)
        p = np.expand_dims(p, 1)
        return np.hstack([1-p, p])
    
ControlClassifier().predict_proba(np.array([[0, 1], [1, 0]]))
model_zoo["control"] = lambda: ControlClassifier()

# %%
model_zoo.keys()

# %%
subgroups = ['African-American', 'Caucasian', 'Hispanic', 'Other', 'Asian', 'Native American']
subgroups_short = ["AA", "CA", "HI", "OT", "AS", "NA"]


# %% [markdown]
# ## Run experiments

# %%
def score(model, X, y_true):
    y_pred = model.predict(X.values if isinstance(X, pd.DataFrame) else X)
    return (y_pred == y_true).mean()


# %%
X, y = process_for_fit(data)
models = ["control", "lr", "nn_8", "nn_32"] + list(dp_model_zoo.keys()) # ["lr_dm", "lr_eo"]
subgroup_label = "Race"
parallel = True
experiment_results = {}
model_tests = {}

for model_name in models:
    print(model_name)
    model_fn = model_zoo[model_name]
    
    def run_iter(rep, rng):
        # Train model.
        train_indices, test_indices = train_test_split(data.index, test_size=0.5, random_state=rng)
        X_train, y_train = X.loc[train_indices], y.loc[train_indices]
        X_test, y_test = X.loc[test_indices], y.loc[test_indices]
        
        fit_args = {}
        if any(keyword in model_name for keyword in ["eo", "dm", "erp"]):
            fit_args = dict(sensitive_features=data.loc[train_indices, subgroup_label])
            
        clf = model_fn()
        clf.fit(X_train.values, y_train.values, **fit_args)

        results = []
        for subgroup in ["All"] + subgroups:
            # Model info.            
            if subgroup == "All":
                group_train_indices = train_indices
                group_test_indices = test_indices
            else:
                group_train_indices = data.loc[train_indices].query(f"{subgroup_label} == '{subgroup}'").index
                group_test_indices = data.loc[test_indices].query(f"{subgroup_label} == '{subgroup}'").index
            
            X_group_train, y_group_train = X_train.loc[group_train_indices], y_train.loc[group_train_indices]
            X_group_test, y_group_test = X_test.loc[group_test_indices], y_test.loc[group_test_indices]
            
            # Model info.
            target_train_acc = score(clf, X_group_train, y_group_train)
            target_test_acc = score(clf, X_group_test, y_group_test)
            overfitting_gap = target_train_acc - target_test_acc
            
            # Vulnerability.
            vuln = run_threshold_estimator(
                y_group_train, infer_from(clf, X_group_train),
                y_group_test, infer_from(clf, X_group_test),
                method="average_loss_threshold",
            )
            
            results.append(dict(rep=rep,
                               vuln=vuln,
                               subgroup=subgroup,
                               target_train_acc=target_train_acc,
                               target_test_acc=target_test_acc,
                               overfitting_gap=overfitting_gap))
            
        return results
    
    reps = range(NUM_SHUFFLES)
    rngs = [np.random.RandomState(i) for i in enumerate(reps)]
    it = tqdm.tqdm(list(zip(reps, rngs)))
    
    # Run experiments and collect results.
    if parallel:
        collected_subgroup_vulns = Parallel(n_jobs=8, verbose=0)(delayed(run_iter)(rep, rng)
                                              for rep, rng in it)
    else:
        collected_subgroup_vulns = []
        for rep, rng in it:
            collected_subgroup_vulns.append(run_iter(rep, rng))

    model_results = pd.DataFrame(itertools.chain(*collected_subgroup_vulns))
    
    # Compute ANOVA p-value right away.
    anova = AnovaRM(model_results.query("subgroup != 'All'"),
        depvar="vuln", subject="rep", within=["subgroup"])
    res = anova.fit()
    f, p = (
        res.anova_table.loc["subgroup", "F Value"],
        res.anova_table.loc["subgroup", "Pr > F"]
    )
    print(f"p={p:.4f}\n")
    
    # Save experimental results for summaries and followup tests.
    experiment_results[model_name] = model_results.assign(model=model_name, f=f, p=p)

# %%
renaming_dict["control"] = "Control"

# %%
summary_data = pd.concat(experiment_results.values()).replace(renaming_dict)
summary_data.query("subgroup == 'All'").groupby(["model"]).agg({
    "p": "mean",
    "target_test_acc": ["mean", "std"],
    "overfitting_gap": ["mean", "std"],
    "vuln": [normalize_vuln_mean, normalize_vuln_std],
}).reindex([renaming_dict[model_name] for model_name in experiment_results.keys()]).rename(columns={
    "model": "Model",
    "target_test_acc": "Test acc.",
    "overfitting_gap": "Gen. gap",
    "vuln": "Overall vuln.",
}).style.format('{:.4f}')

# %%
summary_data = pd.concat(experiment_results.values()).replace(renaming_dict)
summary_data.query(f"subgroup != 'All' and p < {ALPHA}").groupby(["model", "subgroup"]).agg({
    "target_test_acc": ["mean", "std"],
    "overfitting_gap": ["mean", "std"],
    "vuln": [normalize_vuln_mean, normalize_vuln_std],
}).rename(columns={
    "model": "Model",
    "target_test_acc": "Test acc.",
    "overfitting_gap": "Gen. gap",
    "vuln": "Subgroup vuln.",
}).style.format('{:.2f}')

# %%
for model_name, df in experiment_results.items():
    p = df.p.mean()
    if p < ALPHA:
        tukey = pairwise_tukeyhsd(
            endog=df.query("subgroup != 'All'").vuln,\
            groups=df.query("subgroup != 'All'").subgroup,
            alpha=ALPHA
        )
        print(f"{model_name} {p=:.4f}")
        print(tukey)
