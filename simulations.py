# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.11.1
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %%
DATA_FILE_FORMAT = "results/simulations_{}.csv"
BASE_SEED = 0
DATA_SETUP = dict(
    num_features=100,
    noise=0.1,
    delta=0.5,
    divergent=True,
)

# %%
# %load_ext autoreload
# %autoreload 2

import math
import hashlib
import itertools
import collections

import numpy as np
import pandas as pd
import seaborn as sns
import diffprivlib.models as dp

from IPython.display import display
from tqdm import notebook as tqdm
from matplotlib import pyplot as plt
from sklearn.datasets import make_spd_matrix
from sklearn.model_selection import train_test_split
from scipy import stats
from joblib import Parallel, delayed

from utils import infer_from, normalize_vuln_mean, normalize_vuln_std
from mia.vuln import run_shadow_model_attack, run_threshold_estimator, apply_estimator_func
from model_zoo import model_zoo, lr_setup, renaming_dict, DpClassifierFactory
from plotting import add_significance_markers

import plot_params 

# %%
import logging
logger = logging.Logger(name="default")


# %% [markdown]
# ## Simulating disparate vulnerability

# %% [markdown]
# ### Synthetic data

# %%
def create_multinomial_setup(
        num_features=2,
        noise=0.1,
        delta=0.5,
        divergent=True,
        seed=1
):
    gen = np.random.RandomState(seed)
    weights = np.ones(num_features) / num_features
    if divergent:
        means_a = np.zeros([num_features])
        means_b = np.ones([num_features])
    else:
        means_a = gen.uniform(-1, 1, num_features)
        means_b = gen.uniform(-1, 1, num_features)
    
    cov = make_spd_matrix(num_features, random_state=seed) * noise

    return means_a, means_b, cov, noise, delta

def gen_sim_data(setup, gen, size_a, size_b):
    means0, means1, cov, noise, delta = setup
    y = [0] * (size_a // 2) + [1] * math.ceil(size_a / 2) + \
        [0] * (size_b // 2) + [1] * math.ceil(size_b / 2)
    a0 = gen.multivariate_normal(means0 - delta, cov, size=size_a // 2)
    a1 = gen.multivariate_normal(means1, cov, size=math.ceil(size_a / 2))
    b0 = gen.multivariate_normal(means0, cov, size=size_b // 2)
    b1 = gen.multivariate_normal(means1 - delta, cov, size=math.ceil(size_b / 2))
    X = np.vstack([a0, a1, b0, b1])
    
    hi = 2**32
    index = gen.randint(hi, size=size_a + size_b)
    return pd.DataFrame(X, index=index), pd.Series(np.array(y), index=index)

# Demo:
setup = create_multinomial_setup(seed=BASE_SEED, **DATA_SETUP)
gen = np.random.RandomState(BASE_SEED)
size_a = 10000
size_b = 10000
X, y = gen_sim_data(setup, gen, size_a, size_b)
z = np.array(["C"] * size_a + ["T"] * size_b)
print(X.shape, y.shape, z.shape)

sample_df = pd.DataFrame(X)
sample_df = sample_df.assign(y=y).assign(z=z)
sample_df = sample_df.rename(columns={
    "z": "$z$",
    "y": "$y$",
})

g = sns.displot(data=sample_df, x=2, y=3, hue="$y$", col="$z$", kind="kde")
g.set_axis_labels("", "")
g.fig.set_figwidth(12)
g.fig.set_figheight(8)
g.fig.set_tight_layout(tight=True)
g.fig.savefig("images/synthetic_data_demo.pdf")

# %% [markdown]
# ### Populate Model Zoo

# %%
# Compute max data norm for DP classifiers.
setup = create_multinomial_setup(seed=BASE_SEED, **DATA_SETUP)
gen = np.random.RandomState(BASE_SEED)
size_a = 100000
size_b = 100000
sample, _ = gen_sim_data(setup, gen, size_a, size_b)

sample_norm = np.linalg.norm(sample, axis=1, ord=2).max()
sample_norm

# %%
dp_model_zoo = {
    f"eps{eps}": DpClassifierFactory(eps, data_norm=sample_norm)
    for eps in [0.1, 1, 2, 10]
}
model_zoo = dict(model_zoo, **dp_model_zoo)


# %%
class ThreshClassifier:
    def __init__(self, threshold=0.5):
        self.threshold = threshold
        
    def fit(self, *args, **kwargs):
        pass
    
    def predict_proba(self, xs, *args, **kwargs):
        p = xs[:, 0] > self.threshold
        p = np.expand_dims(p, 1)
        return np.hstack([1-p, p])
    
ThreshClassifier().predict_proba(np.array([[0, 1], [1, 0]]))
model_zoo["threshold"] = lambda: ThreshClassifier()

# %%
renaming_dict = dict(renaming_dict, **{
    "threshold": "Control",
    "eps0.1": r"$\varepsilon=0.1$",
    "eps0.5": r"$\varepsilon=0.5$",
    "eps1": r"$\varepsilon=1$",
    "eps2": r"$\varepsilon=2$",
    "eps2.5": r"$\varepsilon=2.5$",
    "eps5": r"$\varepsilon=5$",
    "eps7.5": r"$\varepsilon=7.5$",
    "eps10": r"$\varepsilon=10$",
    "eps15": r"$\varepsilon=15$",
})


# %%
def eval_sim_vuln(
    data,
    method="shadow_attack_preds",
    num_targets=None,
    **kwargs,
):
    if "shadow" in method:
        shadow_indices = data.y_train.index[:-num_targets]
        target_indices = data.y_train.index[-num_targets:]
        
        vuln = run_shadow_model_attack(
            data.y_train.loc[shadow_indices].values,
            data.preds_train.loc[shadow_indices].values,
            data.y_test.loc[shadow_indices].values,
            data.preds_test.loc[shadow_indices].values,
            data.y_train.loc[target_indices].values,
            data.preds_train.loc[target_indices].values,
            data.y_test.loc[target_indices].values,
            data.preds_test.loc[target_indices].values,
            method=method,
            **kwargs,
        )

    elif "threshold" in method:
        vuln = apply_estimator_func(
            data.y_train.values,
            data.preds_train.values,
            data.y_test.values,
            data.preds_test.values,
            run_threshold_estimator,
            method=method,
            **kwargs,
        )
    else:
        raise NotImplementedError

    return vuln


# %%
class SimDatasetGen:
    def __init__(self, setup):
        self.setup = setup

    def __call__(self, rng, treatment_group_size, control_group_size):
        X_a_train, y_a_train = gen_sim_data(self.setup, rng, control_group_size, 0)
        X_b_train, y_b_train = gen_sim_data(self.setup, rng, 0, treatment_group_size)
        X_a_test, y_a_test = gen_sim_data(self.setup, rng, control_group_size, 0)
        X_b_test, y_b_test = gen_sim_data(self.setup, rng, 0, treatment_group_size)
        return (X_a_train, y_a_train, X_b_train, y_b_train,
                X_a_test, y_a_test, X_b_test, y_b_test)


# %%
def start_logger_if_necessary():
    logger = logging.getLogger("default")
    if len(logger.handlers) == 0:
        logger.setLevel(logging.INFO)
        sh = logging.StreamHandler()
        sh.setFormatter(logging.Formatter("%(asctime)s %(levelname)-8s %(message)s"))
        fh = logging.FileHandler('simulations.log', mode='w')
        fh.setFormatter(logging.Formatter("%(asctime)s %(levelname)-8s %(message)s"))
        logger.addHandler(sh)
        logger.addHandler(fh)
    return logger


# %%
def get_attack_data(
    model_fn,
    treatment_group_sizes,
    dataset_gen,
    seed=1,
    num_models=100,
    max_size=None,
    parallel=True,
    sensitive_fit=False,
    sensitive_pred=False,
):
    """Generate datasets, train models, and collect their outputs."""
    
    columns=[
        "group",
        "treatment_size",
        "group_size",
        "rep",
        "baseline_acc",
        "baseline_acc_group",
        "train_acc",
        "test_acc",
        "test_acc_group",
        "preds_train",
        "preds_test",
        "y_train",
        "y_test",
    ]
    if max_size is None:
        max_size = max(treatment_group_sizes)
    it = []
    for size in treatment_group_sizes:
        it.extend(itertools.product(
            [size],
            range(num_models), 
        ))
        
    # Add a PRNG for each iteration.
    rngs = [np.random.RandomState(hash(f"{seed}-{i}") % 2**32) for i, _ in enumerate(it)]
    it = tqdm.tqdm(list(zip(it, rngs)))
    it.set_description("Generating model outputs")
    
    results = pd.DataFrame()
    
    def run_iter(treatment_group_size, rep, rng):
        logger = start_logger_if_necessary()
        
        fit_args = {}
        
        datasets = dataset_gen(rng, treatment_group_size, max_size)
        (X_a_train, y_a_train, X_b_train, y_b_train,
         X_a_test, y_a_test, X_b_test, y_b_test) = datasets
                
        test_acc_a_baseline = max(y_a_test.mean(), 1 - y_a_test.mean())
        test_acc_b_baseline = max(y_b_test.mean(), 1 - y_b_test.mean())
        test_acc_baseline = (y_a_test.sum() + y_b_test.sum()) / (len(y_a_test) + len(y_b_test))
        test_acc_baseline = max(test_acc_baseline, 1 - test_acc_baseline)
        
        X_train = np.vstack([X_a_train, X_b_train])
        y_train = np.concatenate([y_a_train, y_b_train])
        
        # Special case for the fair classifier
        if sensitive_fit or sensitive_pred:
            train_group_ids = np.array([0] * len(X_a_train) + [1] * len(X_b_train))
            fit_args = dict(sensitive_features=train_group_ids)
        
        clf = model_fn()
        clf.fit(X_train, y_train, **fit_args)
        
        preds_a_train = infer_from(clf, X_a_train, sensitive_features=[0] * len(X_a_train)
                                   if sensitive_pred else None)
        preds_a_test = infer_from(clf, X_a_test, sensitive_features=[0] * len(X_a_test)
                                  if sensitive_pred else None)
        preds_b_train = infer_from(clf, X_b_train, sensitive_features=[1] * len(X_b_train)
                                   if sensitive_pred else None)
        preds_b_test = infer_from(clf, X_b_test, sensitive_features=[1] * len(X_b_test)
                                  if sensitive_pred else None)
        
        # Add indices
        preds_a_train = pd.Series(preds_a_train, index=X_a_train.index)
        preds_a_test = pd.Series(preds_a_test, index=X_a_test.index)
        preds_b_train = pd.Series(preds_b_train, index=X_b_train.index)
        preds_b_test = pd.Series(preds_b_test, index=X_b_test.index)
        print(preds_a_train.unique())
        
        # Compute accuracies
        train_acc = np.concatenate([
            (preds_a_train >= 0.5) == y_a_train,
            (preds_b_train >= 0.5) == y_b_train]).mean()
        
        test_acc_a_vals = (preds_a_test >= 0.5) == y_a_test
        test_acc_b_vals = (preds_b_test >= 0.5) == y_b_test
        test_acc = np.concatenate([test_acc_a_vals, test_acc_b_vals]).mean()
        
        test_acc_a = test_acc_a_vals.mean()
        test_acc_b = test_acc_b_vals.mean()
        
        result = pd.DataFrame()
        result = result.append(dict(zip(columns, [
            "a",
            treatment_group_size,
            max_size,
            rep,
            test_acc_baseline,
            test_acc_a_baseline,
            train_acc,
            test_acc,
            test_acc_a,
            preds_a_train,
            preds_a_test,
            y_a_train,
            y_a_test,
        ])), ignore_index=True)
        return result.append(dict(zip(columns, [
            "b",
            treatment_group_size,
            treatment_group_size,
            rep,
            test_acc_baseline,
            test_acc_b_baseline,
            train_acc,
            test_acc,
            test_acc_b,
            preds_b_train,
            preds_b_test,
            y_b_train,
            y_b_test,
        ])), ignore_index=True)
    
    if parallel:
        data = Parallel(n_jobs=8)(delayed(run_iter)(size, rep, rng)
                                             for (size, rep), rng in it)
    else:
        data = []
        for (size, rep), rng in it:
            data.append(run_iter(size, rep, rng))
            
    return pd.concat(data, axis=0, ignore_index=True)


# %%
def run_sim_all_sizes(
    model_fn,
    treatment_group_sizes,
    dataset_gen=None,
    data=None,
    method="average_loss_threshold",
    max_size=None,
    num_models=100,
    num_targets=None,
    parallel=True,
    microdata=False,
    sensitive_fit=False,
    sensitive_pred=False,
    seed=1,
):
    if data is None:
        assert dataset_gen is not None
        data = get_attack_data(
            model_fn,
            treatment_group_sizes,
            dataset_gen,
            max_size=max_size,
            num_models=num_models,
            parallel=parallel,
            sensitive_fit=sensitive_fit,
            sensitive_pred=sensitive_pred,
            seed=seed,
        )
    
    results = pd.DataFrame()
    it = tqdm.tqdm(treatment_group_sizes)
    it.set_description("Evaluating vuln. in group sizes")
    
    for treatment_group_size in it:
        size_data = data.query(f"treatment_size == {treatment_group_size}")
        group_a_data = size_data.query("group == 'a'").reset_index()
        group_b_data = size_data.query("group == 'b'").reset_index()
        
        vuln_a = eval_sim_vuln(group_a_data, method,
                           num_targets=num_targets,
                           parallel=parallel,
                           microdata=microdata)
        vuln_b = eval_sim_vuln(group_b_data, method,
                           num_targets=num_targets,
                           parallel=parallel,
                           microdata=microdata)

        if microdata:
            vuln_a_vals = vuln_a.groupby("model_id").vuln.mean()
            vuln_b_vals = vuln_b.groupby("model_id").vuln.mean()
            
        else:
            vuln_a_vals = vuln_a
            vuln_b_vals = vuln_b
        
        for vuln_a_val, vuln_b_val in zip(vuln_a_vals, vuln_b_vals):
            results = results.append(
                dict(
                    treatment_size=treatment_group_size,
                    control_size=group_a_data.group_size.mean(),
                    vuln_a=vuln_a_val,
                    vuln_b=vuln_b_val,
                    baseline_acc=size_data.baseline_acc.mean() * 100,
                    baseline_acc_a=group_a_data.baseline_acc.mean() * 100,                                 
                    baseline_acc_b=group_b_data.baseline_acc.mean() * 100,
                    train_acc=size_data.train_acc.mean() * 100,
                    test_acc=size_data.test_acc.mean() * 100,
                    test_acc_a=group_a_data.test_acc.mean() * 100,
                    test_acc_b=group_b_data.test_acc.mean() * 100,
                ), ignore_index=True)

    return results


# %%
target_name = "lr_dm_threshold"

setup = create_multinomial_setup(num_features=10, seed=BASE_SEED)

results = run_sim_all_sizes(
    model_zoo[target_name],
    [100, 2500],
    dataset_gen=SimDatasetGen(setup),
    seed=BASE_SEED,
    method="shadow_attack_loss",
    num_models=10,
    num_targets=5,
    microdata=False,
    parallel=True,
    sensitive_fit=any(keyword in target_name for keyword in ["eo", "erp", "dm"]),
    sensitive_pred=("threshold" in target_name),
)

# %%
for size in [100, 2500]:
    df = results.query(f"treatment_size == {size}")
    t, p = stats.ttest_rel(df.vuln_a, df.vuln_b)
    print(size, p)

# %%
results.groupby("treatment_size").agg({
    "vuln_a": ["mean", "std"],
    "vuln_b": ["mean", "std"],
    "baseline_acc": "mean",
    "test_acc": "mean",
})

# %% [markdown]
# ## Sample size experiments

# %%
model_names = ["threshold", "lr", "nn_8", "nn_32"] + list(dp_model_zoo.keys())
model_names

# %%
sample_size_results = pd.DataFrame()

# %%
treatment_group_sizes = [100, 500, 1000, 2500]
num_models = 230
num_targets = 200
method = "shadow_attack_loss"
global_setup = create_multinomial_setup(
    **{**DATA_SETUP},
)

is_fair_model = lambda model_name: any(keyword in model for keyword in ["eo", "erp", "dm"])

for model in model_names:
    print(model)
    model_fn = model_zoo[model]
    sim_res = run_sim_all_sizes(
        model_fn,
        treatment_group_sizes,
        dataset_gen=SimDatasetGen(global_setup),
        seed=BASE_SEED,
        method=method,
        num_models=num_models,
        num_targets=num_targets,
        sensitive_args=is_fair_model(model),
        parallel=True,
    )
    display(sim_res.groupby("treatment_size").agg({
        "vuln_a": ["mean", "std"],
        "vuln_b": ["mean", "std"],
        "baseline_acc": "mean",
        "test_acc": "mean",
    }))

    sample_size_results = sample_size_results.append(
        sim_res.assign(model=model, method=method), ignore_index=True)

# %%
# sample_size_results = pd.read_csv(DATA_FILE_FORMAT.format("may26"))

# %%
# sample_size_results.to_csv(DATA_FILE_FORMAT.format("may26"))

# %%
# Compute p-values.
methods = sample_size_results.method.unique()
for method, model, size in itertools.product(methods, model_names, treatment_group_sizes):
    df = sample_size_results.query(f"model == '{model}' and treatment_size == {size} and method == '{method}'")
    t, p = stats.ttest_rel(df.vuln_a, df.vuln_b)
    sample_size_results.loc[df.index, "p"] = p
    sample_size_results.loc[df.index, "t"] = t
    
sample_size_results.groupby(["method", "model", "treatment_size"]).p.mean().reset_index()

# %%
unrolled_list = []
for method, model, size in itertools.product(methods, model_names, treatment_group_sizes):
    df = sample_size_results.query(f"model == '{model}' and treatment_size == {size} and method == '{method}'").copy()
    dataset_size = size + treatment_group_sizes[-1]
    unrolled = pd.concat([
        pd.DataFrame(dict(vuln=df.vuln_a, target_test_acc=df.test_acc_a)).assign(subgroup="a"),
        pd.DataFrame(dict(vuln=df.vuln_b, target_test_acc=df.test_acc_b)).assign(subgroup="b"),
        pd.DataFrame(dict(vuln=df.vuln_a * size/dataset_size + df.vuln_b * treatment_group_sizes[-1]/dataset_size,
                          target_test_acc=df.test_acc_b)).assign(subgroup="All"),
    ], ignore_index=False).assign(method=method, model=model, treatment_group_size=size)
    unrolled_list.append(unrolled)
    
summary_data = pd.concat(unrolled_list, ignore_index=True)

# %%
summary_data.replace(renaming_dict).query("subgroup == 'All'"
).groupby(["model"]).agg({
    "target_test_acc": ["mean", "std"],
    "vuln": [normalize_vuln_mean, normalize_vuln_std],
}).reindex([renaming_dict[model_name] for model_name in model_names]).rename(columns={
    "model": "Model",
    "target_test_acc": "Test acc.",
    "vuln": "Overall vuln.",
})

# %%
hue_order = [renaming_dict[k] for k in dp_model_zoo.keys()] + [renaming_dict["lr"]]
plot_method = "shadow_attack_loss"

# %%
fig, ax = plt.subplots(figsize=(12, 8))

plot_models = list(dp_model_zoo.keys()) + ["lr"]
plot_df = sample_size_results.query(f"model in {plot_models} and method == '{plot_method}'")
plot_df["model"] = plot_df["model"].replace(renaming_dict)

plot_df = plot_df.rename(columns={
    "model": "Model",
    "test_acc": "Test acc.",
    "treatment_size": "Subgroup size",
})

sns.barplot(data=plot_df,
            x="Subgroup size",
            y="Test acc.",
            hue="Model",
            palette="rocket",
            hue_order=hue_order,
            ax=ax)

# plt.legend().remove()
fig.set_tight_layout(tight=True)

plt.savefig("images/plot_accuracy_dp.pdf")

# %%
fig, ax = plt.subplots(figsize=(12, 8))

plot_models = list(dp_model_zoo.keys()) + ["lr"]
plot_df = sample_size_results.query(f"model in {plot_models} and method == '{plot_method}'")
plot_df["vuln_diff"] = 2 * (plot_df.vuln_b - plot_df.vuln_a) * 100
plot_df["model"] = plot_df["model"].replace(renaming_dict)

plot_df = plot_df.rename(columns={
    "model": "Model",
    "treatment_size": "Subgroup size",
    "vuln_a": "Majority vuln.",
    "vuln_b": "Subgroup vuln.",
    "vuln_diff": "Disparity",
})

g = sns.barplot(
    data=plot_df,
    x="Subgroup size", y="Disparity",
    hue="Model",
    ax=ax,
    palette="rocket",
    hue_order=hue_order,
    alpha=1,
)

add_significance_markers(ax, sample_size_results.query(f"method == '{plot_method}'"),
                         hue_values=plot_models, x_values=treatment_group_sizes)

# ax.set_xscale("log")
plt.legend().remove()
fig.set_tight_layout(tight=True)

plt.savefig("images/plot_sample_size_dp.pdf")

# %%
fig, ax = plt.subplots(figsize=(12, 8))

plot_models = list(dp_model_zoo.keys()) + ["lr"]
plot_df = sample_size_results.query(f"model in {plot_models} and method == '{plot_method}'")
plot_df["model"] = plot_df["model"].replace(renaming_dict)
plot_df["acc_diff"] = plot_df["test_acc_b"] - plot_df["test_acc_a"]

plot_df = plot_df.rename(columns={
    "model": "Model",
    "treatment_size": "Subgroup size",
    "test_acc": "Test acc." ,
    "test_acc_a": "Majority group acc." ,
    "test_acc_b": "Minority group acc." ,
    "acc_diff": "Acc. discrepancy",
})

sns.barplot(data=plot_df,
            x="Subgroup size", y="Minority group acc.",
            hue="Model",
            palette="rocket",
            hue_order=hue_order,
            ax=ax)

# ax.set_xscale("log")
# ax.get_legend().remove()
fig.set_tight_layout(tight=True)

# plt.savefig("images/plot_accuracy_dp.pdf")

# %%
fig, ax = plt.subplots(figsize=(12, 8))

plot_models = ["lr", "nn_8", "nn_32"]
plot_df = sample_size_results.query(f"model in {plot_models}")
plot_df["gen_gap"] = plot_df.train_acc - plot_df.test_acc
plot_df.groupby(["model", "treatment_size"]).gen_gap.mean().reset_index()
plot_df = plot_df.replace(renaming_dict)
plot_df = plot_df.rename(columns={
    "model": "Model",
    "treatment_size": "Subgroup size",
    "gen_gap": "Overfitting", 
})

sns.barplot(data=plot_df, hue="Model", x="Subgroup size", y="Overfitting",
            ax=ax)

# %%
fig, ax = plt.subplots(figsize=(12, 8))

plot_models = ["lr", "nn_8", "nn_32"]
plot_df = sample_size_results.query(f"model in {plot_models} and method == '{plot_method}'")
plot_df["vuln_diff"] = (plot_df.vuln_b - plot_df.vuln_a) * 100
plot_df["model"] = plot_df["model"].replace(renaming_dict)

plot_df = plot_df.rename(columns={
    "model": "Model",
    "treatment_size": "Subgroup size",
    "vuln_b": "Subgroup vuln.",
    "vuln_diff": "Disparity",
})

sns.barplot(data=plot_df,
            x="Subgroup size", y="Disparity",
            hue="Model",
            ax=ax)
# ax.set_xscale("log")

add_significance_markers(ax, sample_size_results.query(f"method == '{plot_method}'"),
                         hue_values=plot_models, x_values=treatment_group_sizes)

fig.set_tight_layout(tight=True)

plt.savefig("images/plot_sample_size_normal.pdf")

# %%
fig, ax = plt.subplots(figsize=(12, 8))

plot_df = sample_size_results.query(f"model in ['lr', 'nn_8', 'nn_32'] and method == '{plot_method}'")
plot_df["model"] = plot_df["model"].replace(renaming_dict)
plot_df["acc_diff"] = plot_df["test_acc_b"] - plot_df["test_acc_a"]

plot_df = plot_df.rename(columns={
    "model": "Model",
    "treatment_size": "Subgroup size",
    "test_acc": "Test acc." ,
    "test_acc_a": "Majority group acc." ,
    "test_acc_b": "Minority group acc." ,
    "acc_diff": "Acc. discrepancy",
})

sns.barplot(data=plot_df,
             x="Subgroup size", y="Test acc.",
             hue="Model", ax=ax)
# ax.set_xscale("log")
fig.set_tight_layout(tight=True)
ax.get_legend().remove()

# plt.savefig("images/plot_accuracy_normal.pdf")


# %%
model_names = ["lr", "lr_dm_threshold", "lr_eo_threshold"]

# %%
fair_results = pd.DataFrame()

# %%
treatment_group_sizes = [100, 500, 1000, 2500]
num_models = 230
num_targets = 200
methods = ["shadow_attack_preds", "shadow_attack_loss"]
global_setup = create_multinomial_setup(num_features=10, seed=BASE_SEED)

for model, method in itertools.product(model_names, methods):
    print(model, method)
    model_fn = model_zoo[model]
    sim_res = run_sim_all_sizes(
        model_fn,
        treatment_group_sizes,
        dataset_gen=SimDatasetGen(global_setup),
        seed=BASE_SEED,
        method=method,
        num_models=num_models,
        num_targets=num_targets,
        sensitive_fit=any(keyword in model for keyword in ["eo", "erp", "dm"]),
        sensitive_pred=("threshold" in model),
        parallel=True,
    )
    display(sim_res.groupby("treatment_size").agg({
        "vuln_a": ["mean", "std"],
        "vuln_b": ["mean", "std"],
        "baseline_acc": "mean",
        "test_acc": "mean",
    }))

    fair_results = fair_results.append(
        sim_res.assign(model=model, method=method), ignore_index=True)

# %%
# Compute p-values.
methods = fair_results.method.unique()
for method, model, size in itertools.product(methods, model_names, treatment_group_sizes):
    df = fair_results.query(f"model == '{model}' and treatment_size == {size} and method == '{method}'")
    t, p = stats.ttest_rel(df.vuln_a, df.vuln_b)
    fair_results.loc[df.index, "p"] = p
    fair_results.loc[df.index, "t"] = t
    
    
fair_results.groupby(["method", "model", "treatment_size"]).p.mean().reset_index()

# %%
unrolled_list = []
for method, model, size in itertools.product(methods, model_names, treatment_group_sizes):
    df = fair_results.query(f"model == '{model}' and treatment_size == {size} and method == '{method}'").copy()
    dataset_size = size + treatment_group_sizes[-1]
    unrolled = pd.concat([
        pd.DataFrame(dict(vuln=df.vuln_a, target_test_acc=df.test_acc_a)).assign(subgroup="a"),
        pd.DataFrame(dict(vuln=df.vuln_b, target_test_acc=df.test_acc_b)).assign(subgroup="b"),
        pd.DataFrame(dict(vuln=df.vuln_a * size/dataset_size + df.vuln_b * treatment_group_sizes[-1]/dataset_size,
                          target_test_acc=df.test_acc_b)).assign(subgroup="All"),
    ], ignore_index=False).assign(method=method, model=model, treatment_group_size=size)
    unrolled_list.append(unrolled)
    
summary_data = pd.concat(unrolled_list, ignore_index=True)

# %%
summary_data.replace(renaming_dict).query("subgroup == 'All'"
).groupby(["model"]).agg({
    "target_test_acc": ["mean", "std"],
    "vuln": [normalize_vuln_mean, normalize_vuln_std],
}).reindex([renaming_dict[model_name] for model_name in model_names]).rename(columns={
    "model": "Model",
    "target_test_acc": "Test acc.",
    "vuln": "Overall vuln.",
})

# %%
fig, ax = plt.subplots(figsize=(12, 8))

plot_models = ["lr", "lr_dm_threshold", "lr_eo_threshold"]
plot_df = fair_results.query(f"model in {plot_models}")
plot_df["model"] = plot_df["model"].replace(renaming_dict)
plot_df["acc_diff"] = plot_df["test_acc_b"] - plot_df["test_acc_a"]

plot_df = plot_df.rename(columns={
    "model": "Model",
    "treatment_size": "Subgroup size",
    "test_acc": "Test acc." ,
    "test_acc_a": "Majority group acc." ,
    "test_acc_b": "Minority group acc." ,
    "acc_diff": "Acc. discrepancy",
})

sns.barplot(data=plot_df,
             x="Subgroup size", y="Minority group acc.",
             hue="Model", ax=ax)
# ax.set_xscale("log")
fig.set_tight_layout(tight=True)
# ax.get_legend().remove()


# %%
fig, ax = plt.subplots(figsize=(12, 8))

plot_models = ["lr", "lr_dm_threshold", "lr_eo_threshold"]
plot_method = "shadow_attack_preds"
plot_df = fair_results.query(f"model in {plot_models} and method == '{plot_method}'")
plot_df["vuln_diff"] = 2 * (plot_df.vuln_b - plot_df.vuln_a) * 100
plot_df["model"] = plot_df["model"].replace(renaming_dict)
plot_df = plot_df.replace({
    "attack_gbc_preds": "$\hat Y$",
    "attack_gbc_loss": "$\ell(\hat Y, Y)$",
})

plot_df = plot_df.rename(columns={
    "model": "Model",
    "treatment_size": "Subgroup size",
    "vuln_b": "Subgroup vuln.",
    "vuln_diff": "Disparity",
})

sns.barplot(
    data=plot_df,
    x="Subgroup size", y="Disparity",
    hue="Model",
    ax=ax,
    hue_order=[renaming_dict[model] for model in plot_models]
)
ax.set_ylim(-0.5, 2.2)
# ax.set_xscale("log")

add_significance_markers(ax, fair_results.query(f"method == '{plot_method}'"),
                         hue_values=plot_models, x_values=treatment_group_sizes)
fig.set_tight_layout(tight=True)

plt.savefig("images/plot_sample_size_fair_preds.pdf")

# %%
fig, ax = plt.subplots(figsize=(12, 8))

plot_models = ["lr", "lr_dm_threshold", "lr_eo_threshold"]
plot_method = "shadow_attack_loss"
plot_df = fair_results.query(f"model in {plot_models} and method == '{plot_method}'")
plot_df["vuln_diff"] = 2 * (plot_df.vuln_b - plot_df.vuln_a) * 100
plot_df["model"] = plot_df["model"].replace(renaming_dict)
plot_df = plot_df.replace({
    "attack_gbc_preds": "$\hat Y$",
    "attack_gbc_loss": "$\ell(\hat Y, Y)$",
})

plot_df = plot_df.rename(columns={
    "model": "Model",
    "treatment_size": "Subgroup size",
    "vuln_b": "Subgroup vuln.",
    "vuln_diff": "Disparity",
})

sns.barplot(
    data=plot_df,
    x="Subgroup size", y="Disparity",
    hue="Model",
    ax=ax,
    hue_order=[renaming_dict[model] for model in plot_models]
)
ax.set_ylim(-0.5, 2.2)
# ax.set_xscale("log")

add_significance_markers(ax, fair_results.query(f"method == '{plot_method}'"),
                         hue_values=plot_models, x_values=treatment_group_sizes)
fig.set_tight_layout(tight=True)
plt.legend().remove()

plt.savefig("images/plot_sample_size_fair_loss.pdf")

# %%
fig, ax = plt.subplots(figsize=(12, 8))

plot_models = ["lr", "lr_dm_threshold", "lr_eo_threshold"]
plot_df = fair_results.query(f"model in {plot_models}")
plot_df["model"] = plot_df["model"].replace(renaming_dict)

plot_df = plot_df.rename(columns={
    "model": "Model",
    "treatment_size": "Subgroup size",
    "test_acc": "Test acc." ,
})

sns.barplot(data=plot_df,
            x="Subgroup size", y="Test acc.",
            hue="Model", ax=ax)
# ax.set_xscale("log")
ax.get_legend().remove()
fig.set_tight_layout(tight=True)

# plt.savefig("images/plot_accuracy_fair.pdf")

# %% [markdown]
# ## Bias study

# %%
bias_results = pd.DataFrame()
bias_treatment_group_sizes = [25, 50, 75, 100, 150, 200, 500, 1000]
num_models = 230
num_targets = 200
global_setup = create_multinomial_setup(
    **{**DATA_SETUP},
) 

bias_methods = ["best_loss_threshold", "average_loss_threshold", "shadow_attack_loss"]
for i, method in enumerate(bias_methods):
    print(method)
    model_fn = model_zoo["threshold"]
    sim_res = run_sim_all_sizes(
        model_fn,
        bias_treatment_group_sizes,
        dataset_gen=SimDatasetGen(global_setup),
        seed=BASE_SEED,  # Use the same datasets.
        method=method,
        num_models=num_models,
        num_targets=num_targets if "shadow" in method else None,
        parallel=True,
    )
    bias_results = bias_results.append(
        sim_res.assign(method=method), ignore_index=True)

# %%
for method, size in itertools.product(bias_methods, bias_treatment_group_sizes):
    df = bias_results.query(f"treatment_size == {size} and method == '{method}'")
    t, p = stats.ttest_rel(df.vuln_a, df.vuln_b)
    bias_results.loc[df.index, "p"] = p
    bias_results.loc[df.index, "t"] = t
    
bias_results.groupby(["method", "treatment_size"]).p.mean().reset_index()

# %%
# bias_results.to_csv(DATA_FILE_FORMAT.format("apr9_bias"))

# %%
fig, ax = plt.subplots(figsize=(12, 8))

plot_df = bias_results.copy()
plot_df["vuln_diff"] = 2 * (plot_df.vuln_b - plot_df.vuln_a) * 100
plot_df = plot_df.replace({
    "best_loss_threshold": "Opt. loss threshold",
    "average_loss_threshold": "Avg. loss threshold",
    "shadow_attack_loss": "Shadow model attack",
})

plot_df = plot_df.rename(columns={
    "treatment_size": "Subgroup size",
    "vuln_b": "Subgroup vuln.",
    "vuln_diff": "Disparity estimate",
})

sns.barplot(
    data=plot_df,
    x="Subgroup size", y="Disparity estimate",
    hue="method",
    ax=ax,
)
# ax.set_xscale("log")

add_significance_markers(ax, bias_results,
                         x_values=bias_treatment_group_sizes, hue_values=bias_methods,
                         x_label="treatment_size", hue_label="method")
fig.set_tight_layout(tight=True)

# plt.savefig("images/plot_estimation_bias.pdf")

# %%
# Train/test sampler
class ConstrainedFinitePopulationSampler:
    def __init__(self, X_a, y_a, X_b, y_b, seed=0):
        self.gen = np.random.RandomState(seed)
        group_a_size = len(y_a)
        group_b_size = len(y_b)

        self.current_partition_a = 0
        self.current_partition_b = 0
        self._generate_partition_a(seed)
        self._generate_partition_b(seed)

    def _generate_partition_a(self, rng):
        X_train, X_test, y_train, y_test = train_test_split(
            self.X_a, self.y_a, test_size=0.5, random_state=rng)
        print(X_train[0])
        self.group_partitions_a = [(X_train, y_train), (X_test, y_test)]
    
    def _generate_partition_b(self, rng):
        X_train, X_test, y_train, y_test = train_test_split(
            self.X_b, self.y_b, test_size=0.5, random_state=rng)
        self.group_partitions_b = [(X_train, y_train), (X_test, y_test)]

    def __call__(self, _, rng, size_a, size_b):
        if size_a > 0:
            X, y = self.group_partitions_a[self.current_partition_a]
            self.current_partition_a = (self.current_partition_a + 1) % 2
            if self.current_partition_a == 0:
                self._generate_partition_a(rng)
            return X.iloc[:size_a], y.iloc[:size_a]
        
        elif size_b > 0:
            X, y = self.group_partitions_b[self.current_partition_b]
            self.current_partition_b = (self.current_partition_b + 1) % 2
            if self.current_partition_b == 0:
                self._generate_partition_b(rng)
            return X.iloc[:size_b], y.iloc[:size_b]
        
class FinitePopDatasetGen:
    def __init__(self, X_a, y_a, X_b, y_b):
        group_a_size = len(y_a)
        group_b_size = len(y_b)
        self.X_a = pd.DataFrame(X_a.values, index=range(group_a_size))
        self.y_a = pd.Series(y_a.values, index=range(group_a_size))
        self.X_b = pd.DataFrame(X_b.values, index=range(group_a_size, group_a_size + group_b_size))
        self.y_b = pd.Series(y_b.values, index=range(group_a_size, group_a_size + group_b_size))
        
    def __call__(self, rng, treatment_group_size, control_group_size):
        if control_group_size > 0:
            X_a_train, X_a_test, y_a_train, y_a_test = train_test_split(
                self.X_a, self.y_a, test_size=0.5, random_state=rng)
        
        X_b_train, X_b_test, y_b_train, y_b_test = train_test_split(
            self.X_b, self.y_b, test_size=0.5, random_state=rng)
        
        return (X_a_train.iloc[:control_group_size],
                y_a_train.iloc[:control_group_size],
                X_b_train.iloc[:treatment_group_size],
                y_b_train.iloc[:treatment_group_size],
                X_a_test.iloc[:control_group_size],
                y_a_test.iloc[:control_group_size],
                X_b_test.iloc[:treatment_group_size],
                y_b_test.iloc[:treatment_group_size])

        
X_a, y_a = gen_sim_data(global_setup, np.random.RandomState(rep), 2 * max_size, 0)
X_b, y_b = gen_sim_data(global_setup, np.random.RandomState(rep), 0, pop_size)
sampler = FinitePopDatasetGen(X_a, y_a, X_b, y_b)
rng1 = np.random.RandomState(0)
rng2 = np.random.RandomState(1)
sampler(rng1, 1, 1)[0][0], \
sampler(rng2, 1, 1)[0][0],

# %%
max_size = 1000
pop_size = 200
num_targets_vals = [30, 50, 80, 100]
treatment_group_sizes = [100]
reps = range(30)
global_setup = create_multinomial_setup(**DATA_SETUP)

method = "average_loss_threshold"
sample_modes = ["Finite population", "Theoretical"]
test_methods = ["Naive"]
models = ["threshold", "lr"]

param_grid = list(itertools.product(
    sample_modes, models, treatment_group_sizes, num_targets_vals))
test_results = pd.DataFrame()
for rep in tqdm.tqdm(reps):
    # Pre-sampled finite population
    rng = np.random.RandomState(rep)
    X_a, y_a = gen_sim_data(global_setup, rng, 2 * max_size, 0)
    X_b, y_b = gen_sim_data(global_setup, rng, 0, pop_size)
    
    for i, (sample_mode, model, treatment_group_size, num_targets) in enumerate(param_grid):
        # Set up the simulated data.
        if sample_mode == "Finite population":
            dataset_gen = FinitePopDatasetGen(X_a, y_a, X_b, y_b)
        else:
            dataset_gen = SimDatasetGen(global_setup)

        model_fn = model_zoo[model]
        sim_data = get_attack_data(
            model_fn,
            [treatment_group_size],
            dataset_gen=dataset_gen,
            max_size=max_size,
            seed=hash(f"{rep}-{i}") % 2**32,
            num_models=num_targets,
            parallel=True,
        )

        # Evaluate vulnerabilities.    
        size_data = sim_data.query(f"treatment_size == {treatment_group_size}")
        group_a_data = size_data.query("group == 'a'").reset_index()
        group_b_data = size_data.query("group == 'b'").reset_index()
        group_a_size = int(group_a_data.group_size.mean())
        group_b_size = int(group_b_data.group_size.mean())
        
        vuln_a = eval_sim_vuln(group_a_data, method, parallel=True, microdata=True)
        vuln_b = eval_sim_vuln(group_b_data, method, parallel=True, microdata=True)
        
        avg_vuln_a = vuln_a.groupby("model_id").vuln.mean()
        avg_vuln_b = vuln_b.groupby("model_id").vuln.mean()
        
#         if model == "threshold" and sample_mode == "Theoretical":
#             null_vuln_df = pd.concat([
#                 vuln_a.reset_index().assign(subgroup="a"),
#                 vuln_b.reset_index().assign(subgroup="b")
#             ])
#         elif model == "nn_32" and sample_mode == "Theoretical":
#             nn_vuln_df = pd.concat([
#                 vuln_a.reset_index().assign(subgroup="a"),
#                 vuln_b.reset_index().assign(subgroup="b")
#             ])
        
        # Statistics hackery.      
        for test_method in test_methods:
            if test_method == "Correlations":
                vuln_mat_a = vulns_to_mat(vuln_a)
                vuln_mat_b = vulns_to_mat(vuln_b)
                
                var_a = np.var(vuln_mat_a, axis=0, ddof=1) * group_a_data.group_size.mean()**(-1)
                var_b = np.var(vuln_mat_b, axis=0, ddof=1) * group_b_data.group_size.mean()**(-1)
                
                cov_a = np.zeros((num_targets, num_targets))
                for i in range(num_targets):
                    for j in range(i+1, num_targets):
                        example_cov_sum = np.cov(vuln_mat_a, ddof=1).sum()
                        cov_a[i, j] = example_cov_sum * group_a_data.group_size.mean()**(-2)
                
                cov_b = np.zeros((num_targets, num_targets))
                for i in range(num_targets):
                    for j in range(i+1, num_targets):
                        example_cov_sum = np.cov(vuln_mat_b, ddof=1).sum()
                        cov_b[i, j] = example_cov_sum * group_b_data.group_size.mean()**(-2)
                            
                cov_a_sum = 0
                cov_b_sum = 0
                for i in range(num_targets):
                    for j in range(i+1, num_targets):
                        cov_a_sum += cov_a[i, j]
                        cov_b_sum += cov_b[i, j]

                loc = avg_vuln_b.mean() - avg_vuln_a.mean()
                std = np.sqrt((var_a.sum() + var_b.sum() + 2 * (cov_a_sum + cov_b_sum)) / num_targets**2)

                p = 2*(1 - stats.norm.cdf(abs(loc / std)))
                print(f"{model} {sample_mode}: {loc=} {p=}")
                print(f"{var_a.mean()=} {var_b.mean()=} {cov_a_sum=} {cov_b_sum=}")

            if test_method == "Naive":
                _, p = stats.ttest_rel(avg_vuln_a, avg_vuln_b)
                
            print(f"{sample_mode=} {model=} {num_targets=} {test_method=} {p=}")

            test_results = test_results.append(dict(
                rep=rep,
                sample_mode=sample_mode,
                model=model,
                num_targets=num_targets,
                shadow_data_prop=treatment_group_size / pop_size,
                disparity=avg_vuln_b.mean() - avg_vuln_a.mean(),
                test=test_method,
                p_value=p,
            ), ignore_index=True)


# %%
test_results.groupby(["test", "sample_mode", "model", "num_targets"]).p_value.mean()

# %%
test_results.groupby(["test", "sample_mode", "model", "num_targets"]).disparity.mean()

# %%
plot_df = test_results.query("test == 'Naive'")
plot_df = plot_df.rename(columns={
    "sample_mode": "Sampling", "num_targets": "Targets", "p_value": "p-value",
    "shadow_data_prop": "\%"
})
plot_df = plot_df.replace({
    "lr": "LR",
    "nn_32": "NN",
    "threshold": "Null",
    "Overlapping": "Proposed",
    "Unlimited access": "Theoretical",
})

g = sns.FacetGrid(data=plot_df, col="model", height=10)
g.map_dataframe(sns.stripplot, x="Targets", y="disparity", hue="Sampling",
                palette=sns.color_palette(), dodge=True)
g.map_dataframe(sns.boxplot, x="Targets", y="disparity", hue="Sampling",
                palette=sns.color_palette(), showfliers=False, boxprops=dict(alpha=0.1))
g.set_axis_labels("Targets", "Disparity")
g.add_legend()

g = sns.FacetGrid(data=plot_df, col="model", height=10)
g.map_dataframe(sns.stripplot, x="Targets", y="p-value", hue="Sampling",
                palette=sns.color_palette(), dodge=True)
g.map_dataframe(sns.boxplot, x="Targets", y="p-value", hue="Sampling",
                palette=sns.color_palette(), showfliers=False, boxprops=dict(alpha=.1))
g.set_axis_labels("Targets", "p-value")
g.add_legend()

ax1, ax2 = g.axes[0]
level = 0.01
ax1.axhline(level, ls="--", color="grey")
ax2.axhline(level, ls="--", color="grey")

# %% [markdown]
# ## Distributions study

# %%
import warnings
warnings.filterwarnings("ignore")

# %%
distro_results = pd.DataFrame()

# %%
num_models = 30
meta_reps = 30
max_size = 1000
method = "average_loss_threshold"

for model in model_names:
    print(model)
    if model in ["lr_eo", "lr_dm"]:
        continue
        
    clf = model_zoo[model]
    for seed in tqdm.trange(meta_reps):
        setup = create_multinomial_setup(seed=seed, **DATA_SETUP)
        sim_res = run_sim_all_sizes(
            clf, [max_size], SimDatasetGen(setup),
            seed=seed,
            num_models=num_models,
            max_size=max_size,
            method=method
        )
        distro_results = distro_results.append(
            sim_res.assign(seed=seed, model=model), ignore_index=True)

# %%
plot_df = distro_results.copy()
plot_df["vuln_diff"] = 2 * (plot_df.vuln_a - plot_df.vuln_b) * 100
plot_df.model = plot_df.model.replace(renaming_dict)

fig, ax = plt.subplots(figsize=(12, 8))

sns.boxplot(data=plot_df,
            x="vuln_diff", y="model", ax=ax, color="white")
# sns.swarmplot(data=plot_df, x="vuln_diff", y="model", ax=ax)

ax.set_xlabel("Disparity")
ax.set_ylabel("Model")
fig.set_tight_layout(tight=True)

# plt.savefig("images/plot_distro_normal.pdf")
