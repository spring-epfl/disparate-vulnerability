import pandas as pd
import numpy as np
from os.path import join

def prepare_compas(file_path, binarize_features_dict=None, replace_dict=None, dummified=True, drop_first=False,
                   drop_duplicates=True, binary_risk_score = False, _list=None):

    train_data = pd.read_csv(file_path.join("train.csv"))
    test_data = pd.read_csv(file_path.join("test.csv"))
    orig_data = train_data.append(test_data)

    # column re-ordering
    # orig_data = orig_data[['Age Category', 'Race', 'Gender',
    #                        'Felony or Misdemeanor', 'Priors Count',
    #                        'Algorithm Risk Evaluation', 'Risk Score',
    #                        'Ground Truth']]

    # attribute renaming
    orig_data["Felony or Misdemeanor"] = orig_data["Felony or Misdemeanor"].apply(
        lambda val: "Felony" if val == "F" else "Misdemeanor" if val == "M" else None)
    orig_data["Ground Truth"] = orig_data["Ground Truth"].apply(
        lambda a: "Will reoffend" if a == 1 else "Will not reoffend")

    # make any replacements
    data = orig_data.replace(replace_dict)

    # binarize data
    if binarize_features_dict is not None:
        for feat, acceptable_feats in binarize_features_dict.items():
            data[feat] = data[feat].where(data[feat].isin(acceptable_feats), "Other_{}".format(feat))

    # pick a subset of data
    if _list is None:
        data = data[['Age Category', 'Race', 'Gender', 'Felony or Misdemeanor',
                     'Priors Count', 'Ground Truth']]
    else:
        data = data[_list]

    if binary_risk_score:
        data['Risk Score'] = data['Risk Score'].apply(
            lambda s: "High risk to reoffend" if s > 5 else "Low risk to reoffend")

    # drop duplicates
    if drop_duplicates:
        len_before = len(data)
        data = data.drop_duplicates().reset_index(drop=True)
        len_after = len(data)
        print("Removed {} duplicates: {} -> {}".format(len_before - len_after, len_before, len_after))

    if dummified:
        data = pd.get_dummies(data,
                              drop_first=drop_first)
        #         print(data.head)
        # cast as all integers
        data = data.astype(int)

    print("There are no duplicates." if not np.any(data.duplicated()) else "Duplicates exist.")