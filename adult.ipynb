{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SHUFFLES = 35\n",
    "FIGWIDTH = 8\n",
    "ALPHA = 0.005\n",
    "LOAD_FROM_PICKLE = True\n",
    "PICKLE = \"results/adult_race_100_reps.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from experiment import run_generic_experiment\n",
    "from model_zoo import model_zoo, renaming_dict, fit_args\n",
    "from utils import metrics_dict_to_dataframe\n",
    "from utils import convert_max_disparity_to_wide, get_latex_table\n",
    "from disparity import compute_emm, compute_anovas, max_disparity\n",
    "from plotting import plot_stat_heatmaps, plot_vuln_dists\n",
    "from loaders.adult import prepare_adult\n",
    "\n",
    "import plot_params\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_undummified = prepare_adult(\"./data/adult/train.csv\", \"./data/adult/test.csv\", binarize=False).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_undummified[\"race\"].value_counts().index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 is the maximum number of ones in a ADULT feature vector after one-hot encoding.\n",
    "model_zoo[\"lr_dp_eps1\"].named_steps.clf.data_norm = np.sqrt(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renaming_dict = dict(**renaming_dict,\n",
    "    **{\n",
    "        \"regular\": \"Regular\",\n",
    "        \"discriminating\": \"Discriminating\",\n",
    "        \"Native American\": \"Native\\nAmerican\",\n",
    "        \"African-American\": \"African-\\nAmerican\",\n",
    "    }\n",
    ")\n",
    "\n",
    "model_order = [renaming_dict[model] for model in [\n",
    "    \"lr\",\n",
    "    \"lr_eo\",\n",
    "    \"nn_6\",\n",
    "    \"nn_100\",\n",
    "    \"nn_500\",\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = ['White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other']\n",
    "subgroups_short = [\"WH\", \"BL\", \"AI\", \"AE\", \"OT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Run the experiments on 'race' subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_FROM_PICKLE:\n",
    "    metrics_race = pd.read_pickle(PICKLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_FROM_PICKLE:\n",
    "    metrics_race = {}\n",
    "    for model_name, clf in model_zoo.items():\n",
    "        print(model_name)\n",
    "        metrics_race[model_name] = run_generic_experiment(\n",
    "            data=data_undummified,\n",
    "            y_label=\"income\",\n",
    "            z_label=\"race\",\n",
    "            z_values=subgroups,\n",
    "            num_batches=NUM_SHUFFLES,\n",
    "            synthetic_bins=10,\n",
    "            parallelize=True,\n",
    "            clf=clf,\n",
    "            fit_args=fit_args.get(model_name, {})\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_FROM_PICKLE:\n",
    "    pd.to_pickle(metrics_race, PICKLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in metrics_race.items():\n",
    "    metrics_race[k] = metrics_race[k].query(f\"batch_no < {NUM_SHUFFLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacks to put group sizes on the plot legends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = dict(zip(subgroups, subgroups_short))\n",
    "subgroup_sizes = {}\n",
    "subgroup_renaming_dict = {}\n",
    "for subgroup in subgroups:\n",
    "    subgroup_sizes[subgroup] = int(\n",
    "        metrics_race[\"lr\"].query(f\"subgroup == '{subgroup}'\").support.mean()\n",
    "    )\n",
    "total = sum(subgroup_sizes.values())\n",
    "subgroups = list(sorted(subgroups, key=lambda k: -subgroup_sizes[k]))\n",
    "\n",
    "for subgroup in subgroups + [\"Overall\"]:\n",
    "    if subgroup == \"Overall\":\n",
    "        size = total\n",
    "    else:\n",
    "        size = subgroup_sizes[subgroup]\n",
    "    subgroup_renaming_dict[subgroup] = \"{}: {}\\n({}\\\\%)\".format(\n",
    "        subgroup, size, round(size / total * 100)\n",
    "    )\n",
    "\n",
    "subgroup_annotated_ordered = {s: subgroup_renaming_dict[s] for s in (subgroups)}\n",
    "renaming_dict = dict(renaming_dict, **subgroup_annotated_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [42, 65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (2*FIGWIDTH, FIGWIDTH)\n",
    "options = dict(pad_inches=1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=figsize, sharey=False, sharex=True,\n",
    "                         gridspec_kw=dict(height_ratios=[4, 1], hspace=0.))\n",
    "plot_vuln_dists(metrics_race,\n",
    "                models=[\"lr\", \"nn_6\"],\n",
    "                axes=axes,\n",
    "                renaming_dict=renaming_dict,\n",
    "                order=subgroup_annotated_ordered.values(),\n",
    "                vuln_method=\"counts\",\n",
    "                ignore_subgroups=[\"Native American\", \"Asian\"],\n",
    "                percentages=True,\n",
    "                legend=True,\n",
    "                legend_ax_id=0)\n",
    "\n",
    "for ax in itertools.chain(axes[0], axes[1]):\n",
    "    ax.set_xlim(*xlim)\n",
    "    ax.set_facecolor('none')\n",
    "    ax.set_alpha(0)\n",
    "    \n",
    "axes[0, 0].legend(bbox_to_anchor=(-0.27, 0.06))\n",
    "    \n",
    "fig.set_tight_layout(tight=True)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "fig.patch.set_alpha(0.4)\n",
    "fig.savefig(\"images/adult_race_dist_plots_normal.pdf\", **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (2*FIGWIDTH, FIGWIDTH)\n",
    "options = dict(pad_inches=1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=figsize, sharey=False, sharex=True,\n",
    "                         gridspec_kw=dict(height_ratios=[4, 1], hspace=0.))\n",
    "plot_vuln_dists(metrics_race,\n",
    "                models=[\"nn_100\", \"nn_500\"],\n",
    "                axes=axes,\n",
    "                renaming_dict=renaming_dict,\n",
    "                order=subgroup_annotated_ordered.values(),\n",
    "                vuln_method=\"counts\",\n",
    "                ignore_subgroups=[\"Native American\", \"Asian\"],\n",
    "                percentages=True,\n",
    "                legend=True,\n",
    "                legend_ax_id=0)\n",
    "\n",
    "for ax in itertools.chain(axes[0], axes[1]):\n",
    "    ax.set_xlim(*xlim)\n",
    "    ax.set_facecolor('none')\n",
    "    ax.set_alpha(0)\n",
    "    \n",
    "axes[0, 0].legend(bbox_to_anchor=(-0.27, 0.06))\n",
    "    \n",
    "fig.set_tight_layout(tight=True)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "fig.patch.set_alpha(0.4)\n",
    "fig.savefig(\"images/adult_race_dist_plots_overfitting.pdf\", **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (FIGWIDTH * 1.28, FIGWIDTH)\n",
    "options = dict(pad_inches=1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=figsize, sharey=False, sharex=True,\n",
    "                         gridspec_kw=dict(height_ratios=[4, 1], hspace=0.))\n",
    "plot_vuln_dists(metrics_race,\n",
    "                models=[\"lr_eo\"],\n",
    "                axes=np.expand_dims(axes, 1),\n",
    "                renaming_dict=renaming_dict,\n",
    "                vuln_method=\"counts\",\n",
    "                order=subgroup_annotated_ordered.values(),\n",
    "                percentages=True,\n",
    "                legend=False)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlim(*xlim)\n",
    "    ax.set_facecolor('none')\n",
    "    ax.set_alpha(0)\n",
    "    \n",
    "fig.set_tight_layout(tight=True)\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "fig.patch.set_alpha(0.4)\n",
    "fig.savefig(\"images/adult_race_dist_plots_eo.pdf\", **options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_anovas_by_model = compute_anovas(\n",
    "    metrics_race,\n",
    "    attacker=\"regular\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminating_anovas_by_model = compute_anovas(\n",
    "    metrics_race,\n",
    "    attacker=\"discriminating\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, (p_value, _) in regular_anovas_by_model.items():\n",
    "    print(f\"{model}: {p_value:.4f} (regular)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, (p_value, _) in discriminating_anovas_by_model.items():\n",
    "    print(f\"{model}: {p_value:.4f} (discriminating)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_p_matrices_by_model = compute_emm(\n",
    "    regular_anovas_by_model,\n",
    "    subgroups=subgroups,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminating_p_matrices_by_model = compute_emm(\n",
    "    discriminating_anovas_by_model,\n",
    "    subgroups=subgroups,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(2*FIGWIDTH, FIGWIDTH), sharex=True, sharey=True)\n",
    "cmap = sns.color_palette([\"#ECF0F1\", \"#E74C3C\"])\n",
    "for ax_id, model_name in enumerate([\"lr\", \"nn_6\"]):\n",
    "    ax = axes[ax_id]\n",
    "    ax.set_title(renaming_dict[model_name])\n",
    "    metrics = metrics_race[model_name].replace(group_names)\n",
    "    plot_stat_heatmaps(\n",
    "        metrics,\n",
    "        [t.rename(index=group_names, columns=group_names)\n",
    "         for t in regular_p_matrices_by_model[model_name]],\n",
    "        subgroups=subgroups_short,\n",
    "        alpha=ALPHA,\n",
    "        ax=ax,\n",
    "        xticklabels=True,\n",
    "        cmap=cmap\n",
    "    )\n",
    "fig.set_tight_layout(tight=True)\n",
    "plt.show()\n",
    "fig.savefig(\"images/adult_p_plots_normal_regular.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(2*FIGWIDTH, FIGWIDTH), sharex=True, sharey=True)\n",
    "cmap = sns.color_palette([\"#ECF0F1\", \"#E74C3C\"])\n",
    "for ax_id, model_name in enumerate([\"lr\", \"nn_6\"]):\n",
    "    ax = axes[ax_id]\n",
    "    ax.set_title(renaming_dict[model_name])\n",
    "    metrics = metrics_race[model_name].replace(group_names)\n",
    "    plot_stat_heatmaps(\n",
    "        metrics,\n",
    "        [t.rename(index=group_names, columns=group_names)\n",
    "         for t in discriminating_p_matrices_by_model[model_name]],\n",
    "        subgroups=subgroups_short,\n",
    "        alpha=ALPHA,\n",
    "        ax=ax,\n",
    "        xticklabels=True,\n",
    "        cmap=cmap\n",
    "    )\n",
    "fig.set_tight_layout(tight=True)\n",
    "plt.show()\n",
    "fig.savefig(\"images/adult_p_plots_normal_discriminating.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(2*FIGWIDTH, FIGWIDTH), sharex=True, sharey=True)\n",
    "cmap = sns.color_palette([\"#ECF0F1\", \"#E74C3C\"])\n",
    "for ax_id, model_name in enumerate([\"nn_100\", \"nn_500\"]):\n",
    "    ax = axes[ax_id]\n",
    "    ax.set_title(renaming_dict[model_name])\n",
    "    metrics = metrics_race[model_name].replace(group_names)\n",
    "    plot_stat_heatmaps(\n",
    "        metrics,\n",
    "        [t.rename(index=group_names, columns=group_names)\n",
    "         for t in regular_p_matrices_by_model[model_name]],\n",
    "        subgroups=subgroups_short,\n",
    "        alpha=ALPHA,\n",
    "        ax=ax,\n",
    "        xticklabels=True,\n",
    "        cmap=cmap\n",
    "    )\n",
    "fig.set_tight_layout(tight=True)\n",
    "plt.show()\n",
    "fig.savefig(\"images/adult_p_plots_overfit_regular.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(2*FIGWIDTH, FIGWIDTH), sharex=True, sharey=True)\n",
    "cmap = sns.color_palette([\"#ECF0F1\", \"#E74C3C\"])\n",
    "for ax_id, model_name in enumerate([\"nn_100\", \"nn_500\"]):\n",
    "    ax = axes[ax_id]\n",
    "    ax.set_title(renaming_dict[model_name])\n",
    "    metrics = metrics_race[model_name].replace(group_names)\n",
    "    plot_stat_heatmaps(\n",
    "        metrics,\n",
    "        [t.rename(index=group_names, columns=group_names)\n",
    "         for t in discriminating_p_matrices_by_model[model_name]],\n",
    "        subgroups=subgroups_short,\n",
    "        alpha=ALPHA,\n",
    "        ax=ax,\n",
    "        xticklabels=True,\n",
    "        cmap=cmap\n",
    "    )\n",
    "fig.set_tight_layout(tight=True)\n",
    "plt.show()\n",
    "fig.savefig(\"images/adult_p_plots_overfit_discriminating.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(FIGWIDTH, FIGWIDTH), sharex=True, sharey=True)\n",
    "cmap = sns.color_palette([\"#ECF0F1\", \"#E74C3C\"])\n",
    "model_name = \"lr_eo\"\n",
    "ax.set_title(renaming_dict[model_name])\n",
    "metrics = metrics_race[model_name].replace(dict(zip(subgroups, subgroups_short)))\n",
    "plot_stat_heatmaps(\n",
    "    metrics,\n",
    "    [t.rename(index=group_names, columns=group_names) for t in regular_p_matrices_by_model[model_name]],\n",
    "    subgroups=subgroups_short,\n",
    "    alpha=ALPHA, ax=ax,\n",
    "    xticklabels=True,\n",
    "    mode=\"meta\", cmap=cmap\n",
    ")\n",
    "fig.set_tight_layout(tight=True)\n",
    "plt.show()\n",
    "fig.savefig(\"images/adult_p_plots_eo_regular.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(FIGWIDTH, FIGWIDTH), sharex=True, sharey=True)\n",
    "cmap = sns.color_palette([\"#ECF0F1\", \"#E74C3C\"])\n",
    "model_name = \"lr_eo\"\n",
    "ax.set_title(renaming_dict[model_name])\n",
    "metrics = metrics_race[model_name].replace(dict(zip(subgroups, subgroups_short)))\n",
    "plot_stat_heatmaps(\n",
    "    metrics,\n",
    "    [t.rename(index=group_names, columns=group_names) for t in discriminating_p_matrices_by_model[model_name]],\n",
    "    subgroups=subgroups_short,\n",
    "    alpha=ALPHA, ax=ax,\n",
    "    xticklabels=True,\n",
    "    mode=\"meta\", cmap=cmap\n",
    ")\n",
    "fig.set_tight_layout(tight=True)\n",
    "plt.show()\n",
    "fig.savefig(\"images/adult_p_plots_eo_discriminating.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_disparity_race_wide = pd.DataFrame()\n",
    "\n",
    "for model_name, metrics in metrics_race.items():\n",
    "    # calculate max disparity table\n",
    "    max_disparity_race = max_disparity({model_name: metrics}, renaming_dict=renaming_dict)\n",
    "\n",
    "    # convert to wide format (Regular | Discriminating)\n",
    "    max_disparity_race_wide = max_disparity_race_wide.append(\n",
    "        convert_max_disparity_to_wide(max_disparity_race))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming dictionaries\n",
    "renaming_dict2 = dict(attacker=\"Attacker\", model=\"Model\", target_train_acc=\"Train Acc.\", target_test_acc=\"Test Acc.\", \n",
    "                      overfitting_gap=\"Overfitting\", vuln=\"Vulnerability\", max_vuln_disparity=\"Max Vuln. Disparity\")\n",
    "\n",
    "paper_table = (max_disparity_race_wide\n",
    "               .rename(columns=renaming_dict, level=0)\n",
    "               .rename(columns=renaming_dict2, level=1)\n",
    "               .rename(index=renaming_dict))\n",
    "\n",
    "# Prettification and ordering\n",
    "paper_table = (paper_table * 100).round(2)\n",
    "paper_table = paper_table.reindex(model_order)\n",
    "\n",
    "print(get_latex_table(paper_table))\n",
    "paper_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
