{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subgroup Distibutions Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from plot_utils import plot_stat_heatmaps, plot_vuln_dists\n",
    "from experiment import *\n",
    "from fixes import model_zoo, renaming_dict\n",
    "\n",
    "import plot_params\n",
    "\n",
    "from paper_commons import diagnostics\n",
    "from helpers import metrics_dict_to_dataframe, compute_p_matrices, max_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "# import cv2\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "# import umap\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utkface import prepare_utkface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTKFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, meta_data = prepare_utkface(\"data/UTKFace/\", \"data/UTKFace.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = meta_data.race.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Indian', 'Black', 'Other', 'White', 'Asian'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(meta_data, z_label, subgroups, random_state=None):\n",
    "    _data = meta_data.loc[meta_data[z_label].isin(subgroups)]\n",
    "    counts = _data.race.value_counts()\n",
    "    smallest_subgroup_size = counts.min()\n",
    "    state = np.random.RandomState(random_state) if random_state is not None else np.random.RandomState() \n",
    "    \n",
    "    train_idx_dict = {}\n",
    "    test_idx_dict = {}\n",
    "    for group in subgroups:\n",
    "        group_idx = state.choice(\n",
    "                _data.query(f\"{z_label} == '{group}'\").index, size=int(smallest_subgroup_size)).tolist()\n",
    "        \n",
    "        train_idx_dict[group] = group_idx[:int(smallest_subgroup_size/2)]\n",
    "        test_idx_dict[group] = group_idx[int(smallest_subgroup_size/2):]\n",
    "        \n",
    "    return train_idx_dict, test_idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_dict, test_idx_dict = subsample(meta_data, \"race\", list(filter(lambda x: x != \"Other\", subgroups)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1717, 1717), (1717, 1717), (1717, 1717), (1717, 1717)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(len(train_set), len(test_idx_dict[group])) for group, train_set in train_idx_dict.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        8224      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 535,142\n",
      "Trainable params: 535,142\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clf = tf.keras.Sequential()\n",
    "\n",
    "clf.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(32,32,3))) \n",
    "clf.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "clf.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "clf.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "clf.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "clf.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "clf.add(tf.keras.layers.Flatten())\n",
    "clf.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "clf.add(tf.keras.layers.Dropout(0.5))\n",
    "clf.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "# clf.compile(loss='sparse_categorical_crossentropy',\n",
    "#              optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"models/subgroup_disto_model_{}.h5\"\n",
    "fit_args = dict(batch_size=64, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = [200, 500, 700, 1000, 1300, 1717]\n",
    "# train_sizes = [200]\n",
    "validation_ratio = 0.2\n",
    "reps = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b771938aff4172b43a635583b86592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 3s 4ms/sample - loss: 1.6501 - accuracy: 0.4109 - val_loss: 1.6395 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 194us/sample - loss: 1.6036 - accuracy: 0.4313 - val_loss: 1.6326 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 161us/sample - loss: 1.5775 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 102us/sample - loss: 1.5799 - accuracy: 0.4313 - val_loss: 1.6016 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5702 - accuracy: 0.4313 - val_loss: 1.5980 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5713 - accuracy: 0.4313 - val_loss: 1.5981 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 98us/sample - loss: 1.5730 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 149us/sample - loss: 1.5693 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5701 - accuracy: 0.4313 - val_loss: 1.6006 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5632 - accuracy: 0.4313 - val_loss: 1.5953 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5765 - accuracy: 0.4313 - val_loss: 1.6090 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5797 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5673 - accuracy: 0.4313 - val_loss: 1.5985 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 143us/sample - loss: 1.5669 - accuracy: 0.4313 - val_loss: 1.5904 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5667 - accuracy: 0.4313 - val_loss: 1.5981 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5677 - accuracy: 0.4313 - val_loss: 1.5945 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5669 - accuracy: 0.4313 - val_loss: 1.5982 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5691 - accuracy: 0.4313 - val_loss: 1.5941 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5673 - accuracy: 0.4313 - val_loss: 1.5903 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5662 - accuracy: 0.4313 - val_loss: 1.5910 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5738 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5684 - accuracy: 0.4313 - val_loss: 1.5959 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5690 - accuracy: 0.4313 - val_loss: 1.5914 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5649 - accuracy: 0.4313 - val_loss: 1.5917 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5663 - accuracy: 0.4313 - val_loss: 1.5889 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5602 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 140us/sample - loss: 1.5622 - accuracy: 0.4313 - val_loss: 1.5878 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5619 - accuracy: 0.4313 - val_loss: 1.5860 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 139us/sample - loss: 1.5506 - accuracy: 0.4313 - val_loss: 1.5858 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5516 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.6965 - accuracy: 0.3969 - val_loss: 1.5889 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5854 - accuracy: 0.4313 - val_loss: 1.6297 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5802 - accuracy: 0.4313 - val_loss: 1.5947 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5630 - accuracy: 0.4313 - val_loss: 1.5992 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 98us/sample - loss: 1.5853 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 97us/sample - loss: 1.5695 - accuracy: 0.4313 - val_loss: 1.5985 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 96us/sample - loss: 1.5664 - accuracy: 0.4313 - val_loss: 1.5959 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5674 - accuracy: 0.4313 - val_loss: 1.5982 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5721 - accuracy: 0.4313 - val_loss: 1.6021 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5630 - accuracy: 0.4313 - val_loss: 1.5946 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5763 - accuracy: 0.4313 - val_loss: 1.6060 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5755 - accuracy: 0.4313 - val_loss: 1.5896 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5809 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5681 - accuracy: 0.4313 - val_loss: 1.5950 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5728 - accuracy: 0.4313 - val_loss: 1.5980 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5662 - accuracy: 0.4313 - val_loss: 1.5958 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5654 - accuracy: 0.4313 - val_loss: 1.5923 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5741 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5592 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5599 - accuracy: 0.4313 - val_loss: 1.6005 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5696 - accuracy: 0.4313 - val_loss: 1.5932 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5721 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5668 - accuracy: 0.4313 - val_loss: 1.5957 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5702 - accuracy: 0.4313 - val_loss: 1.5914 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5655 - accuracy: 0.4313 - val_loss: 1.5931 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 144us/sample - loss: 1.5666 - accuracy: 0.4313 - val_loss: 1.5889 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5620 - accuracy: 0.4313 - val_loss: 1.5902 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5610 - accuracy: 0.4313 - val_loss: 1.5895 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5724 - accuracy: 0.4313 - val_loss: 1.5916 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5593 - accuracy: 0.4313 - val_loss: 1.5884 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.6947 - accuracy: 0.4078 - val_loss: 1.5960 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5650 - accuracy: 0.4313 - val_loss: 1.6225 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 165us/sample - loss: 1.5823 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5718 - accuracy: 0.4313 - val_loss: 1.5962 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 104us/sample - loss: 1.5767 - accuracy: 0.4313 - val_loss: 1.6018 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 96us/sample - loss: 1.5678 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 97us/sample - loss: 1.5677 - accuracy: 0.4313 - val_loss: 1.6028 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 156us/sample - loss: 1.5692 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5793 - accuracy: 0.4313 - val_loss: 1.6058 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5644 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5810 - accuracy: 0.4313 - val_loss: 1.5951 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5656 - accuracy: 0.4313 - val_loss: 1.5970 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5745 - accuracy: 0.4313 - val_loss: 1.5962 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5636 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 145us/sample - loss: 1.5671 - accuracy: 0.4313 - val_loss: 1.5902 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5731 - accuracy: 0.4313 - val_loss: 1.5994 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5652 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5694 - accuracy: 0.4313 - val_loss: 1.5889 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5593 - accuracy: 0.4313 - val_loss: 1.5902 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5678 - accuracy: 0.4313 - val_loss: 1.5905 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5659 - accuracy: 0.4313 - val_loss: 1.5898 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5707 - accuracy: 0.4313 - val_loss: 1.5995 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5770 - accuracy: 0.4313 - val_loss: 1.5913 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5594 - accuracy: 0.4313 - val_loss: 1.5909 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5698 - accuracy: 0.4313 - val_loss: 1.5891 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5540 - accuracy: 0.4313 - val_loss: 1.5901 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 257us/sample - loss: 1.5687 - accuracy: 0.4313 - val_loss: 1.5848 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5532 - accuracy: 0.4313 - val_loss: 1.5873 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 285us/sample - loss: 1.5651 - accuracy: 0.4313 - val_loss: 1.5848 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 256us/sample - loss: 1.5528 - accuracy: 0.4313 - val_loss: 1.5830 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 885us/sample - loss: 1.6892 - accuracy: 0.4094 - val_loss: 1.6036 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5896 - accuracy: 0.4313 - val_loss: 1.6184 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 164us/sample - loss: 1.5751 - accuracy: 0.4313 - val_loss: 1.5945 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 102us/sample - loss: 1.5680 - accuracy: 0.4313 - val_loss: 1.6012 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5673 - accuracy: 0.4313 - val_loss: 1.5961 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 96us/sample - loss: 1.5654 - accuracy: 0.4313 - val_loss: 1.5971 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 198us/sample - loss: 1.5601 - accuracy: 0.4313 - val_loss: 1.5921 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 99us/sample - loss: 1.5767 - accuracy: 0.4313 - val_loss: 1.5965 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5689 - accuracy: 0.4313 - val_loss: 1.5994 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5753 - accuracy: 0.4313 - val_loss: 1.6006 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 153us/sample - loss: 1.5680 - accuracy: 0.4313 - val_loss: 1.5908 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5728 - accuracy: 0.4313 - val_loss: 1.6007 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5711 - accuracy: 0.4313 - val_loss: 1.5952 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5679 - accuracy: 0.4313 - val_loss: 1.5963 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5698 - accuracy: 0.4313 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5777 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5797 - accuracy: 0.4313 - val_loss: 1.5981 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5639 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5656 - accuracy: 0.4313 - val_loss: 1.5916 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5675 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5666 - accuracy: 0.4313 - val_loss: 1.5947 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 140us/sample - loss: 1.5620 - accuracy: 0.4313 - val_loss: 1.5890 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5656 - accuracy: 0.4313 - val_loss: 1.5899 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5595 - accuracy: 0.4313 - val_loss: 1.5921 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 139us/sample - loss: 1.5639 - accuracy: 0.4313 - val_loss: 1.5887 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5681 - accuracy: 0.4313 - val_loss: 1.6113 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 139us/sample - loss: 1.5602 - accuracy: 0.4313 - val_loss: 1.5863 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5598 - accuracy: 0.4313 - val_loss: 1.5950 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 386us/sample - loss: 1.5653 - accuracy: 0.4313 - val_loss: 1.5805 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5552 - accuracy: 0.4313 - val_loss: 1.5812 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 902us/sample - loss: 1.6907 - accuracy: 0.4000 - val_loss: 1.6039 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 117us/sample - loss: 1.5850 - accuracy: 0.4313 - val_loss: 1.6274 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 185us/sample - loss: 1.5834 - accuracy: 0.4313 - val_loss: 1.6022 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 167us/sample - loss: 1.5733 - accuracy: 0.4313 - val_loss: 1.5996 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 170us/sample - loss: 1.5721 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 170us/sample - loss: 1.5695 - accuracy: 0.4313 - val_loss: 1.5920 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 114us/sample - loss: 1.5712 - accuracy: 0.4313 - val_loss: 1.6001 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 171us/sample - loss: 1.5789 - accuracy: 0.4313 - val_loss: 1.5918 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5764 - accuracy: 0.4313 - val_loss: 1.6016 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 103us/sample - loss: 1.5786 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 103us/sample - loss: 1.5634 - accuracy: 0.4313 - val_loss: 1.5947 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 153us/sample - loss: 1.5689 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5637 - accuracy: 0.4313 - val_loss: 1.5941 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 93us/sample - loss: 1.5719 - accuracy: 0.4313 - val_loss: 1.5970 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5649 - accuracy: 0.4313 - val_loss: 1.5958 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5650 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 92us/sample - loss: 1.5633 - accuracy: 0.4313 - val_loss: 1.5951 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5749 - accuracy: 0.4313 - val_loss: 1.5913 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5642 - accuracy: 0.4313 - val_loss: 1.5918 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5655 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5668 - accuracy: 0.4313 - val_loss: 1.5941 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5695 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5711 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5658 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5656 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5590 - accuracy: 0.4313 - val_loss: 1.5940 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5694 - accuracy: 0.4313 - val_loss: 1.5947 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5655 - accuracy: 0.4313 - val_loss: 1.5921 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5619 - accuracy: 0.4313 - val_loss: 1.5940 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5602 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 896us/sample - loss: 1.6912 - accuracy: 0.4062 - val_loss: 1.5936 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5795 - accuracy: 0.4313 - val_loss: 1.6122 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 109us/sample - loss: 1.5662 - accuracy: 0.4313 - val_loss: 1.5954 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5797 - accuracy: 0.4313 - val_loss: 1.6101 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5770 - accuracy: 0.4313 - val_loss: 1.5941 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 104us/sample - loss: 1.5716 - accuracy: 0.4313 - val_loss: 1.5991 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 156us/sample - loss: 1.5736 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 146us/sample - loss: 1.5756 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 96us/sample - loss: 1.5647 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 93us/sample - loss: 1.5783 - accuracy: 0.4313 - val_loss: 1.6066 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5688 - accuracy: 0.4313 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5687 - accuracy: 0.4313 - val_loss: 1.6036 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5660 - accuracy: 0.4313 - val_loss: 1.5941 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5749 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5694 - accuracy: 0.4313 - val_loss: 1.5972 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5644 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5790 - accuracy: 0.4313 - val_loss: 1.6032 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5688 - accuracy: 0.4313 - val_loss: 1.5930 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5711 - accuracy: 0.4313 - val_loss: 1.5987 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 177us/sample - loss: 1.5711 - accuracy: 0.4313 - val_loss: 1.5918 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5618 - accuracy: 0.4313 - val_loss: 1.5913 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 199us/sample - loss: 1.5627 - accuracy: 0.4313 - val_loss: 1.5899 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 234us/sample - loss: 1.5671 - accuracy: 0.4313 - val_loss: 1.5886 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5526 - accuracy: 0.4313 - val_loss: 1.5893 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5661 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 200us/sample - loss: 1.5601 - accuracy: 0.4313 - val_loss: 1.5886 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 199us/sample - loss: 1.5638 - accuracy: 0.4313 - val_loss: 1.5871 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 243us/sample - loss: 1.5530 - accuracy: 0.4313 - val_loss: 1.5840 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5471 - accuracy: 0.4313 - val_loss: 1.5842 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 429us/sample - loss: 1.5537 - accuracy: 0.4313 - val_loss: 1.5770 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 878us/sample - loss: 1.7048 - accuracy: 0.3906 - val_loss: 1.5890 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5718 - accuracy: 0.4313 - val_loss: 1.6061 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5704 - accuracy: 0.4313 - val_loss: 1.5954 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 104us/sample - loss: 1.5667 - accuracy: 0.4313 - val_loss: 1.5978 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5735 - accuracy: 0.4313 - val_loss: 1.5986 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5601 - accuracy: 0.4313 - val_loss: 1.5971 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5716 - accuracy: 0.4313 - val_loss: 1.5927 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5759 - accuracy: 0.4313 - val_loss: 1.5923 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5694 - accuracy: 0.4313 - val_loss: 1.5941 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5771 - accuracy: 0.4313 - val_loss: 1.5940 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5684 - accuracy: 0.4313 - val_loss: 1.5966 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5711 - accuracy: 0.4313 - val_loss: 1.5979 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5707 - accuracy: 0.4313 - val_loss: 1.5941 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5622 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5678 - accuracy: 0.4313 - val_loss: 1.5940 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5694 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5698 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5595 - accuracy: 0.4313 - val_loss: 1.5916 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5696 - accuracy: 0.4313 - val_loss: 1.5963 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5621 - accuracy: 0.4313 - val_loss: 1.5917 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5649 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5632 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5665 - accuracy: 0.4313 - val_loss: 1.5931 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5727 - accuracy: 0.4313 - val_loss: 1.5892 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5690 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5652 - accuracy: 0.4313 - val_loss: 1.5900 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5559 - accuracy: 0.4313 - val_loss: 1.5916 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5687 - accuracy: 0.4313 - val_loss: 1.5865 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5569 - accuracy: 0.4313 - val_loss: 1.5880 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5558 - accuracy: 0.4313 - val_loss: 1.5837 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 896us/sample - loss: 1.6841 - accuracy: 0.4016 - val_loss: 1.5992 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5763 - accuracy: 0.4313 - val_loss: 1.6258 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 168us/sample - loss: 1.5848 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 103us/sample - loss: 1.5763 - accuracy: 0.4313 - val_loss: 1.6087 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5726 - accuracy: 0.4313 - val_loss: 1.5971 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5698 - accuracy: 0.4313 - val_loss: 1.5991 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 152us/sample - loss: 1.5598 - accuracy: 0.4313 - val_loss: 1.5940 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 150us/sample - loss: 1.5740 - accuracy: 0.4313 - val_loss: 1.5917 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5684 - accuracy: 0.4313 - val_loss: 1.6040 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5877 - accuracy: 0.4313 - val_loss: 1.5967 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5738 - accuracy: 0.4313 - val_loss: 1.5959 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5748 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5816 - accuracy: 0.4313 - val_loss: 1.5986 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5732 - accuracy: 0.4313 - val_loss: 1.5969 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5672 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5686 - accuracy: 0.4313 - val_loss: 1.5949 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 145us/sample - loss: 1.5698 - accuracy: 0.4313 - val_loss: 1.5890 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5734 - accuracy: 0.4313 - val_loss: 1.5949 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5692 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5695 - accuracy: 0.4313 - val_loss: 1.5980 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5755 - accuracy: 0.4313 - val_loss: 1.5947 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5701 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5720 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5675 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5698 - accuracy: 0.4313 - val_loss: 1.5917 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5688 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5699 - accuracy: 0.4313 - val_loss: 1.5904 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5650 - accuracy: 0.4313 - val_loss: 1.5937 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5640 - accuracy: 0.4313 - val_loss: 1.5910 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5615 - accuracy: 0.4313 - val_loss: 1.5897 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 953us/sample - loss: 1.6960 - accuracy: 0.4031 - val_loss: 1.5889 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5793 - accuracy: 0.4313 - val_loss: 1.6077 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5796 - accuracy: 0.4313 - val_loss: 1.6036 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5715 - accuracy: 0.4313 - val_loss: 1.5932 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5679 - accuracy: 0.4313 - val_loss: 1.6001 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 96us/sample - loss: 1.5756 - accuracy: 0.4313 - val_loss: 1.5965 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5701 - accuracy: 0.4313 - val_loss: 1.6020 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5693 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5727 - accuracy: 0.4313 - val_loss: 1.5987 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5752 - accuracy: 0.4313 - val_loss: 1.5976 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5768 - accuracy: 0.4313 - val_loss: 1.5964 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5700 - accuracy: 0.4313 - val_loss: 1.6005 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5685 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5814 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5753 - accuracy: 0.4313 - val_loss: 1.5917 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5701 - accuracy: 0.4313 - val_loss: 1.5981 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5664 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5634 - accuracy: 0.4313 - val_loss: 1.5951 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5646 - accuracy: 0.4313 - val_loss: 1.5932 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5709 - accuracy: 0.4313 - val_loss: 1.5922 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5746 - accuracy: 0.4313 - val_loss: 1.5951 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5693 - accuracy: 0.4313 - val_loss: 1.5911 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5613 - accuracy: 0.4313 - val_loss: 1.5897 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5593 - accuracy: 0.4313 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5776 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5690 - accuracy: 0.4313 - val_loss: 1.5912 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 203us/sample - loss: 1.5586 - accuracy: 0.4313 - val_loss: 1.5868 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5550 - accuracy: 0.4313 - val_loss: 1.5872 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5569 - accuracy: 0.4313 - val_loss: 1.5980 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 298us/sample - loss: 1.5611 - accuracy: 0.4313 - val_loss: 1.5802 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 903us/sample - loss: 1.7148 - accuracy: 0.3969 - val_loss: 1.5910 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5914 - accuracy: 0.4313 - val_loss: 1.6117 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 166us/sample - loss: 1.5753 - accuracy: 0.4313 - val_loss: 1.5903 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 103us/sample - loss: 1.5801 - accuracy: 0.4313 - val_loss: 1.6036 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 102us/sample - loss: 1.5747 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5719 - accuracy: 0.4313 - val_loss: 1.6122 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5841 - accuracy: 0.4313 - val_loss: 1.5903 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5803 - accuracy: 0.4313 - val_loss: 1.5973 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5813 - accuracy: 0.4313 - val_loss: 1.6017 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5724 - accuracy: 0.4313 - val_loss: 1.5981 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5660 - accuracy: 0.4313 - val_loss: 1.6014 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5576 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5659 - accuracy: 0.4313 - val_loss: 1.5960 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5642 - accuracy: 0.4313 - val_loss: 1.5917 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5718 - accuracy: 0.4313 - val_loss: 1.5957 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5704 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5867 - accuracy: 0.4313 - val_loss: 1.5997 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5786 - accuracy: 0.4313 - val_loss: 1.5930 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5726 - accuracy: 0.4313 - val_loss: 1.6001 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5712 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5671 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5761 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 145us/sample - loss: 1.5715 - accuracy: 0.4313 - val_loss: 1.5903 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5633 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5701 - accuracy: 0.4313 - val_loss: 1.5901 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5699 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5658 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5559 - accuracy: 0.4313 - val_loss: 1.5908 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5670 - accuracy: 0.4313 - val_loss: 1.5931 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 140us/sample - loss: 1.5606 - accuracy: 0.4313 - val_loss: 1.5888 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 899us/sample - loss: 1.6897 - accuracy: 0.4047 - val_loss: 1.5986 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5977 - accuracy: 0.4313 - val_loss: 1.6531 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 212us/sample - loss: 1.5891 - accuracy: 0.4297 - val_loss: 1.5984 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5943 - accuracy: 0.4313 - val_loss: 1.6098 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 221us/sample - loss: 1.5769 - accuracy: 0.4313 - val_loss: 1.5864 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5717 - accuracy: 0.4313 - val_loss: 1.5999 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 104us/sample - loss: 1.5740 - accuracy: 0.4313 - val_loss: 1.5937 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 103us/sample - loss: 1.5633 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 102us/sample - loss: 1.5678 - accuracy: 0.4313 - val_loss: 1.5986 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5690 - accuracy: 0.4313 - val_loss: 1.5922 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5697 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5628 - accuracy: 0.4313 - val_loss: 1.5921 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5793 - accuracy: 0.4313 - val_loss: 1.6060 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5741 - accuracy: 0.4313 - val_loss: 1.5932 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5664 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5666 - accuracy: 0.4313 - val_loss: 1.5922 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5710 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5706 - accuracy: 0.4313 - val_loss: 1.5969 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5702 - accuracy: 0.4313 - val_loss: 1.5898 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5641 - accuracy: 0.4313 - val_loss: 1.5889 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5632 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5624 - accuracy: 0.4313 - val_loss: 1.5894 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5678 - accuracy: 0.4313 - val_loss: 1.5868 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5624 - accuracy: 0.4313 - val_loss: 1.5893 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5505 - accuracy: 0.4313 - val_loss: 1.5865 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 204us/sample - loss: 1.5539 - accuracy: 0.4313 - val_loss: 1.5856 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 372us/sample - loss: 1.5514 - accuracy: 0.4313 - val_loss: 1.5798 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 378us/sample - loss: 1.5378 - accuracy: 0.4313 - val_loss: 1.5777 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 720us/sample - loss: 1.5321 - accuracy: 0.4313 - val_loss: 1.5683 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 1s 823us/sample - loss: 1.5169 - accuracy: 0.4313 - val_loss: 1.5621 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 886us/sample - loss: 1.7181 - accuracy: 0.3984 - val_loss: 1.6025 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5875 - accuracy: 0.4313 - val_loss: 1.6094 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 202us/sample - loss: 1.5688 - accuracy: 0.4313 - val_loss: 1.5952 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5733 - accuracy: 0.4313 - val_loss: 1.5974 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5794 - accuracy: 0.4313 - val_loss: 1.6030 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 161us/sample - loss: 1.5669 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 97us/sample - loss: 1.5800 - accuracy: 0.4313 - val_loss: 1.5950 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5691 - accuracy: 0.4313 - val_loss: 1.5909 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 97us/sample - loss: 1.5739 - accuracy: 0.4313 - val_loss: 1.6007 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5613 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5652 - accuracy: 0.4313 - val_loss: 1.5977 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5692 - accuracy: 0.4313 - val_loss: 1.5973 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 150us/sample - loss: 1.5681 - accuracy: 0.4313 - val_loss: 1.5897 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5678 - accuracy: 0.4313 - val_loss: 1.5982 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5699 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5719 - accuracy: 0.4313 - val_loss: 1.5962 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5661 - accuracy: 0.4313 - val_loss: 1.5912 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5729 - accuracy: 0.4313 - val_loss: 1.5958 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5638 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5705 - accuracy: 0.4313 - val_loss: 1.5957 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5622 - accuracy: 0.4313 - val_loss: 1.5927 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5668 - accuracy: 0.4313 - val_loss: 1.5904 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5768 - accuracy: 0.4313 - val_loss: 1.5979 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5613 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5732 - accuracy: 0.4313 - val_loss: 1.5941 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 92us/sample - loss: 1.5610 - accuracy: 0.4313 - val_loss: 1.5899 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5635 - accuracy: 0.4313 - val_loss: 1.5958 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5669 - accuracy: 0.4313 - val_loss: 1.5901 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5603 - accuracy: 0.4313 - val_loss: 1.5909 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5655 - accuracy: 0.4313 - val_loss: 1.5902 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 899us/sample - loss: 1.7115 - accuracy: 0.3844 - val_loss: 1.5904 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5999 - accuracy: 0.4313 - val_loss: 1.6207 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5822 - accuracy: 0.4313 - val_loss: 1.6037 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 102us/sample - loss: 1.5752 - accuracy: 0.4313 - val_loss: 1.5960 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5624 - accuracy: 0.4313 - val_loss: 1.6017 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5660 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5711 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5725 - accuracy: 0.4313 - val_loss: 1.5992 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5758 - accuracy: 0.4313 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5686 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5612 - accuracy: 0.4313 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5696 - accuracy: 0.4313 - val_loss: 1.5981 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5728 - accuracy: 0.4313 - val_loss: 1.6010 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5696 - accuracy: 0.4313 - val_loss: 1.5937 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5623 - accuracy: 0.4313 - val_loss: 1.5953 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5664 - accuracy: 0.4313 - val_loss: 1.5953 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5736 - accuracy: 0.4313 - val_loss: 1.5945 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5669 - accuracy: 0.4313 - val_loss: 1.5941 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5713 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5589 - accuracy: 0.4313 - val_loss: 1.5982 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5778 - accuracy: 0.4313 - val_loss: 1.5957 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5708 - accuracy: 0.4313 - val_loss: 1.5959 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5681 - accuracy: 0.4313 - val_loss: 1.5921 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5670 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5655 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5676 - accuracy: 0.4313 - val_loss: 1.5946 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5684 - accuracy: 0.4313 - val_loss: 1.5930 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5617 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5650 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5611 - accuracy: 0.4313 - val_loss: 1.5912 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 905us/sample - loss: 1.7123 - accuracy: 0.3875 - val_loss: 1.5995 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 111us/sample - loss: 1.5958 - accuracy: 0.4313 - val_loss: 1.6230 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 167us/sample - loss: 1.5809 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5764 - accuracy: 0.4313 - val_loss: 1.6018 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 98us/sample - loss: 1.5676 - accuracy: 0.4313 - val_loss: 1.5968 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 96us/sample - loss: 1.5713 - accuracy: 0.4313 - val_loss: 1.5995 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 98us/sample - loss: 1.5766 - accuracy: 0.4313 - val_loss: 1.5951 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5678 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5747 - accuracy: 0.4313 - val_loss: 1.5979 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 148us/sample - loss: 1.5676 - accuracy: 0.4313 - val_loss: 1.5921 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5715 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5740 - accuracy: 0.4313 - val_loss: 1.5987 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5677 - accuracy: 0.4313 - val_loss: 1.5952 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5731 - accuracy: 0.4313 - val_loss: 1.5984 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 189us/sample - loss: 1.5687 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5756 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5645 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5633 - accuracy: 0.4313 - val_loss: 1.5976 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5674 - accuracy: 0.4313 - val_loss: 1.5936 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5621 - accuracy: 0.4313 - val_loss: 1.5953 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5687 - accuracy: 0.4313 - val_loss: 1.5963 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5656 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5660 - accuracy: 0.4313 - val_loss: 1.5911 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 199us/sample - loss: 1.5626 - accuracy: 0.4313 - val_loss: 1.5898 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5629 - accuracy: 0.4313 - val_loss: 1.5912 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5584 - accuracy: 0.4313 - val_loss: 1.5909 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 254us/sample - loss: 1.5661 - accuracy: 0.4313 - val_loss: 1.5895 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 299us/sample - loss: 1.5602 - accuracy: 0.4313 - val_loss: 1.5890 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 293us/sample - loss: 1.5594 - accuracy: 0.4313 - val_loss: 1.5849 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 257us/sample - loss: 1.5651 - accuracy: 0.4313 - val_loss: 1.5849 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 902us/sample - loss: 1.6880 - accuracy: 0.3906 - val_loss: 1.5964 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 110us/sample - loss: 1.5998 - accuracy: 0.4313 - val_loss: 1.6219 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 171us/sample - loss: 1.5759 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 109us/sample - loss: 1.5691 - accuracy: 0.4313 - val_loss: 1.6032 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 161us/sample - loss: 1.5653 - accuracy: 0.4313 - val_loss: 1.5901 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 104us/sample - loss: 1.5885 - accuracy: 0.4313 - val_loss: 1.6071 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 99us/sample - loss: 1.5839 - accuracy: 0.4313 - val_loss: 1.5983 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5700 - accuracy: 0.4313 - val_loss: 1.6038 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5641 - accuracy: 0.4313 - val_loss: 1.5946 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5663 - accuracy: 0.4313 - val_loss: 1.5967 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5665 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5673 - accuracy: 0.4313 - val_loss: 1.5930 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5697 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5667 - accuracy: 0.4313 - val_loss: 1.5932 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5725 - accuracy: 0.4313 - val_loss: 1.5990 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5669 - accuracy: 0.4313 - val_loss: 1.5921 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5764 - accuracy: 0.4313 - val_loss: 1.5989 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5723 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5670 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5697 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 144us/sample - loss: 1.5754 - accuracy: 0.4313 - val_loss: 1.5901 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5720 - accuracy: 0.4313 - val_loss: 1.5941 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5660 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5601 - accuracy: 0.4313 - val_loss: 1.5911 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5591 - accuracy: 0.4313 - val_loss: 1.5886 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5649 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5669 - accuracy: 0.4313 - val_loss: 1.5936 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5665 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5691 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5590 - accuracy: 0.4313 - val_loss: 1.5885 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 903us/sample - loss: 1.7024 - accuracy: 0.3906 - val_loss: 1.5984 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 109us/sample - loss: 1.5740 - accuracy: 0.4313 - val_loss: 1.6098 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 167us/sample - loss: 1.5765 - accuracy: 0.4313 - val_loss: 1.5950 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5685 - accuracy: 0.4313 - val_loss: 1.5966 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 161us/sample - loss: 1.5670 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 102us/sample - loss: 1.5784 - accuracy: 0.4313 - val_loss: 1.6020 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 97us/sample - loss: 1.5834 - accuracy: 0.4313 - val_loss: 1.6027 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 97us/sample - loss: 1.5674 - accuracy: 0.4313 - val_loss: 1.5914 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 97us/sample - loss: 1.5615 - accuracy: 0.4313 - val_loss: 1.5934 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5803 - accuracy: 0.4313 - val_loss: 1.6005 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5651 - accuracy: 0.4313 - val_loss: 1.5927 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5740 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5755 - accuracy: 0.4313 - val_loss: 1.5971 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5686 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5697 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5751 - accuracy: 0.4313 - val_loss: 1.5990 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5680 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5686 - accuracy: 0.4313 - val_loss: 1.5936 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5624 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 143us/sample - loss: 1.5582 - accuracy: 0.4313 - val_loss: 1.5899 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5617 - accuracy: 0.4313 - val_loss: 1.5959 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5675 - accuracy: 0.4313 - val_loss: 1.5901 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5661 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 200us/sample - loss: 1.5567 - accuracy: 0.4313 - val_loss: 1.5882 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5553 - accuracy: 0.4313 - val_loss: 1.5888 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5550 - accuracy: 0.4313 - val_loss: 1.5900 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 376us/sample - loss: 1.5574 - accuracy: 0.4313 - val_loss: 1.5840 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 374us/sample - loss: 1.5482 - accuracy: 0.4313 - val_loss: 1.5831 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 373us/sample - loss: 1.5565 - accuracy: 0.4313 - val_loss: 1.5788 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 375us/sample - loss: 1.5440 - accuracy: 0.4313 - val_loss: 1.5764 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.6979 - accuracy: 0.3922 - val_loss: 1.5888 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 109us/sample - loss: 1.5745 - accuracy: 0.4313 - val_loss: 1.6115 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5853 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 102us/sample - loss: 1.5762 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 97us/sample - loss: 1.5771 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 97us/sample - loss: 1.5715 - accuracy: 0.4313 - val_loss: 1.5986 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5668 - accuracy: 0.4313 - val_loss: 1.5966 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5639 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5764 - accuracy: 0.4313 - val_loss: 1.6012 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5668 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5741 - accuracy: 0.4313 - val_loss: 1.5965 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5722 - accuracy: 0.4313 - val_loss: 1.5961 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5612 - accuracy: 0.4313 - val_loss: 1.5945 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5603 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5699 - accuracy: 0.4313 - val_loss: 1.6037 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5694 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5801 - accuracy: 0.4313 - val_loss: 1.5986 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5725 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5689 - accuracy: 0.4313 - val_loss: 1.5945 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5637 - accuracy: 0.4313 - val_loss: 1.5910 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5722 - accuracy: 0.4313 - val_loss: 1.5950 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5602 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5632 - accuracy: 0.4313 - val_loss: 1.5945 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5683 - accuracy: 0.4313 - val_loss: 1.5932 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5661 - accuracy: 0.4313 - val_loss: 1.5922 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5632 - accuracy: 0.4313 - val_loss: 1.5921 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5679 - accuracy: 0.4313 - val_loss: 1.5964 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5756 - accuracy: 0.4313 - val_loss: 1.5965 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5702 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5665 - accuracy: 0.4313 - val_loss: 1.5905 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 892us/sample - loss: 1.6851 - accuracy: 0.4062 - val_loss: 1.6027 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 110us/sample - loss: 1.6012 - accuracy: 0.4313 - val_loss: 1.6153 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 167us/sample - loss: 1.5773 - accuracy: 0.4313 - val_loss: 1.5934 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5651 - accuracy: 0.4313 - val_loss: 1.6032 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 104us/sample - loss: 1.5739 - accuracy: 0.4313 - val_loss: 1.5947 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5769 - accuracy: 0.4313 - val_loss: 1.5959 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 99us/sample - loss: 1.5722 - accuracy: 0.4313 - val_loss: 1.6000 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 157us/sample - loss: 1.5658 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5816 - accuracy: 0.4313 - val_loss: 1.6033 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 93us/sample - loss: 1.5670 - accuracy: 0.4313 - val_loss: 1.5996 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5740 - accuracy: 0.4313 - val_loss: 1.5930 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5689 - accuracy: 0.4313 - val_loss: 1.5979 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 146us/sample - loss: 1.5733 - accuracy: 0.4313 - val_loss: 1.5908 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5796 - accuracy: 0.4313 - val_loss: 1.6113 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5799 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5766 - accuracy: 0.4313 - val_loss: 1.5932 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5677 - accuracy: 0.4313 - val_loss: 1.5947 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5684 - accuracy: 0.4313 - val_loss: 1.5954 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5687 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5671 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5678 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 145us/sample - loss: 1.5625 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5636 - accuracy: 0.4313 - val_loss: 1.5934 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5714 - accuracy: 0.4313 - val_loss: 1.5890 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5624 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5653 - accuracy: 0.4313 - val_loss: 1.5891 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 232us/sample - loss: 1.5646 - accuracy: 0.4313 - val_loss: 1.5887 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5568 - accuracy: 0.4313 - val_loss: 1.5889 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 196us/sample - loss: 1.5493 - accuracy: 0.4313 - val_loss: 1.5867 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5578 - accuracy: 0.4313 - val_loss: 1.5917 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 898us/sample - loss: 1.7149 - accuracy: 0.3984 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 109us/sample - loss: 1.5848 - accuracy: 0.4313 - val_loss: 1.6107 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5757 - accuracy: 0.4313 - val_loss: 1.5972 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5747 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5637 - accuracy: 0.4313 - val_loss: 1.5985 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 102us/sample - loss: 1.5715 - accuracy: 0.4313 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 93us/sample - loss: 1.5661 - accuracy: 0.4313 - val_loss: 1.5981 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 93us/sample - loss: 1.5640 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 93us/sample - loss: 1.5759 - accuracy: 0.4313 - val_loss: 1.5964 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5685 - accuracy: 0.4313 - val_loss: 1.6006 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5646 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5702 - accuracy: 0.4313 - val_loss: 1.5971 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5667 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5648 - accuracy: 0.4313 - val_loss: 1.5986 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5726 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 144us/sample - loss: 1.5670 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5695 - accuracy: 0.4313 - val_loss: 1.5996 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5699 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5688 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5851 - accuracy: 0.4313 - val_loss: 1.5936 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5596 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5655 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5672 - accuracy: 0.4313 - val_loss: 1.5952 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5717 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5666 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5698 - accuracy: 0.4313 - val_loss: 1.5920 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5647 - accuracy: 0.4313 - val_loss: 1.5931 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5624 - accuracy: 0.4313 - val_loss: 1.5934 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5641 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5674 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.7089 - accuracy: 0.3953 - val_loss: 1.5871 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 110us/sample - loss: 1.5788 - accuracy: 0.4313 - val_loss: 1.6118 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5754 - accuracy: 0.4313 - val_loss: 1.5934 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5750 - accuracy: 0.4313 - val_loss: 1.6015 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5734 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5683 - accuracy: 0.4313 - val_loss: 1.5965 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5679 - accuracy: 0.4313 - val_loss: 1.5982 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5707 - accuracy: 0.4313 - val_loss: 1.5899 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 92us/sample - loss: 1.5693 - accuracy: 0.4313 - val_loss: 1.5940 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5724 - accuracy: 0.4313 - val_loss: 1.6030 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5784 - accuracy: 0.4313 - val_loss: 1.5946 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5706 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5699 - accuracy: 0.4313 - val_loss: 1.6000 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5729 - accuracy: 0.4313 - val_loss: 1.5962 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5599 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5736 - accuracy: 0.4313 - val_loss: 1.6031 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5742 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5690 - accuracy: 0.4313 - val_loss: 1.5914 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5667 - accuracy: 0.4313 - val_loss: 1.5917 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5547 - accuracy: 0.4313 - val_loss: 1.5897 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5679 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5679 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5733 - accuracy: 0.4313 - val_loss: 1.5904 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5629 - accuracy: 0.4313 - val_loss: 1.5930 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5678 - accuracy: 0.4313 - val_loss: 1.5909 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5636 - accuracy: 0.4313 - val_loss: 1.5891 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5583 - accuracy: 0.4313 - val_loss: 1.5904 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5646 - accuracy: 0.4313 - val_loss: 1.5872 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5613 - accuracy: 0.4313 - val_loss: 1.5876 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5676 - accuracy: 0.4313 - val_loss: 1.5963 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.6846 - accuracy: 0.4016 - val_loss: 1.5977 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5996 - accuracy: 0.4313 - val_loss: 1.6251 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 166us/sample - loss: 1.5774 - accuracy: 0.4313 - val_loss: 1.5965 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5723 - accuracy: 0.4313 - val_loss: 1.6002 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 164us/sample - loss: 1.5782 - accuracy: 0.4313 - val_loss: 1.5953 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 197us/sample - loss: 1.5655 - accuracy: 0.4313 - val_loss: 1.5911 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5698 - accuracy: 0.4313 - val_loss: 1.5979 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 98us/sample - loss: 1.5662 - accuracy: 0.4313 - val_loss: 1.5996 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5764 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5622 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5763 - accuracy: 0.4313 - val_loss: 1.6032 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5745 - accuracy: 0.4313 - val_loss: 1.5941 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5683 - accuracy: 0.4313 - val_loss: 1.5949 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5653 - accuracy: 0.4313 - val_loss: 1.5946 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5777 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5596 - accuracy: 0.4313 - val_loss: 1.5967 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5707 - accuracy: 0.4313 - val_loss: 1.5965 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 145us/sample - loss: 1.5705 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5743 - accuracy: 0.4313 - val_loss: 1.5952 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5693 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5732 - accuracy: 0.4313 - val_loss: 1.5963 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5737 - accuracy: 0.4313 - val_loss: 1.5997 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5726 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5663 - accuracy: 0.4313 - val_loss: 1.5920 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5730 - accuracy: 0.4313 - val_loss: 1.6008 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 140us/sample - loss: 1.5708 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5709 - accuracy: 0.4313 - val_loss: 1.5914 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5744 - accuracy: 0.4313 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5628 - accuracy: 0.4313 - val_loss: 1.5932 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5651 - accuracy: 0.4313 - val_loss: 1.5962 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.7073 - accuracy: 0.3984 - val_loss: 1.5936 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 109us/sample - loss: 1.5726 - accuracy: 0.4313 - val_loss: 1.5977 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5649 - accuracy: 0.4313 - val_loss: 1.5993 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5673 - accuracy: 0.4313 - val_loss: 1.5951 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 96us/sample - loss: 1.5779 - accuracy: 0.4313 - val_loss: 1.6103 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 155us/sample - loss: 1.5818 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5813 - accuracy: 0.4313 - val_loss: 1.5982 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 149us/sample - loss: 1.5648 - accuracy: 0.4313 - val_loss: 1.5916 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5677 - accuracy: 0.4313 - val_loss: 1.6035 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5664 - accuracy: 0.4313 - val_loss: 1.5970 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5686 - accuracy: 0.4313 - val_loss: 1.5979 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5683 - accuracy: 0.4313 - val_loss: 1.6028 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5583 - accuracy: 0.4313 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5677 - accuracy: 0.4313 - val_loss: 1.6010 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5723 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5760 - accuracy: 0.4313 - val_loss: 1.5940 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5632 - accuracy: 0.4313 - val_loss: 1.5985 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5717 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 145us/sample - loss: 1.5628 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5680 - accuracy: 0.4313 - val_loss: 1.5913 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5654 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5645 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 201us/sample - loss: 1.5610 - accuracy: 0.4313 - val_loss: 1.5878 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 422us/sample - loss: 1.5617 - accuracy: 0.4313 - val_loss: 1.5851 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5711 - accuracy: 0.4313 - val_loss: 1.5950 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5636 - accuracy: 0.4313 - val_loss: 1.5859 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 456us/sample - loss: 1.5650 - accuracy: 0.4313 - val_loss: 1.5837 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5479 - accuracy: 0.4313 - val_loss: 1.6065 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 421us/sample - loss: 1.5460 - accuracy: 0.4313 - val_loss: 1.5775 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 709us/sample - loss: 1.5376 - accuracy: 0.4313 - val_loss: 1.5715 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 891us/sample - loss: 1.6929 - accuracy: 0.4094 - val_loss: 1.6044 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5670 - accuracy: 0.4313 - val_loss: 1.6085 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 168us/sample - loss: 1.5790 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5734 - accuracy: 0.4313 - val_loss: 1.6110 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5686 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 99us/sample - loss: 1.5709 - accuracy: 0.4313 - val_loss: 1.6083 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5846 - accuracy: 0.4313 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5783 - accuracy: 0.4313 - val_loss: 1.5967 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 150us/sample - loss: 1.5687 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 150us/sample - loss: 1.5673 - accuracy: 0.4313 - val_loss: 1.5908 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5782 - accuracy: 0.4313 - val_loss: 1.5971 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5692 - accuracy: 0.4313 - val_loss: 1.5968 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5672 - accuracy: 0.4313 - val_loss: 1.5937 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5687 - accuracy: 0.4313 - val_loss: 1.6015 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5676 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5706 - accuracy: 0.4313 - val_loss: 1.5950 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5676 - accuracy: 0.4313 - val_loss: 1.5920 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5595 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5698 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5696 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5725 - accuracy: 0.4313 - val_loss: 1.5917 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 143us/sample - loss: 1.5593 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 143us/sample - loss: 1.5647 - accuracy: 0.4313 - val_loss: 1.5896 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5745 - accuracy: 0.4313 - val_loss: 1.5947 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5654 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5692 - accuracy: 0.4313 - val_loss: 1.5916 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5676 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5661 - accuracy: 0.4313 - val_loss: 1.5905 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5616 - accuracy: 0.4313 - val_loss: 1.5911 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 143us/sample - loss: 1.5655 - accuracy: 0.4313 - val_loss: 1.5884 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.7074 - accuracy: 0.4031 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 110us/sample - loss: 1.5809 - accuracy: 0.4313 - val_loss: 1.6311 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5751 - accuracy: 0.4297 - val_loss: 1.5953 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5707 - accuracy: 0.4313 - val_loss: 1.5971 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 168us/sample - loss: 1.5756 - accuracy: 0.4313 - val_loss: 1.5896 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5765 - accuracy: 0.4313 - val_loss: 1.6059 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 96us/sample - loss: 1.5749 - accuracy: 0.4313 - val_loss: 1.5995 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5651 - accuracy: 0.4313 - val_loss: 1.5985 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5654 - accuracy: 0.4313 - val_loss: 1.5931 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5703 - accuracy: 0.4313 - val_loss: 1.5962 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 92us/sample - loss: 1.5677 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5693 - accuracy: 0.4313 - val_loss: 1.5968 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5755 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5708 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5671 - accuracy: 0.4313 - val_loss: 1.5999 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5760 - accuracy: 0.4313 - val_loss: 1.5957 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5678 - accuracy: 0.4313 - val_loss: 1.6021 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5752 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5664 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5686 - accuracy: 0.4313 - val_loss: 1.5981 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5633 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5724 - accuracy: 0.4313 - val_loss: 1.5991 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5622 - accuracy: 0.4313 - val_loss: 1.5940 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5777 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5655 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5580 - accuracy: 0.4313 - val_loss: 1.5902 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5716 - accuracy: 0.4313 - val_loss: 1.6014 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5645 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5698 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5520 - accuracy: 0.4313 - val_loss: 1.5918 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 888us/sample - loss: 1.6852 - accuracy: 0.4156 - val_loss: 1.5980 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5738 - accuracy: 0.4313 - val_loss: 1.6158 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 168us/sample - loss: 1.5743 - accuracy: 0.4313 - val_loss: 1.5979 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 109us/sample - loss: 1.5606 - accuracy: 0.4313 - val_loss: 1.6006 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 277us/sample - loss: 1.5743 - accuracy: 0.4313 - val_loss: 1.5877 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5746 - accuracy: 0.4313 - val_loss: 1.6025 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 102us/sample - loss: 1.5721 - accuracy: 0.4313 - val_loss: 1.5930 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5638 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 99us/sample - loss: 1.5740 - accuracy: 0.4313 - val_loss: 1.6037 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5639 - accuracy: 0.4313 - val_loss: 1.5936 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5721 - accuracy: 0.4313 - val_loss: 1.5978 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 92us/sample - loss: 1.5634 - accuracy: 0.4313 - val_loss: 1.5885 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5695 - accuracy: 0.4313 - val_loss: 1.5897 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5713 - accuracy: 0.4313 - val_loss: 1.5974 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5734 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5706 - accuracy: 0.4313 - val_loss: 1.6002 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5713 - accuracy: 0.4313 - val_loss: 1.5885 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5815 - accuracy: 0.4313 - val_loss: 1.6049 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 308us/sample - loss: 1.5619 - accuracy: 0.4313 - val_loss: 1.5845 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5640 - accuracy: 0.4313 - val_loss: 1.5874 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 253us/sample - loss: 1.5598 - accuracy: 0.4313 - val_loss: 1.5832 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 260us/sample - loss: 1.5568 - accuracy: 0.4313 - val_loss: 1.5772 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 1s 885us/sample - loss: 1.5384 - accuracy: 0.4313 - val_loss: 1.5703 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 1s 911us/sample - loss: 1.5509 - accuracy: 0.4328 - val_loss: 1.5660 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5400 - accuracy: 0.4313 - val_loss: 1.5805 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5282 - accuracy: 0.4328 - val_loss: 1.5706 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5233 - accuracy: 0.4344 - val_loss: 1.5694 - val_accuracy: 0.4375\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 1s 878us/sample - loss: 1.5059 - accuracy: 0.4391 - val_loss: 1.5640 - val_accuracy: 0.4250\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 1.4914 - accuracy: 0.4563 - val_loss: 1.5567 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 98us/sample - loss: 1.5010 - accuracy: 0.4437 - val_loss: 1.5601 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.7074 - accuracy: 0.3922 - val_loss: 1.5902 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 109us/sample - loss: 1.5784 - accuracy: 0.4313 - val_loss: 1.6108 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5754 - accuracy: 0.4313 - val_loss: 1.5968 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5742 - accuracy: 0.4313 - val_loss: 1.5911 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5738 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 93us/sample - loss: 1.5654 - accuracy: 0.4313 - val_loss: 1.5946 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 96us/sample - loss: 1.5772 - accuracy: 0.4297 - val_loss: 1.5954 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 96us/sample - loss: 1.5588 - accuracy: 0.4313 - val_loss: 1.5983 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5697 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5742 - accuracy: 0.4313 - val_loss: 1.5923 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5765 - accuracy: 0.4313 - val_loss: 1.6012 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5763 - accuracy: 0.4313 - val_loss: 1.5908 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5673 - accuracy: 0.4313 - val_loss: 1.5994 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5741 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5663 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5635 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5604 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5715 - accuracy: 0.4313 - val_loss: 1.5902 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5663 - accuracy: 0.4313 - val_loss: 1.5903 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 145us/sample - loss: 1.5782 - accuracy: 0.4313 - val_loss: 1.5884 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5500 - accuracy: 0.4313 - val_loss: 1.5909 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5606 - accuracy: 0.4313 - val_loss: 1.5901 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5554 - accuracy: 0.4313 - val_loss: 1.5904 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5561 - accuracy: 0.4313 - val_loss: 1.5849 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 144us/sample - loss: 1.5579 - accuracy: 0.4313 - val_loss: 1.5838 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5524 - accuracy: 0.4313 - val_loss: 1.5877 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5478 - accuracy: 0.4313 - val_loss: 1.5757 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5436 - accuracy: 0.4313 - val_loss: 1.5790 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5372 - accuracy: 0.4313 - val_loss: 1.5774 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5363 - accuracy: 0.4406 - val_loss: 1.6008 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 875us/sample - loss: 1.6816 - accuracy: 0.4109 - val_loss: 1.6102 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 108us/sample - loss: 1.5830 - accuracy: 0.4313 - val_loss: 1.6194 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 166us/sample - loss: 1.5774 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5718 - accuracy: 0.4313 - val_loss: 1.5979 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5726 - accuracy: 0.4313 - val_loss: 1.5937 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 97us/sample - loss: 1.5717 - accuracy: 0.4313 - val_loss: 1.6025 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 96us/sample - loss: 1.5637 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5659 - accuracy: 0.4313 - val_loss: 1.5978 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5677 - accuracy: 0.4313 - val_loss: 1.5969 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5762 - accuracy: 0.4313 - val_loss: 1.6030 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5769 - accuracy: 0.4313 - val_loss: 1.5940 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5699 - accuracy: 0.4313 - val_loss: 1.5965 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5715 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5683 - accuracy: 0.4313 - val_loss: 1.5945 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5661 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5728 - accuracy: 0.4313 - val_loss: 1.5936 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5606 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5668 - accuracy: 0.4313 - val_loss: 1.5984 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5751 - accuracy: 0.4313 - val_loss: 1.5927 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5672 - accuracy: 0.4313 - val_loss: 1.5972 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5767 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5646 - accuracy: 0.4313 - val_loss: 1.5937 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5681 - accuracy: 0.4313 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5653 - accuracy: 0.4313 - val_loss: 1.5931 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5752 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 139us/sample - loss: 1.5648 - accuracy: 0.4313 - val_loss: 1.5914 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5744 - accuracy: 0.4313 - val_loss: 1.5977 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5630 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5734 - accuracy: 0.4313 - val_loss: 1.5979 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5718 - accuracy: 0.4313 - val_loss: 1.5917 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 883us/sample - loss: 1.6809 - accuracy: 0.4047 - val_loss: 1.6175 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.6039 - accuracy: 0.4313 - val_loss: 1.6323 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 163us/sample - loss: 1.5831 - accuracy: 0.4313 - val_loss: 1.5959 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5596 - accuracy: 0.4313 - val_loss: 1.6062 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5695 - accuracy: 0.4313 - val_loss: 1.5966 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 158us/sample - loss: 1.5714 - accuracy: 0.4313 - val_loss: 1.5957 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 148us/sample - loss: 1.5643 - accuracy: 0.4313 - val_loss: 1.5931 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 92us/sample - loss: 1.5724 - accuracy: 0.4313 - val_loss: 1.6015 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5734 - accuracy: 0.4313 - val_loss: 1.5961 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5616 - accuracy: 0.4313 - val_loss: 1.5963 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 149us/sample - loss: 1.5722 - accuracy: 0.4313 - val_loss: 1.5920 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5697 - accuracy: 0.4313 - val_loss: 1.6040 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5755 - accuracy: 0.4313 - val_loss: 1.5969 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5727 - accuracy: 0.4313 - val_loss: 1.5916 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5729 - accuracy: 0.4313 - val_loss: 1.5992 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5720 - accuracy: 0.4313 - val_loss: 1.5987 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5699 - accuracy: 0.4313 - val_loss: 1.5937 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5680 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5611 - accuracy: 0.4313 - val_loss: 1.5919 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 200us/sample - loss: 1.5641 - accuracy: 0.4313 - val_loss: 1.5895 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5690 - accuracy: 0.4313 - val_loss: 1.6000 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5655 - accuracy: 0.4313 - val_loss: 1.5910 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5624 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 196us/sample - loss: 1.5661 - accuracy: 0.4313 - val_loss: 1.5882 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5503 - accuracy: 0.4313 - val_loss: 1.5962 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 195us/sample - loss: 1.5596 - accuracy: 0.4313 - val_loss: 1.5877 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5622 - accuracy: 0.4313 - val_loss: 1.6000 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5693 - accuracy: 0.4313 - val_loss: 1.5886 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 195us/sample - loss: 1.5623 - accuracy: 0.4313 - val_loss: 1.5830 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5479 - accuracy: 0.4313 - val_loss: 1.5870 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.6738 - accuracy: 0.3984 - val_loss: 1.6025 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.6153 - accuracy: 0.4313 - val_loss: 1.6450 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 163us/sample - loss: 1.5950 - accuracy: 0.4313 - val_loss: 1.5995 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 162us/sample - loss: 1.5811 - accuracy: 0.4313 - val_loss: 1.5957 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5699 - accuracy: 0.4313 - val_loss: 1.5988 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 155us/sample - loss: 1.5736 - accuracy: 0.4313 - val_loss: 1.5927 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5664 - accuracy: 0.4313 - val_loss: 1.5937 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 98us/sample - loss: 1.5753 - accuracy: 0.4313 - val_loss: 1.5970 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 97us/sample - loss: 1.5747 - accuracy: 0.4313 - val_loss: 1.5927 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5664 - accuracy: 0.4313 - val_loss: 1.5974 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5663 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5640 - accuracy: 0.4313 - val_loss: 1.6001 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5644 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5758 - accuracy: 0.4313 - val_loss: 1.6008 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5791 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5641 - accuracy: 0.4313 - val_loss: 1.5945 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5687 - accuracy: 0.4313 - val_loss: 1.5934 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5645 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5741 - accuracy: 0.4313 - val_loss: 1.5957 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5771 - accuracy: 0.4313 - val_loss: 1.5937 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5650 - accuracy: 0.4313 - val_loss: 1.5921 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5618 - accuracy: 0.4313 - val_loss: 1.5923 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5683 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5660 - accuracy: 0.4313 - val_loss: 1.5945 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5744 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5665 - accuracy: 0.4313 - val_loss: 1.5955 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5593 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5673 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 140us/sample - loss: 1.5633 - accuracy: 0.4313 - val_loss: 1.5918 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 139us/sample - loss: 1.5617 - accuracy: 0.4313 - val_loss: 1.5912 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 877us/sample - loss: 1.6899 - accuracy: 0.4109 - val_loss: 1.6086 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5803 - accuracy: 0.4313 - val_loss: 1.6221 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 165us/sample - loss: 1.5854 - accuracy: 0.4313 - val_loss: 1.5993 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 164us/sample - loss: 1.5674 - accuracy: 0.4313 - val_loss: 1.5975 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 157us/sample - loss: 1.5694 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5746 - accuracy: 0.4313 - val_loss: 1.6001 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 98us/sample - loss: 1.5662 - accuracy: 0.4313 - val_loss: 1.5990 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 155us/sample - loss: 1.5693 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 155us/sample - loss: 1.5621 - accuracy: 0.4313 - val_loss: 1.5923 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 95us/sample - loss: 1.5692 - accuracy: 0.4313 - val_loss: 1.5982 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 93us/sample - loss: 1.5710 - accuracy: 0.4313 - val_loss: 1.5971 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5677 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5649 - accuracy: 0.4313 - val_loss: 1.5962 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5558 - accuracy: 0.4313 - val_loss: 1.5950 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5674 - accuracy: 0.4313 - val_loss: 1.6010 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 143us/sample - loss: 1.5664 - accuracy: 0.4313 - val_loss: 1.5914 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5726 - accuracy: 0.4313 - val_loss: 1.5930 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5670 - accuracy: 0.4313 - val_loss: 1.5937 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5783 - accuracy: 0.4313 - val_loss: 1.5980 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5769 - accuracy: 0.4313 - val_loss: 1.6014 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5794 - accuracy: 0.4313 - val_loss: 1.6000 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5692 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5621 - accuracy: 0.4313 - val_loss: 1.5934 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 200us/sample - loss: 1.5671 - accuracy: 0.4313 - val_loss: 1.5902 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5624 - accuracy: 0.4313 - val_loss: 1.5903 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5674 - accuracy: 0.4313 - val_loss: 1.5923 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5721 - accuracy: 0.4313 - val_loss: 1.5964 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5575 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5611 - accuracy: 0.4313 - val_loss: 1.5909 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 197us/sample - loss: 1.5653 - accuracy: 0.4313 - val_loss: 1.5895 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.6735 - accuracy: 0.3922 - val_loss: 1.6156 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5923 - accuracy: 0.4313 - val_loss: 1.6191 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 166us/sample - loss: 1.5803 - accuracy: 0.4313 - val_loss: 1.5954 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 103us/sample - loss: 1.5710 - accuracy: 0.4313 - val_loss: 1.6028 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 153us/sample - loss: 1.5616 - accuracy: 0.4313 - val_loss: 1.5953 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 98us/sample - loss: 1.5696 - accuracy: 0.4313 - val_loss: 1.5969 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 93us/sample - loss: 1.5707 - accuracy: 0.4313 - val_loss: 1.5954 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 147us/sample - loss: 1.5643 - accuracy: 0.4313 - val_loss: 1.5946 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 93us/sample - loss: 1.5660 - accuracy: 0.4313 - val_loss: 1.5975 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5680 - accuracy: 0.4313 - val_loss: 1.5966 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 147us/sample - loss: 1.5668 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5648 - accuracy: 0.4313 - val_loss: 1.5967 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5823 - accuracy: 0.4313 - val_loss: 1.5983 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5667 - accuracy: 0.4313 - val_loss: 1.5954 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5740 - accuracy: 0.4313 - val_loss: 1.6011 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5664 - accuracy: 0.4313 - val_loss: 1.5957 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 139us/sample - loss: 1.5654 - accuracy: 0.4313 - val_loss: 1.5943 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 139us/sample - loss: 1.5736 - accuracy: 0.4313 - val_loss: 1.5934 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5665 - accuracy: 0.4313 - val_loss: 1.5947 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 139us/sample - loss: 1.5609 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 139us/sample - loss: 1.5712 - accuracy: 0.4313 - val_loss: 1.5923 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5684 - accuracy: 0.4313 - val_loss: 1.5962 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5608 - accuracy: 0.4313 - val_loss: 1.5934 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 139us/sample - loss: 1.5622 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5690 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5611 - accuracy: 0.4313 - val_loss: 1.5907 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5584 - accuracy: 0.4313 - val_loss: 1.5917 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5614 - accuracy: 0.4313 - val_loss: 1.5911 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 252us/sample - loss: 1.5680 - accuracy: 0.4313 - val_loss: 1.5893 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 252us/sample - loss: 1.5802 - accuracy: 0.4313 - val_loss: 1.5884 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.6991 - accuracy: 0.3922 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5742 - accuracy: 0.4297 - val_loss: 1.6198 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 104us/sample - loss: 1.5648 - accuracy: 0.4313 - val_loss: 1.5930 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5706 - accuracy: 0.4313 - val_loss: 1.6098 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 158us/sample - loss: 1.5842 - accuracy: 0.4313 - val_loss: 1.5914 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 92us/sample - loss: 1.5921 - accuracy: 0.4313 - val_loss: 1.6070 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5799 - accuracy: 0.4313 - val_loss: 1.5986 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5683 - accuracy: 0.4313 - val_loss: 1.6006 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5693 - accuracy: 0.4313 - val_loss: 1.5920 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5678 - accuracy: 0.4313 - val_loss: 1.5930 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5628 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5648 - accuracy: 0.4313 - val_loss: 1.5964 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5679 - accuracy: 0.4313 - val_loss: 1.5962 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5707 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5725 - accuracy: 0.4313 - val_loss: 1.5947 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5796 - accuracy: 0.4313 - val_loss: 1.5942 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5743 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5798 - accuracy: 0.4313 - val_loss: 1.5996 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5595 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5708 - accuracy: 0.4313 - val_loss: 1.6023 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5722 - accuracy: 0.4313 - val_loss: 1.5938 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 80us/sample - loss: 1.5661 - accuracy: 0.4313 - val_loss: 1.5959 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5524 - accuracy: 0.4313 - val_loss: 1.5928 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5712 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5665 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 144us/sample - loss: 1.5646 - accuracy: 0.4313 - val_loss: 1.5904 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 138us/sample - loss: 1.5627 - accuracy: 0.4313 - val_loss: 1.5891 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5622 - accuracy: 0.4313 - val_loss: 1.5997 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5688 - accuracy: 0.4313 - val_loss: 1.5899 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 306us/sample - loss: 1.5588 - accuracy: 0.4313 - val_loss: 1.5875 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 876us/sample - loss: 1.6828 - accuracy: 0.4125 - val_loss: 1.6091 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 107us/sample - loss: 1.5893 - accuracy: 0.4313 - val_loss: 1.6286 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 166us/sample - loss: 1.5832 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5724 - accuracy: 0.4313 - val_loss: 1.6029 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 159us/sample - loss: 1.5739 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5643 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 98us/sample - loss: 1.5724 - accuracy: 0.4313 - val_loss: 1.5965 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5759 - accuracy: 0.4313 - val_loss: 1.5984 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 92us/sample - loss: 1.5736 - accuracy: 0.4313 - val_loss: 1.5977 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5655 - accuracy: 0.4313 - val_loss: 1.5958 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 87us/sample - loss: 1.5748 - accuracy: 0.4313 - val_loss: 1.5931 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5714 - accuracy: 0.4313 - val_loss: 1.5924 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5632 - accuracy: 0.4313 - val_loss: 1.5980 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5633 - accuracy: 0.4313 - val_loss: 1.5918 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5758 - accuracy: 0.4313 - val_loss: 1.5988 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5802 - accuracy: 0.4313 - val_loss: 1.5933 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5739 - accuracy: 0.4313 - val_loss: 1.5952 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5887 - accuracy: 0.4313 - val_loss: 1.6033 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5679 - accuracy: 0.4313 - val_loss: 1.5948 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5677 - accuracy: 0.4313 - val_loss: 1.5935 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 143us/sample - loss: 1.5701 - accuracy: 0.4313 - val_loss: 1.5913 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 136us/sample - loss: 1.5743 - accuracy: 0.4313 - val_loss: 1.5886 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5686 - accuracy: 0.4313 - val_loss: 1.5936 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5663 - accuracy: 0.4313 - val_loss: 1.5897 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5692 - accuracy: 0.4313 - val_loss: 1.5916 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 368us/sample - loss: 1.5611 - accuracy: 0.4313 - val_loss: 1.5854 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5568 - accuracy: 0.4313 - val_loss: 1.5873 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 362us/sample - loss: 1.5534 - accuracy: 0.4313 - val_loss: 1.5833 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5454 - accuracy: 0.4313 - val_loss: 1.5883 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 477us/sample - loss: 1.5454 - accuracy: 0.4313 - val_loss: 1.5758 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 881us/sample - loss: 1.6962 - accuracy: 0.4031 - val_loss: 1.5968 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5808 - accuracy: 0.4313 - val_loss: 1.6191 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 164us/sample - loss: 1.5735 - accuracy: 0.4313 - val_loss: 1.5926 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 101us/sample - loss: 1.5745 - accuracy: 0.4313 - val_loss: 1.5972 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 99us/sample - loss: 1.5706 - accuracy: 0.4313 - val_loss: 1.5971 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5754 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 94us/sample - loss: 1.5770 - accuracy: 0.4313 - val_loss: 1.6002 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 92us/sample - loss: 1.5752 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 90us/sample - loss: 1.5701 - accuracy: 0.4313 - val_loss: 1.5932 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5647 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5665 - accuracy: 0.4313 - val_loss: 1.5977 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5689 - accuracy: 0.4313 - val_loss: 1.5900 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5718 - accuracy: 0.4313 - val_loss: 1.6026 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5732 - accuracy: 0.4313 - val_loss: 1.5922 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5635 - accuracy: 0.4313 - val_loss: 1.5956 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5721 - accuracy: 0.4313 - val_loss: 1.5972 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5694 - accuracy: 0.4313 - val_loss: 1.5946 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5675 - accuracy: 0.4313 - val_loss: 1.5947 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5569 - accuracy: 0.4313 - val_loss: 1.5910 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5614 - accuracy: 0.4313 - val_loss: 1.5940 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5669 - accuracy: 0.4313 - val_loss: 1.5913 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 140us/sample - loss: 1.5613 - accuracy: 0.4313 - val_loss: 1.5897 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5663 - accuracy: 0.4313 - val_loss: 1.5940 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 85us/sample - loss: 1.5595 - accuracy: 0.4313 - val_loss: 1.5905 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5624 - accuracy: 0.4313 - val_loss: 1.5978 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5711 - accuracy: 0.4313 - val_loss: 1.5882 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5648 - accuracy: 0.4313 - val_loss: 1.5894 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5535 - accuracy: 0.4313 - val_loss: 1.5918 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5569 - accuracy: 0.4313 - val_loss: 1.5909 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5509 - accuracy: 0.4313 - val_loss: 1.5896 - val_accuracy: 0.4313\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "640/640 [==============================] - 1s 1ms/sample - loss: 1.7012 - accuracy: 0.4000 - val_loss: 1.5921 - val_accuracy: 0.4313\n",
      "Epoch 2/30\n",
      "640/640 [==============================] - 0s 106us/sample - loss: 1.5886 - accuracy: 0.4313 - val_loss: 1.6114 - val_accuracy: 0.4313\n",
      "Epoch 3/30\n",
      "640/640 [==============================] - 0s 105us/sample - loss: 1.5709 - accuracy: 0.4313 - val_loss: 1.5976 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "640/640 [==============================] - 0s 162us/sample - loss: 1.5673 - accuracy: 0.4313 - val_loss: 1.5913 - val_accuracy: 0.4313\n",
      "Epoch 5/30\n",
      "640/640 [==============================] - 0s 100us/sample - loss: 1.5704 - accuracy: 0.4313 - val_loss: 1.5954 - val_accuracy: 0.4313\n",
      "Epoch 6/30\n",
      "640/640 [==============================] - 0s 98us/sample - loss: 1.5799 - accuracy: 0.4313 - val_loss: 1.6031 - val_accuracy: 0.4313\n",
      "Epoch 7/30\n",
      "640/640 [==============================] - 0s 92us/sample - loss: 1.5766 - accuracy: 0.4313 - val_loss: 1.5970 - val_accuracy: 0.4313\n",
      "Epoch 8/30\n",
      "640/640 [==============================] - 0s 91us/sample - loss: 1.5628 - accuracy: 0.4313 - val_loss: 1.5920 - val_accuracy: 0.4313\n",
      "Epoch 9/30\n",
      "640/640 [==============================] - 0s 92us/sample - loss: 1.5663 - accuracy: 0.4313 - val_loss: 1.5974 - val_accuracy: 0.4313\n",
      "Epoch 10/30\n",
      "640/640 [==============================] - 0s 89us/sample - loss: 1.5753 - accuracy: 0.4313 - val_loss: 1.5929 - val_accuracy: 0.4313\n",
      "Epoch 11/30\n",
      "640/640 [==============================] - 0s 88us/sample - loss: 1.5627 - accuracy: 0.4313 - val_loss: 1.5915 - val_accuracy: 0.4313\n",
      "Epoch 12/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5721 - accuracy: 0.4313 - val_loss: 1.5916 - val_accuracy: 0.4313\n",
      "Epoch 13/30\n",
      "640/640 [==============================] - 0s 86us/sample - loss: 1.5663 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5714 - accuracy: 0.4313 - val_loss: 1.5983 - val_accuracy: 0.4313\n",
      "Epoch 15/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5724 - accuracy: 0.4313 - val_loss: 1.6032 - val_accuracy: 0.4313\n",
      "Epoch 16/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5840 - accuracy: 0.4313 - val_loss: 1.5944 - val_accuracy: 0.4313\n",
      "Epoch 17/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5674 - accuracy: 0.4313 - val_loss: 1.5939 - val_accuracy: 0.4313\n",
      "Epoch 18/30\n",
      "640/640 [==============================] - 0s 81us/sample - loss: 1.5690 - accuracy: 0.4313 - val_loss: 1.5937 - val_accuracy: 0.4313\n",
      "Epoch 19/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5729 - accuracy: 0.4313 - val_loss: 1.5957 - val_accuracy: 0.4313\n",
      "Epoch 20/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5659 - accuracy: 0.4313 - val_loss: 1.5930 - val_accuracy: 0.4313\n",
      "Epoch 21/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5722 - accuracy: 0.4313 - val_loss: 1.5908 - val_accuracy: 0.4313\n",
      "Epoch 22/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5580 - accuracy: 0.4313 - val_loss: 1.5927 - val_accuracy: 0.4313\n",
      "Epoch 23/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5721 - accuracy: 0.4313 - val_loss: 1.5925 - val_accuracy: 0.4313\n",
      "Epoch 24/30\n",
      "640/640 [==============================] - 0s 141us/sample - loss: 1.5600 - accuracy: 0.4313 - val_loss: 1.5898 - val_accuracy: 0.4313\n",
      "Epoch 25/30\n",
      "640/640 [==============================] - 0s 82us/sample - loss: 1.5543 - accuracy: 0.4313 - val_loss: 1.5906 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "640/640 [==============================] - 0s 142us/sample - loss: 1.5578 - accuracy: 0.4313 - val_loss: 1.5883 - val_accuracy: 0.4313\n",
      "Epoch 27/30\n",
      "640/640 [==============================] - 0s 197us/sample - loss: 1.5599 - accuracy: 0.4313 - val_loss: 1.5882 - val_accuracy: 0.4313\n",
      "Epoch 28/30\n",
      "640/640 [==============================] - 0s 84us/sample - loss: 1.5665 - accuracy: 0.4313 - val_loss: 1.5953 - val_accuracy: 0.4313\n",
      "Epoch 29/30\n",
      "640/640 [==============================] - 0s 83us/sample - loss: 1.5680 - accuracy: 0.4313 - val_loss: 1.5883 - val_accuracy: 0.4313\n",
      "Epoch 30/30\n",
      "640/640 [==============================] - 0s 198us/sample - loss: 1.5673 - accuracy: 0.4313 - val_loss: 1.5880 - val_accuracy: 0.4313\n",
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbb4a8984fd4262b1ff8dc01bf97bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 1.6131 - accuracy: 0.4187 - val_loss: 1.5415 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 1s 896us/sample - loss: 1.5692 - accuracy: 0.4331 - val_loss: 1.5379 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 2s 941us/sample - loss: 1.5611 - accuracy: 0.4331 - val_loss: 1.5359 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5584 - accuracy: 0.4331 - val_loss: 1.5388 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 88us/sample - loss: 1.5670 - accuracy: 0.4331 - val_loss: 1.5365 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 82us/sample - loss: 1.5654 - accuracy: 0.4331 - val_loss: 1.5594 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5603 - accuracy: 0.4331 - val_loss: 1.5391 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5556 - accuracy: 0.4331 - val_loss: 1.5370 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 1s 876us/sample - loss: 1.5628 - accuracy: 0.4331 - val_loss: 1.5333 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5561 - accuracy: 0.4331 - val_loss: 1.5638 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5556 - accuracy: 0.4331 - val_loss: 1.5458 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5565 - accuracy: 0.4331 - val_loss: 1.5362 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5605 - accuracy: 0.4331 - val_loss: 1.5478 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 1s 882us/sample - loss: 1.5541 - accuracy: 0.4331 - val_loss: 1.5273 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5479 - accuracy: 0.4331 - val_loss: 1.5304 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5537 - accuracy: 0.4331 - val_loss: 1.5465 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 1s 899us/sample - loss: 1.5419 - accuracy: 0.4331 - val_loss: 1.5164 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 1s 886us/sample - loss: 1.5172 - accuracy: 0.4344 - val_loss: 1.4958 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 1.5047 - accuracy: 0.4456 - val_loss: 1.4652 - val_accuracy: 0.4850\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 1.4803 - accuracy: 0.4500 - val_loss: 1.4467 - val_accuracy: 0.4825\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 1.4688 - accuracy: 0.4606 - val_loss: 1.4320 - val_accuracy: 0.4825\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4604 - accuracy: 0.4644 - val_loss: 1.4427 - val_accuracy: 0.4775\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 2s 983us/sample - loss: 1.4531 - accuracy: 0.4675 - val_loss: 1.4112 - val_accuracy: 0.4900\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 1.4529 - accuracy: 0.4606 - val_loss: 1.4060 - val_accuracy: 0.4900\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 83us/sample - loss: 1.4354 - accuracy: 0.4681 - val_loss: 1.4096 - val_accuracy: 0.4925\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 1.4145 - accuracy: 0.4700 - val_loss: 1.3795 - val_accuracy: 0.4850\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 1.3963 - accuracy: 0.4756 - val_loss: 1.3607 - val_accuracy: 0.4975\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 1.4034 - accuracy: 0.4688 - val_loss: 1.3576 - val_accuracy: 0.5075\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 1.3759 - accuracy: 0.4869 - val_loss: 1.3314 - val_accuracy: 0.4925\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 1.3587 - accuracy: 0.4944 - val_loss: 1.3155 - val_accuracy: 0.5150\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 420us/sample - loss: 1.6139 - accuracy: 0.4244 - val_loss: 1.5492 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 123us/sample - loss: 1.5700 - accuracy: 0.4331 - val_loss: 1.5406 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 112us/sample - loss: 1.5604 - accuracy: 0.4331 - val_loss: 1.5361 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 85us/sample - loss: 1.5595 - accuracy: 0.4331 - val_loss: 1.5368 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 104us/sample - loss: 1.5589 - accuracy: 0.4331 - val_loss: 1.5350 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5624 - accuracy: 0.4331 - val_loss: 1.5394 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5561 - accuracy: 0.4331 - val_loss: 1.5323 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5610 - accuracy: 0.4331 - val_loss: 1.5499 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5545 - accuracy: 0.4331 - val_loss: 1.5360 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5669 - accuracy: 0.4331 - val_loss: 1.5311 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5556 - accuracy: 0.4331 - val_loss: 1.5448 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5574 - accuracy: 0.4331 - val_loss: 1.5431 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.5564 - accuracy: 0.4331 - val_loss: 1.5396 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5530 - accuracy: 0.4331 - val_loss: 1.5350 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5488 - accuracy: 0.4331 - val_loss: 1.5370 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5518 - accuracy: 0.4331 - val_loss: 1.5239 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5408 - accuracy: 0.4331 - val_loss: 1.5125 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5327 - accuracy: 0.4331 - val_loss: 1.5037 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5242 - accuracy: 0.4331 - val_loss: 1.4971 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5128 - accuracy: 0.4338 - val_loss: 1.4789 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4944 - accuracy: 0.4469 - val_loss: 1.4790 - val_accuracy: 0.4825\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 101us/sample - loss: 1.4939 - accuracy: 0.4531 - val_loss: 1.4527 - val_accuracy: 0.4900\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4734 - accuracy: 0.4606 - val_loss: 1.4396 - val_accuracy: 0.4875\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.4593 - accuracy: 0.4663 - val_loss: 1.4460 - val_accuracy: 0.4800\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4394 - accuracy: 0.4650 - val_loss: 1.4360 - val_accuracy: 0.4700\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.4415 - accuracy: 0.4669 - val_loss: 1.4259 - val_accuracy: 0.4900\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4388 - accuracy: 0.4706 - val_loss: 1.4544 - val_accuracy: 0.4800\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4235 - accuracy: 0.4731 - val_loss: 1.3925 - val_accuracy: 0.4975\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.4298 - accuracy: 0.4650 - val_loss: 1.3788 - val_accuracy: 0.5025\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 118us/sample - loss: 1.4103 - accuracy: 0.4675 - val_loss: 1.3652 - val_accuracy: 0.4950\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 423us/sample - loss: 1.5990 - accuracy: 0.4269 - val_loss: 1.5379 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5715 - accuracy: 0.4331 - val_loss: 1.5525 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 89us/sample - loss: 1.5692 - accuracy: 0.4331 - val_loss: 1.5718 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 83us/sample - loss: 1.5678 - accuracy: 0.4331 - val_loss: 1.5380 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5622 - accuracy: 0.4331 - val_loss: 1.5340 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5541 - accuracy: 0.4331 - val_loss: 1.5344 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5577 - accuracy: 0.4331 - val_loss: 1.5339 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5578 - accuracy: 0.4331 - val_loss: 1.5516 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5559 - accuracy: 0.4331 - val_loss: 1.5432 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5597 - accuracy: 0.4331 - val_loss: 1.5383 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5534 - accuracy: 0.4331 - val_loss: 1.5398 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5600 - accuracy: 0.4331 - val_loss: 1.5541 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5588 - accuracy: 0.4331 - val_loss: 1.5325 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.5486 - accuracy: 0.4331 - val_loss: 1.5520 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 115us/sample - loss: 1.5453 - accuracy: 0.4331 - val_loss: 1.5225 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5434 - accuracy: 0.4331 - val_loss: 1.5169 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5383 - accuracy: 0.4331 - val_loss: 1.5115 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5256 - accuracy: 0.4331 - val_loss: 1.4985 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5237 - accuracy: 0.4331 - val_loss: 1.4843 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5125 - accuracy: 0.4350 - val_loss: 1.4737 - val_accuracy: 0.4550\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4882 - accuracy: 0.4538 - val_loss: 1.4735 - val_accuracy: 0.4850\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4807 - accuracy: 0.4675 - val_loss: 1.4422 - val_accuracy: 0.4850\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.4528 - accuracy: 0.4700 - val_loss: 1.4335 - val_accuracy: 0.4925\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4464 - accuracy: 0.4669 - val_loss: 1.4201 - val_accuracy: 0.4900\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4345 - accuracy: 0.4706 - val_loss: 1.4366 - val_accuracy: 0.4800\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4186 - accuracy: 0.4750 - val_loss: 1.4041 - val_accuracy: 0.4850\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.4110 - accuracy: 0.4781 - val_loss: 1.3893 - val_accuracy: 0.4925\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.4043 - accuracy: 0.4731 - val_loss: 1.3605 - val_accuracy: 0.5000\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 123us/sample - loss: 1.3866 - accuracy: 0.4819 - val_loss: 1.3493 - val_accuracy: 0.5000\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 124us/sample - loss: 1.3714 - accuracy: 0.4850 - val_loss: 1.3355 - val_accuracy: 0.5075\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 423us/sample - loss: 1.6168 - accuracy: 0.4281 - val_loss: 1.5457 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 94us/sample - loss: 1.5583 - accuracy: 0.4331 - val_loss: 1.5467 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 90us/sample - loss: 1.5721 - accuracy: 0.4331 - val_loss: 1.5670 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 103us/sample - loss: 1.5636 - accuracy: 0.4331 - val_loss: 1.5404 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 77us/sample - loss: 1.5616 - accuracy: 0.4331 - val_loss: 1.5459 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5577 - accuracy: 0.4331 - val_loss: 1.5396 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5527 - accuracy: 0.4331 - val_loss: 1.5627 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.5566 - accuracy: 0.4331 - val_loss: 1.5444 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5688 - accuracy: 0.4331 - val_loss: 1.5616 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.5656 - accuracy: 0.4331 - val_loss: 1.5441 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5566 - accuracy: 0.4331 - val_loss: 1.5417 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.5539 - accuracy: 0.4331 - val_loss: 1.5406 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5547 - accuracy: 0.4331 - val_loss: 1.5399 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5585 - accuracy: 0.4331 - val_loss: 1.5327 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5576 - accuracy: 0.4331 - val_loss: 1.5379 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5551 - accuracy: 0.4331 - val_loss: 1.5480 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5508 - accuracy: 0.4331 - val_loss: 1.5281 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5554 - accuracy: 0.4331 - val_loss: 1.5306 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5517 - accuracy: 0.4331 - val_loss: 1.5217 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5439 - accuracy: 0.4331 - val_loss: 1.5392 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5551 - accuracy: 0.4331 - val_loss: 1.5416 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5476 - accuracy: 0.4331 - val_loss: 1.5210 - val_accuracy: 0.4500\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5215 - accuracy: 0.4331 - val_loss: 1.4921 - val_accuracy: 0.4500\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5100 - accuracy: 0.4363 - val_loss: 1.4880 - val_accuracy: 0.4750\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4917 - accuracy: 0.4462 - val_loss: 1.4534 - val_accuracy: 0.4750\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4783 - accuracy: 0.4538 - val_loss: 1.4450 - val_accuracy: 0.4750\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4781 - accuracy: 0.4531 - val_loss: 1.4464 - val_accuracy: 0.4700\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4685 - accuracy: 0.4688 - val_loss: 1.4316 - val_accuracy: 0.4700\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4527 - accuracy: 0.4706 - val_loss: 1.4163 - val_accuracy: 0.4825\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4500 - accuracy: 0.4619 - val_loss: 1.4316 - val_accuracy: 0.4825\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 523us/sample - loss: 1.6176 - accuracy: 0.4175 - val_loss: 1.5444 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5646 - accuracy: 0.4331 - val_loss: 1.5539 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 113us/sample - loss: 1.5656 - accuracy: 0.4331 - val_loss: 1.5402 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 85us/sample - loss: 1.5581 - accuracy: 0.4331 - val_loss: 1.5403 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 104us/sample - loss: 1.5543 - accuracy: 0.4331 - val_loss: 1.5341 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5578 - accuracy: 0.4331 - val_loss: 1.5326 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5636 - accuracy: 0.4331 - val_loss: 1.5459 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5554 - accuracy: 0.4331 - val_loss: 1.5595 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5577 - accuracy: 0.4331 - val_loss: 1.5344 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5608 - accuracy: 0.4331 - val_loss: 1.5371 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5549 - accuracy: 0.4331 - val_loss: 1.5372 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5592 - accuracy: 0.4331 - val_loss: 1.5392 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5556 - accuracy: 0.4331 - val_loss: 1.5378 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5581 - accuracy: 0.4331 - val_loss: 1.5331 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5594 - accuracy: 0.4331 - val_loss: 1.5331 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5503 - accuracy: 0.4331 - val_loss: 1.5305 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5507 - accuracy: 0.4331 - val_loss: 1.5248 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5460 - accuracy: 0.4331 - val_loss: 1.5160 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5392 - accuracy: 0.4331 - val_loss: 1.5045 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 77us/sample - loss: 1.5323 - accuracy: 0.4331 - val_loss: 1.5184 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5148 - accuracy: 0.4344 - val_loss: 1.4975 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5021 - accuracy: 0.4375 - val_loss: 1.4667 - val_accuracy: 0.4700\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4768 - accuracy: 0.4569 - val_loss: 1.4534 - val_accuracy: 0.4800\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4713 - accuracy: 0.4619 - val_loss: 1.4552 - val_accuracy: 0.4825\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4924 - accuracy: 0.4563 - val_loss: 1.4435 - val_accuracy: 0.4800\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4454 - accuracy: 0.4694 - val_loss: 1.4236 - val_accuracy: 0.4875\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.4563 - accuracy: 0.4581 - val_loss: 1.4152 - val_accuracy: 0.4825\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 122us/sample - loss: 1.4346 - accuracy: 0.4712 - val_loss: 1.3948 - val_accuracy: 0.4925\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 122us/sample - loss: 1.4083 - accuracy: 0.4806 - val_loss: 1.3830 - val_accuracy: 0.4925\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.4117 - accuracy: 0.4762 - val_loss: 1.3673 - val_accuracy: 0.4975\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 422us/sample - loss: 1.6343 - accuracy: 0.4187 - val_loss: 1.5801 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 122us/sample - loss: 1.5686 - accuracy: 0.4331 - val_loss: 1.5423 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 89us/sample - loss: 1.5655 - accuracy: 0.4331 - val_loss: 1.5563 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 106us/sample - loss: 1.5628 - accuracy: 0.4331 - val_loss: 1.5360 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 84us/sample - loss: 1.5575 - accuracy: 0.4331 - val_loss: 1.5514 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 83us/sample - loss: 1.5622 - accuracy: 0.4331 - val_loss: 1.5457 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 80us/sample - loss: 1.5630 - accuracy: 0.4331 - val_loss: 1.5407 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 79us/sample - loss: 1.5608 - accuracy: 0.4331 - val_loss: 1.5419 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 79us/sample - loss: 1.5636 - accuracy: 0.4331 - val_loss: 1.5495 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 79us/sample - loss: 1.5642 - accuracy: 0.4331 - val_loss: 1.5686 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 105us/sample - loss: 1.5543 - accuracy: 0.4331 - val_loss: 1.5337 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 103us/sample - loss: 1.5632 - accuracy: 0.4331 - val_loss: 1.5318 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5541 - accuracy: 0.4331 - val_loss: 1.5313 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5552 - accuracy: 0.4331 - val_loss: 1.5342 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5482 - accuracy: 0.4331 - val_loss: 1.5293 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5445 - accuracy: 0.4331 - val_loss: 1.5202 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5495 - accuracy: 0.4331 - val_loss: 1.5386 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5503 - accuracy: 0.4331 - val_loss: 1.5171 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5377 - accuracy: 0.4331 - val_loss: 1.5061 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5326 - accuracy: 0.4331 - val_loss: 1.5020 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5172 - accuracy: 0.4331 - val_loss: 1.4871 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 102us/sample - loss: 1.5105 - accuracy: 0.4381 - val_loss: 1.4712 - val_accuracy: 0.4500\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4922 - accuracy: 0.4437 - val_loss: 1.4593 - val_accuracy: 0.4650\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.4827 - accuracy: 0.4600 - val_loss: 1.4567 - val_accuracy: 0.4725\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4674 - accuracy: 0.4575 - val_loss: 1.4413 - val_accuracy: 0.4775\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4561 - accuracy: 0.4600 - val_loss: 1.4491 - val_accuracy: 0.4750\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4580 - accuracy: 0.4581 - val_loss: 1.4238 - val_accuracy: 0.4875\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4380 - accuracy: 0.4675 - val_loss: 1.4174 - val_accuracy: 0.4800\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4379 - accuracy: 0.4656 - val_loss: 1.4180 - val_accuracy: 0.4875\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.4216 - accuracy: 0.4706 - val_loss: 1.3929 - val_accuracy: 0.4875\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 418us/sample - loss: 1.6201 - accuracy: 0.4231 - val_loss: 1.5392 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.5606 - accuracy: 0.4331 - val_loss: 1.5390 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 88us/sample - loss: 1.5620 - accuracy: 0.4331 - val_loss: 1.5418 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 82us/sample - loss: 1.5612 - accuracy: 0.4331 - val_loss: 1.5456 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 79us/sample - loss: 1.5641 - accuracy: 0.4331 - val_loss: 1.5492 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5631 - accuracy: 0.4331 - val_loss: 1.5333 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5560 - accuracy: 0.4331 - val_loss: 1.5490 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5601 - accuracy: 0.4331 - val_loss: 1.5347 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5582 - accuracy: 0.4331 - val_loss: 1.5366 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5626 - accuracy: 0.4331 - val_loss: 1.5381 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5583 - accuracy: 0.4331 - val_loss: 1.5512 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5525 - accuracy: 0.4331 - val_loss: 1.5403 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5558 - accuracy: 0.4331 - val_loss: 1.5295 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5530 - accuracy: 0.4331 - val_loss: 1.5256 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5557 - accuracy: 0.4331 - val_loss: 1.5223 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5457 - accuracy: 0.4331 - val_loss: 1.5181 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5306 - accuracy: 0.4331 - val_loss: 1.5089 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5324 - accuracy: 0.4350 - val_loss: 1.4995 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5063 - accuracy: 0.4350 - val_loss: 1.4823 - val_accuracy: 0.4800\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4925 - accuracy: 0.4531 - val_loss: 1.4736 - val_accuracy: 0.4750\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4894 - accuracy: 0.4525 - val_loss: 1.4713 - val_accuracy: 0.4675\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4753 - accuracy: 0.4575 - val_loss: 1.4715 - val_accuracy: 0.4775\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4677 - accuracy: 0.4588 - val_loss: 1.4456 - val_accuracy: 0.4875\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4506 - accuracy: 0.4681 - val_loss: 1.4202 - val_accuracy: 0.4800\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4455 - accuracy: 0.4663 - val_loss: 1.4144 - val_accuracy: 0.4775\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4401 - accuracy: 0.4675 - val_loss: 1.4048 - val_accuracy: 0.4775\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 119us/sample - loss: 1.4316 - accuracy: 0.4700 - val_loss: 1.3927 - val_accuracy: 0.4850\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 120us/sample - loss: 1.4221 - accuracy: 0.4787 - val_loss: 1.3849 - val_accuracy: 0.4875\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4013 - accuracy: 0.4787 - val_loss: 1.3965 - val_accuracy: 0.4875\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 120us/sample - loss: 1.3990 - accuracy: 0.4762 - val_loss: 1.3753 - val_accuracy: 0.5100\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 418us/sample - loss: 1.6208 - accuracy: 0.4238 - val_loss: 1.5659 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 91us/sample - loss: 1.5581 - accuracy: 0.4331 - val_loss: 1.5675 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 90us/sample - loss: 1.5784 - accuracy: 0.4331 - val_loss: 1.5711 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5633 - accuracy: 0.4331 - val_loss: 1.5530 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5630 - accuracy: 0.4331 - val_loss: 1.5403 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5591 - accuracy: 0.4331 - val_loss: 1.5417 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5664 - accuracy: 0.4331 - val_loss: 1.5491 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5558 - accuracy: 0.4331 - val_loss: 1.5347 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5618 - accuracy: 0.4331 - val_loss: 1.5550 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5587 - accuracy: 0.4331 - val_loss: 1.5578 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5566 - accuracy: 0.4331 - val_loss: 1.5302 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5569 - accuracy: 0.4331 - val_loss: 1.5407 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5538 - accuracy: 0.4331 - val_loss: 1.5212 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5420 - accuracy: 0.4331 - val_loss: 1.5389 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5347 - accuracy: 0.4331 - val_loss: 1.5152 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5296 - accuracy: 0.4331 - val_loss: 1.5064 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5186 - accuracy: 0.4369 - val_loss: 1.4789 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4946 - accuracy: 0.4481 - val_loss: 1.4613 - val_accuracy: 0.4725\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4780 - accuracy: 0.4581 - val_loss: 1.4584 - val_accuracy: 0.4825\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4779 - accuracy: 0.4538 - val_loss: 1.4544 - val_accuracy: 0.4775\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4860 - accuracy: 0.4469 - val_loss: 1.4517 - val_accuracy: 0.4775\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4552 - accuracy: 0.4644 - val_loss: 1.4269 - val_accuracy: 0.4750\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4435 - accuracy: 0.4663 - val_loss: 1.4172 - val_accuracy: 0.4750\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4420 - accuracy: 0.4688 - val_loss: 1.4097 - val_accuracy: 0.4850\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4196 - accuracy: 0.4694 - val_loss: 1.3903 - val_accuracy: 0.4900\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 141us/sample - loss: 1.4139 - accuracy: 0.4719 - val_loss: 1.3752 - val_accuracy: 0.4850\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 188us/sample - loss: 1.3947 - accuracy: 0.4700 - val_loss: 1.3516 - val_accuracy: 0.4850\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 188us/sample - loss: 1.3867 - accuracy: 0.4794 - val_loss: 1.3412 - val_accuracy: 0.4825\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.3740 - accuracy: 0.4781 - val_loss: 1.3612 - val_accuracy: 0.4900\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 236us/sample - loss: 1.3609 - accuracy: 0.4806 - val_loss: 1.3253 - val_accuracy: 0.5100\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 522us/sample - loss: 1.6137 - accuracy: 0.4281 - val_loss: 1.5410 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 96us/sample - loss: 1.5638 - accuracy: 0.4331 - val_loss: 1.5415 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 89us/sample - loss: 1.5608 - accuracy: 0.4331 - val_loss: 1.5543 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 79us/sample - loss: 1.5643 - accuracy: 0.4331 - val_loss: 1.5610 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5662 - accuracy: 0.4331 - val_loss: 1.5543 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5669 - accuracy: 0.4331 - val_loss: 1.5394 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5588 - accuracy: 0.4331 - val_loss: 1.5535 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5650 - accuracy: 0.4331 - val_loss: 1.5581 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5621 - accuracy: 0.4331 - val_loss: 1.5348 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5535 - accuracy: 0.4331 - val_loss: 1.5404 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5552 - accuracy: 0.4331 - val_loss: 1.5399 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5555 - accuracy: 0.4331 - val_loss: 1.5305 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5504 - accuracy: 0.4331 - val_loss: 1.5270 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5513 - accuracy: 0.4331 - val_loss: 1.5286 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5443 - accuracy: 0.4331 - val_loss: 1.5190 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5304 - accuracy: 0.4331 - val_loss: 1.5146 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5245 - accuracy: 0.4344 - val_loss: 1.4946 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5160 - accuracy: 0.4350 - val_loss: 1.4776 - val_accuracy: 0.4750\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5056 - accuracy: 0.4431 - val_loss: 1.4692 - val_accuracy: 0.4525\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4910 - accuracy: 0.4531 - val_loss: 1.4571 - val_accuracy: 0.4800\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4879 - accuracy: 0.4406 - val_loss: 1.5005 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4928 - accuracy: 0.4481 - val_loss: 1.4753 - val_accuracy: 0.4600\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4816 - accuracy: 0.4525 - val_loss: 1.4379 - val_accuracy: 0.4875\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4519 - accuracy: 0.4606 - val_loss: 1.4287 - val_accuracy: 0.4875\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4472 - accuracy: 0.4706 - val_loss: 1.4445 - val_accuracy: 0.4825\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4398 - accuracy: 0.4625 - val_loss: 1.4194 - val_accuracy: 0.4750\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 107us/sample - loss: 1.4260 - accuracy: 0.4688 - val_loss: 1.4037 - val_accuracy: 0.4900\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4154 - accuracy: 0.4731 - val_loss: 1.3762 - val_accuracy: 0.4900\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.3987 - accuracy: 0.4831 - val_loss: 1.3632 - val_accuracy: 0.4825\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.3924 - accuracy: 0.4781 - val_loss: 1.3592 - val_accuracy: 0.5025\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 419us/sample - loss: 1.6396 - accuracy: 0.4175 - val_loss: 1.5613 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 122us/sample - loss: 1.5632 - accuracy: 0.4331 - val_loss: 1.5350 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 112us/sample - loss: 1.5622 - accuracy: 0.4331 - val_loss: 1.5334 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 84us/sample - loss: 1.5621 - accuracy: 0.4331 - val_loss: 1.5399 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 79us/sample - loss: 1.5682 - accuracy: 0.4331 - val_loss: 1.5349 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.5601 - accuracy: 0.4331 - val_loss: 1.5379 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5580 - accuracy: 0.4331 - val_loss: 1.5495 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5624 - accuracy: 0.4331 - val_loss: 1.5379 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5639 - accuracy: 0.4331 - val_loss: 1.5349 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5595 - accuracy: 0.4331 - val_loss: 1.5436 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5559 - accuracy: 0.4331 - val_loss: 1.5388 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5687 - accuracy: 0.4331 - val_loss: 1.5406 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5644 - accuracy: 0.4331 - val_loss: 1.5337 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5583 - accuracy: 0.4331 - val_loss: 1.5475 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5535 - accuracy: 0.4331 - val_loss: 1.5363 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5560 - accuracy: 0.4331 - val_loss: 1.5314 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5569 - accuracy: 0.4331 - val_loss: 1.5537 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5542 - accuracy: 0.4331 - val_loss: 1.5518 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5580 - accuracy: 0.4331 - val_loss: 1.5367 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5466 - accuracy: 0.4331 - val_loss: 1.5238 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5459 - accuracy: 0.4331 - val_loss: 1.5216 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5374 - accuracy: 0.4331 - val_loss: 1.5050 - val_accuracy: 0.4500\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5370 - accuracy: 0.4331 - val_loss: 1.5093 - val_accuracy: 0.4500\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5164 - accuracy: 0.4338 - val_loss: 1.4867 - val_accuracy: 0.4500\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5025 - accuracy: 0.4437 - val_loss: 1.4826 - val_accuracy: 0.4500\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4967 - accuracy: 0.4500 - val_loss: 1.4696 - val_accuracy: 0.4750\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4828 - accuracy: 0.4656 - val_loss: 1.4412 - val_accuracy: 0.4850\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4762 - accuracy: 0.4631 - val_loss: 1.4590 - val_accuracy: 0.4725\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4802 - accuracy: 0.4494 - val_loss: 1.4342 - val_accuracy: 0.4825\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4562 - accuracy: 0.4675 - val_loss: 1.4294 - val_accuracy: 0.4900\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 418us/sample - loss: 1.6155 - accuracy: 0.4269 - val_loss: 1.5537 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 114us/sample - loss: 1.5661 - accuracy: 0.4331 - val_loss: 1.5384 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 112us/sample - loss: 1.5673 - accuracy: 0.4331 - val_loss: 1.5380 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 106us/sample - loss: 1.5638 - accuracy: 0.4331 - val_loss: 1.5326 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 104us/sample - loss: 1.5650 - accuracy: 0.4331 - val_loss: 1.5322 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 79us/sample - loss: 1.5546 - accuracy: 0.4331 - val_loss: 1.5368 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 78us/sample - loss: 1.5636 - accuracy: 0.4331 - val_loss: 1.5387 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5583 - accuracy: 0.4331 - val_loss: 1.5510 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5599 - accuracy: 0.4331 - val_loss: 1.5457 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5594 - accuracy: 0.4331 - val_loss: 1.5339 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5588 - accuracy: 0.4331 - val_loss: 1.5469 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5543 - accuracy: 0.4331 - val_loss: 1.5320 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5576 - accuracy: 0.4331 - val_loss: 1.5303 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5580 - accuracy: 0.4331 - val_loss: 1.5336 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5430 - accuracy: 0.4331 - val_loss: 1.5334 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5437 - accuracy: 0.4331 - val_loss: 1.5184 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5351 - accuracy: 0.4331 - val_loss: 1.5082 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5249 - accuracy: 0.4331 - val_loss: 1.5096 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5290 - accuracy: 0.4319 - val_loss: 1.5424 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5195 - accuracy: 0.4356 - val_loss: 1.5142 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5062 - accuracy: 0.4456 - val_loss: 1.4804 - val_accuracy: 0.4800\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4771 - accuracy: 0.4625 - val_loss: 1.4572 - val_accuracy: 0.4775\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4660 - accuracy: 0.4675 - val_loss: 1.4320 - val_accuracy: 0.4875\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 120us/sample - loss: 1.4490 - accuracy: 0.4669 - val_loss: 1.4111 - val_accuracy: 0.4850\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4418 - accuracy: 0.4669 - val_loss: 1.4233 - val_accuracy: 0.4850\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 119us/sample - loss: 1.4420 - accuracy: 0.4638 - val_loss: 1.4025 - val_accuracy: 0.4850\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4138 - accuracy: 0.4769 - val_loss: 1.4077 - val_accuracy: 0.4850\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 120us/sample - loss: 1.4156 - accuracy: 0.4787 - val_loss: 1.3790 - val_accuracy: 0.4900\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 120us/sample - loss: 1.4034 - accuracy: 0.4781 - val_loss: 1.3648 - val_accuracy: 0.5000\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 143us/sample - loss: 1.3851 - accuracy: 0.4806 - val_loss: 1.3444 - val_accuracy: 0.5050\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 417us/sample - loss: 1.6284 - accuracy: 0.4162 - val_loss: 1.5711 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 116us/sample - loss: 1.5703 - accuracy: 0.4331 - val_loss: 1.5424 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 87us/sample - loss: 1.5616 - accuracy: 0.4331 - val_loss: 1.5448 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 82us/sample - loss: 1.5586 - accuracy: 0.4331 - val_loss: 1.5513 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 103us/sample - loss: 1.5592 - accuracy: 0.4331 - val_loss: 1.5377 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5614 - accuracy: 0.4331 - val_loss: 1.5389 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5583 - accuracy: 0.4331 - val_loss: 1.5375 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5573 - accuracy: 0.4331 - val_loss: 1.5313 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5562 - accuracy: 0.4331 - val_loss: 1.5653 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5616 - accuracy: 0.4331 - val_loss: 1.5343 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5553 - accuracy: 0.4331 - val_loss: 1.5422 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5538 - accuracy: 0.4331 - val_loss: 1.5320 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5561 - accuracy: 0.4331 - val_loss: 1.5419 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5462 - accuracy: 0.4331 - val_loss: 1.5261 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5502 - accuracy: 0.4331 - val_loss: 1.5207 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5485 - accuracy: 0.4331 - val_loss: 1.5398 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5416 - accuracy: 0.4331 - val_loss: 1.5120 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5282 - accuracy: 0.4331 - val_loss: 1.4964 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5272 - accuracy: 0.4331 - val_loss: 1.5286 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5295 - accuracy: 0.4344 - val_loss: 1.4890 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5085 - accuracy: 0.4363 - val_loss: 1.4798 - val_accuracy: 0.4900\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4826 - accuracy: 0.4519 - val_loss: 1.4538 - val_accuracy: 0.4800\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 115us/sample - loss: 1.4698 - accuracy: 0.4531 - val_loss: 1.4401 - val_accuracy: 0.4850\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4639 - accuracy: 0.4600 - val_loss: 1.4310 - val_accuracy: 0.4875\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4562 - accuracy: 0.4606 - val_loss: 1.4264 - val_accuracy: 0.4850\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4443 - accuracy: 0.4681 - val_loss: 1.4126 - val_accuracy: 0.4825\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4320 - accuracy: 0.4656 - val_loss: 1.3985 - val_accuracy: 0.4850\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4294 - accuracy: 0.4594 - val_loss: 1.4205 - val_accuracy: 0.4850\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4158 - accuracy: 0.4725 - val_loss: 1.3681 - val_accuracy: 0.4950\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4094 - accuracy: 0.4688 - val_loss: 1.3719 - val_accuracy: 0.4825\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 421us/sample - loss: 1.6236 - accuracy: 0.4125 - val_loss: 1.5642 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 114us/sample - loss: 1.5643 - accuracy: 0.4331 - val_loss: 1.5498 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 112us/sample - loss: 1.5630 - accuracy: 0.4331 - val_loss: 1.5367 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 82us/sample - loss: 1.5632 - accuracy: 0.4331 - val_loss: 1.5456 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 78us/sample - loss: 1.5557 - accuracy: 0.4331 - val_loss: 1.5617 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5544 - accuracy: 0.4331 - val_loss: 1.5563 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5573 - accuracy: 0.4331 - val_loss: 1.5577 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5630 - accuracy: 0.4331 - val_loss: 1.5592 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5584 - accuracy: 0.4331 - val_loss: 1.5394 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5603 - accuracy: 0.4331 - val_loss: 1.5367 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5580 - accuracy: 0.4331 - val_loss: 1.5330 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5542 - accuracy: 0.4331 - val_loss: 1.5387 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5586 - accuracy: 0.4331 - val_loss: 1.5363 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 96us/sample - loss: 1.5539 - accuracy: 0.4331 - val_loss: 1.5316 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5480 - accuracy: 0.4331 - val_loss: 1.5585 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5653 - accuracy: 0.4331 - val_loss: 1.5315 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5581 - accuracy: 0.4331 - val_loss: 1.5278 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5472 - accuracy: 0.4331 - val_loss: 1.5453 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5608 - accuracy: 0.4331 - val_loss: 1.5236 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5393 - accuracy: 0.4331 - val_loss: 1.5122 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5417 - accuracy: 0.4331 - val_loss: 1.5279 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5276 - accuracy: 0.4331 - val_loss: 1.5028 - val_accuracy: 0.4500\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5104 - accuracy: 0.4331 - val_loss: 1.4821 - val_accuracy: 0.4500\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5010 - accuracy: 0.4350 - val_loss: 1.4654 - val_accuracy: 0.4650\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4899 - accuracy: 0.4519 - val_loss: 1.4524 - val_accuracy: 0.4875\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4733 - accuracy: 0.4581 - val_loss: 1.4381 - val_accuracy: 0.4925\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4655 - accuracy: 0.4638 - val_loss: 1.4461 - val_accuracy: 0.4925\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4561 - accuracy: 0.4700 - val_loss: 1.4177 - val_accuracy: 0.4950\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4292 - accuracy: 0.4750 - val_loss: 1.3977 - val_accuracy: 0.4875\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4186 - accuracy: 0.4688 - val_loss: 1.3831 - val_accuracy: 0.4925\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 417us/sample - loss: 1.6255 - accuracy: 0.4200 - val_loss: 1.5721 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 114us/sample - loss: 1.5668 - accuracy: 0.4331 - val_loss: 1.5362 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 89us/sample - loss: 1.5631 - accuracy: 0.4331 - val_loss: 1.5417 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 81us/sample - loss: 1.5603 - accuracy: 0.4331 - val_loss: 1.5522 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 77us/sample - loss: 1.5630 - accuracy: 0.4331 - val_loss: 1.5483 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.5588 - accuracy: 0.4331 - val_loss: 1.5416 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5672 - accuracy: 0.4331 - val_loss: 1.5370 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5595 - accuracy: 0.4331 - val_loss: 1.5350 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5620 - accuracy: 0.4331 - val_loss: 1.5442 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5554 - accuracy: 0.4331 - val_loss: 1.5474 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5608 - accuracy: 0.4331 - val_loss: 1.5376 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5565 - accuracy: 0.4331 - val_loss: 1.5331 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5558 - accuracy: 0.4331 - val_loss: 1.5394 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5605 - accuracy: 0.4331 - val_loss: 1.5324 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5592 - accuracy: 0.4331 - val_loss: 1.5337 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5556 - accuracy: 0.4331 - val_loss: 1.5365 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5565 - accuracy: 0.4331 - val_loss: 1.5364 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5586 - accuracy: 0.4331 - val_loss: 1.5655 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5624 - accuracy: 0.4331 - val_loss: 1.5433 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5583 - accuracy: 0.4331 - val_loss: 1.5308 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5513 - accuracy: 0.4331 - val_loss: 1.5364 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5566 - accuracy: 0.4331 - val_loss: 1.5361 - val_accuracy: 0.4500\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5488 - accuracy: 0.4331 - val_loss: 1.5492 - val_accuracy: 0.4500\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5514 - accuracy: 0.4331 - val_loss: 1.5222 - val_accuracy: 0.4500\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5465 - accuracy: 0.4331 - val_loss: 1.5209 - val_accuracy: 0.4500\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5367 - accuracy: 0.4331 - val_loss: 1.5177 - val_accuracy: 0.4500\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5289 - accuracy: 0.4331 - val_loss: 1.4992 - val_accuracy: 0.4500\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5266 - accuracy: 0.4350 - val_loss: 1.4915 - val_accuracy: 0.4500\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5052 - accuracy: 0.4350 - val_loss: 1.4781 - val_accuracy: 0.4700\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4906 - accuracy: 0.4456 - val_loss: 1.4625 - val_accuracy: 0.4700\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 418us/sample - loss: 1.6290 - accuracy: 0.4169 - val_loss: 1.5716 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 116us/sample - loss: 1.5691 - accuracy: 0.4331 - val_loss: 1.5428 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 110us/sample - loss: 1.5673 - accuracy: 0.4331 - val_loss: 1.5375 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 82us/sample - loss: 1.5614 - accuracy: 0.4331 - val_loss: 1.5413 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 78us/sample - loss: 1.5589 - accuracy: 0.4331 - val_loss: 1.5385 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5633 - accuracy: 0.4331 - val_loss: 1.5322 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5706 - accuracy: 0.4331 - val_loss: 1.5341 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5595 - accuracy: 0.4331 - val_loss: 1.5410 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5612 - accuracy: 0.4331 - val_loss: 1.5324 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5562 - accuracy: 0.4331 - val_loss: 1.5356 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5639 - accuracy: 0.4331 - val_loss: 1.5386 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5568 - accuracy: 0.4331 - val_loss: 1.5347 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5515 - accuracy: 0.4331 - val_loss: 1.5312 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5562 - accuracy: 0.4331 - val_loss: 1.5307 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5519 - accuracy: 0.4331 - val_loss: 1.5288 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5430 - accuracy: 0.4331 - val_loss: 1.5224 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5466 - accuracy: 0.4331 - val_loss: 1.5172 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5278 - accuracy: 0.4331 - val_loss: 1.5094 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5285 - accuracy: 0.4331 - val_loss: 1.4993 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5154 - accuracy: 0.4356 - val_loss: 1.4854 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4973 - accuracy: 0.4481 - val_loss: 1.4665 - val_accuracy: 0.4750\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 120us/sample - loss: 1.4883 - accuracy: 0.4588 - val_loss: 1.4550 - val_accuracy: 0.4825\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 120us/sample - loss: 1.4733 - accuracy: 0.4619 - val_loss: 1.4437 - val_accuracy: 0.4875\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 122us/sample - loss: 1.4605 - accuracy: 0.4644 - val_loss: 1.4290 - val_accuracy: 0.4825\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4589 - accuracy: 0.4688 - val_loss: 1.4394 - val_accuracy: 0.4850\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 118us/sample - loss: 1.4483 - accuracy: 0.4631 - val_loss: 1.4098 - val_accuracy: 0.4950\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.4355 - accuracy: 0.4644 - val_loss: 1.4210 - val_accuracy: 0.4875\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 119us/sample - loss: 1.4288 - accuracy: 0.4644 - val_loss: 1.3933 - val_accuracy: 0.4875\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 143us/sample - loss: 1.4157 - accuracy: 0.4731 - val_loss: 1.3795 - val_accuracy: 0.4975\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 164us/sample - loss: 1.4025 - accuracy: 0.4806 - val_loss: 1.3632 - val_accuracy: 0.4975\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 421us/sample - loss: 1.6060 - accuracy: 0.4231 - val_loss: 1.5638 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.5656 - accuracy: 0.4331 - val_loss: 1.5318 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 89us/sample - loss: 1.5611 - accuracy: 0.4331 - val_loss: 1.5655 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 81us/sample - loss: 1.5637 - accuracy: 0.4331 - val_loss: 1.5441 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 78us/sample - loss: 1.5607 - accuracy: 0.4331 - val_loss: 1.5378 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5670 - accuracy: 0.4331 - val_loss: 1.5390 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5588 - accuracy: 0.4331 - val_loss: 1.5415 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5612 - accuracy: 0.4331 - val_loss: 1.5339 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5572 - accuracy: 0.4331 - val_loss: 1.5393 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5588 - accuracy: 0.4331 - val_loss: 1.5348 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5553 - accuracy: 0.4331 - val_loss: 1.5366 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5557 - accuracy: 0.4331 - val_loss: 1.5409 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5528 - accuracy: 0.4331 - val_loss: 1.5411 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5532 - accuracy: 0.4331 - val_loss: 1.5272 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5523 - accuracy: 0.4331 - val_loss: 1.5578 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5613 - accuracy: 0.4331 - val_loss: 1.5471 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5536 - accuracy: 0.4331 - val_loss: 1.5251 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5416 - accuracy: 0.4331 - val_loss: 1.5150 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5322 - accuracy: 0.4331 - val_loss: 1.5275 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5211 - accuracy: 0.4331 - val_loss: 1.4903 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5137 - accuracy: 0.4363 - val_loss: 1.4668 - val_accuracy: 0.4600\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4988 - accuracy: 0.4500 - val_loss: 1.4608 - val_accuracy: 0.4600\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4884 - accuracy: 0.4550 - val_loss: 1.4555 - val_accuracy: 0.4750\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4666 - accuracy: 0.4625 - val_loss: 1.4467 - val_accuracy: 0.4850\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4560 - accuracy: 0.4650 - val_loss: 1.4242 - val_accuracy: 0.4875\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4508 - accuracy: 0.4712 - val_loss: 1.4110 - val_accuracy: 0.4900\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4499 - accuracy: 0.4663 - val_loss: 1.4046 - val_accuracy: 0.4925\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4319 - accuracy: 0.4669 - val_loss: 1.4065 - val_accuracy: 0.4875\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4109 - accuracy: 0.4762 - val_loss: 1.3857 - val_accuracy: 0.4925\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4055 - accuracy: 0.4750 - val_loss: 1.3819 - val_accuracy: 0.4925\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 419us/sample - loss: 1.6303 - accuracy: 0.4225 - val_loss: 1.6070 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 117us/sample - loss: 1.5782 - accuracy: 0.4331 - val_loss: 1.5683 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 112us/sample - loss: 1.5666 - accuracy: 0.4331 - val_loss: 1.5388 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 106us/sample - loss: 1.5602 - accuracy: 0.4331 - val_loss: 1.5386 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 102us/sample - loss: 1.5622 - accuracy: 0.4331 - val_loss: 1.5337 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.5624 - accuracy: 0.4331 - val_loss: 1.5391 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5650 - accuracy: 0.4331 - val_loss: 1.5442 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5589 - accuracy: 0.4331 - val_loss: 1.5454 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5579 - accuracy: 0.4331 - val_loss: 1.5557 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5588 - accuracy: 0.4331 - val_loss: 1.5618 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5592 - accuracy: 0.4331 - val_loss: 1.5566 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5622 - accuracy: 0.4331 - val_loss: 1.5515 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5603 - accuracy: 0.4331 - val_loss: 1.5383 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5581 - accuracy: 0.4331 - val_loss: 1.5353 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5570 - accuracy: 0.4331 - val_loss: 1.5330 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5545 - accuracy: 0.4331 - val_loss: 1.5366 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5574 - accuracy: 0.4331 - val_loss: 1.5308 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5537 - accuracy: 0.4331 - val_loss: 1.5288 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5547 - accuracy: 0.4331 - val_loss: 1.5241 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5462 - accuracy: 0.4331 - val_loss: 1.5194 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5403 - accuracy: 0.4331 - val_loss: 1.5530 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5402 - accuracy: 0.4331 - val_loss: 1.5033 - val_accuracy: 0.4500\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5171 - accuracy: 0.4350 - val_loss: 1.4887 - val_accuracy: 0.4500\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5096 - accuracy: 0.4444 - val_loss: 1.4746 - val_accuracy: 0.4825\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4806 - accuracy: 0.4594 - val_loss: 1.4668 - val_accuracy: 0.4775\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 115us/sample - loss: 1.4924 - accuracy: 0.4512 - val_loss: 1.4610 - val_accuracy: 0.4775\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4656 - accuracy: 0.4663 - val_loss: 1.4408 - val_accuracy: 0.4800\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4660 - accuracy: 0.4631 - val_loss: 1.4488 - val_accuracy: 0.4825\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4670 - accuracy: 0.4619 - val_loss: 1.4356 - val_accuracy: 0.4825\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4556 - accuracy: 0.4638 - val_loss: 1.4322 - val_accuracy: 0.4825\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 422us/sample - loss: 1.6211 - accuracy: 0.4187 - val_loss: 1.5614 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.5795 - accuracy: 0.4331 - val_loss: 1.5352 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 89us/sample - loss: 1.5632 - accuracy: 0.4331 - val_loss: 1.5481 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 83us/sample - loss: 1.5671 - accuracy: 0.4331 - val_loss: 1.5597 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 103us/sample - loss: 1.5515 - accuracy: 0.4331 - val_loss: 1.5337 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5604 - accuracy: 0.4331 - val_loss: 1.5319 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5594 - accuracy: 0.4331 - val_loss: 1.5329 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5605 - accuracy: 0.4331 - val_loss: 1.5334 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5526 - accuracy: 0.4331 - val_loss: 1.5537 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5549 - accuracy: 0.4331 - val_loss: 1.5448 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5628 - accuracy: 0.4331 - val_loss: 1.5534 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5574 - accuracy: 0.4331 - val_loss: 1.5297 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5597 - accuracy: 0.4331 - val_loss: 1.5309 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5515 - accuracy: 0.4331 - val_loss: 1.5427 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5561 - accuracy: 0.4331 - val_loss: 1.5421 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5521 - accuracy: 0.4331 - val_loss: 1.5268 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5440 - accuracy: 0.4331 - val_loss: 1.5171 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5361 - accuracy: 0.4331 - val_loss: 1.5378 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5292 - accuracy: 0.4331 - val_loss: 1.5005 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5066 - accuracy: 0.4331 - val_loss: 1.4737 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5013 - accuracy: 0.4431 - val_loss: 1.4735 - val_accuracy: 0.4825\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4825 - accuracy: 0.4500 - val_loss: 1.4460 - val_accuracy: 0.4850\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4948 - accuracy: 0.4550 - val_loss: 1.4571 - val_accuracy: 0.4725\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4732 - accuracy: 0.4600 - val_loss: 1.4523 - val_accuracy: 0.4800\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 120us/sample - loss: 1.4586 - accuracy: 0.4656 - val_loss: 1.4209 - val_accuracy: 0.4875\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4436 - accuracy: 0.4700 - val_loss: 1.4340 - val_accuracy: 0.4850\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.4319 - accuracy: 0.4675 - val_loss: 1.4090 - val_accuracy: 0.4800\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.4374 - accuracy: 0.4681 - val_loss: 1.4028 - val_accuracy: 0.4850\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 122us/sample - loss: 1.4255 - accuracy: 0.4700 - val_loss: 1.3906 - val_accuracy: 0.4925\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 143us/sample - loss: 1.4166 - accuracy: 0.4756 - val_loss: 1.3755 - val_accuracy: 0.4975\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 421us/sample - loss: 1.6210 - accuracy: 0.4238 - val_loss: 1.5600 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.5744 - accuracy: 0.4331 - val_loss: 1.5365 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 114us/sample - loss: 1.5586 - accuracy: 0.4331 - val_loss: 1.5346 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 106us/sample - loss: 1.5699 - accuracy: 0.4331 - val_loss: 1.5344 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 79us/sample - loss: 1.5574 - accuracy: 0.4331 - val_loss: 1.5422 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5616 - accuracy: 0.4331 - val_loss: 1.5350 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5549 - accuracy: 0.4331 - val_loss: 1.5320 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5646 - accuracy: 0.4331 - val_loss: 1.5465 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5596 - accuracy: 0.4331 - val_loss: 1.5417 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5580 - accuracy: 0.4331 - val_loss: 1.5498 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5660 - accuracy: 0.4331 - val_loss: 1.5457 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5547 - accuracy: 0.4331 - val_loss: 1.5321 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5562 - accuracy: 0.4331 - val_loss: 1.5322 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5569 - accuracy: 0.4331 - val_loss: 1.5416 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5543 - accuracy: 0.4331 - val_loss: 1.5300 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5565 - accuracy: 0.4331 - val_loss: 1.5290 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5485 - accuracy: 0.4331 - val_loss: 1.5321 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5510 - accuracy: 0.4331 - val_loss: 1.5254 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5493 - accuracy: 0.4331 - val_loss: 1.5302 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5434 - accuracy: 0.4331 - val_loss: 1.5229 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5383 - accuracy: 0.4331 - val_loss: 1.5134 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5364 - accuracy: 0.4331 - val_loss: 1.5010 - val_accuracy: 0.4500\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5264 - accuracy: 0.4331 - val_loss: 1.5118 - val_accuracy: 0.4500\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5179 - accuracy: 0.4331 - val_loss: 1.4793 - val_accuracy: 0.4500\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4950 - accuracy: 0.4412 - val_loss: 1.4606 - val_accuracy: 0.4600\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4879 - accuracy: 0.4450 - val_loss: 1.4432 - val_accuracy: 0.4775\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4685 - accuracy: 0.4613 - val_loss: 1.5345 - val_accuracy: 0.3950\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4698 - accuracy: 0.4563 - val_loss: 1.4162 - val_accuracy: 0.4950\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4328 - accuracy: 0.4688 - val_loss: 1.4047 - val_accuracy: 0.5000\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4205 - accuracy: 0.4769 - val_loss: 1.3993 - val_accuracy: 0.4875\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 530us/sample - loss: 1.6238 - accuracy: 0.4225 - val_loss: 1.5427 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5681 - accuracy: 0.4331 - val_loss: 1.5502 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 91us/sample - loss: 1.5623 - accuracy: 0.4331 - val_loss: 1.5457 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 81us/sample - loss: 1.5555 - accuracy: 0.4331 - val_loss: 1.5508 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 115us/sample - loss: 1.5660 - accuracy: 0.4331 - val_loss: 1.5333 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5638 - accuracy: 0.4331 - val_loss: 1.5326 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5638 - accuracy: 0.4331 - val_loss: 1.5389 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5632 - accuracy: 0.4331 - val_loss: 1.5434 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5583 - accuracy: 0.4331 - val_loss: 1.5326 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5621 - accuracy: 0.4331 - val_loss: 1.5411 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5560 - accuracy: 0.4331 - val_loss: 1.5347 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5524 - accuracy: 0.4331 - val_loss: 1.5304 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5482 - accuracy: 0.4331 - val_loss: 1.5349 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5583 - accuracy: 0.4331 - val_loss: 1.5545 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5639 - accuracy: 0.4331 - val_loss: 1.5608 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5595 - accuracy: 0.4331 - val_loss: 1.5268 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5525 - accuracy: 0.4331 - val_loss: 1.5257 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5384 - accuracy: 0.4331 - val_loss: 1.5149 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5351 - accuracy: 0.4331 - val_loss: 1.5081 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5202 - accuracy: 0.4356 - val_loss: 1.4809 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5108 - accuracy: 0.4325 - val_loss: 1.4650 - val_accuracy: 0.4550\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4913 - accuracy: 0.4550 - val_loss: 1.4819 - val_accuracy: 0.4500\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4911 - accuracy: 0.4538 - val_loss: 1.4403 - val_accuracy: 0.4900\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4573 - accuracy: 0.4606 - val_loss: 1.4367 - val_accuracy: 0.4875\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4500 - accuracy: 0.4675 - val_loss: 1.4127 - val_accuracy: 0.4875\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4539 - accuracy: 0.4613 - val_loss: 1.4250 - val_accuracy: 0.4800\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 133us/sample - loss: 1.4280 - accuracy: 0.4737 - val_loss: 1.3927 - val_accuracy: 0.4975\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.4143 - accuracy: 0.4731 - val_loss: 1.3816 - val_accuracy: 0.4925\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4168 - accuracy: 0.4731 - val_loss: 1.3857 - val_accuracy: 0.5000\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 278us/sample - loss: 1.3983 - accuracy: 0.4850 - val_loss: 1.3614 - val_accuracy: 0.4975\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 418us/sample - loss: 1.6113 - accuracy: 0.4194 - val_loss: 1.5790 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.5631 - accuracy: 0.4331 - val_loss: 1.5370 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 112us/sample - loss: 1.5811 - accuracy: 0.4331 - val_loss: 1.5330 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 85us/sample - loss: 1.5616 - accuracy: 0.4331 - val_loss: 1.5338 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 80us/sample - loss: 1.5589 - accuracy: 0.4331 - val_loss: 1.5410 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.5584 - accuracy: 0.4331 - val_loss: 1.5457 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 116us/sample - loss: 1.5615 - accuracy: 0.4331 - val_loss: 1.5328 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5644 - accuracy: 0.4331 - val_loss: 1.5324 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5616 - accuracy: 0.4331 - val_loss: 1.5455 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5546 - accuracy: 0.4331 - val_loss: 1.5450 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5620 - accuracy: 0.4331 - val_loss: 1.5395 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5694 - accuracy: 0.4331 - val_loss: 1.5472 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5599 - accuracy: 0.4331 - val_loss: 1.5455 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5554 - accuracy: 0.4331 - val_loss: 1.5446 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5604 - accuracy: 0.4331 - val_loss: 1.5528 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5516 - accuracy: 0.4331 - val_loss: 1.5319 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5511 - accuracy: 0.4331 - val_loss: 1.5210 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5427 - accuracy: 0.4331 - val_loss: 1.5195 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5361 - accuracy: 0.4331 - val_loss: 1.5178 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5287 - accuracy: 0.4331 - val_loss: 1.5017 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5191 - accuracy: 0.4338 - val_loss: 1.4834 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5098 - accuracy: 0.4400 - val_loss: 1.4602 - val_accuracy: 0.4500\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4928 - accuracy: 0.4519 - val_loss: 1.4483 - val_accuracy: 0.4950\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4946 - accuracy: 0.4544 - val_loss: 1.4706 - val_accuracy: 0.4875\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4647 - accuracy: 0.4650 - val_loss: 1.4412 - val_accuracy: 0.4850\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4538 - accuracy: 0.4650 - val_loss: 1.4576 - val_accuracy: 0.4800\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4360 - accuracy: 0.4644 - val_loss: 1.4031 - val_accuracy: 0.4900\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4418 - accuracy: 0.4700 - val_loss: 1.3996 - val_accuracy: 0.4900\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4345 - accuracy: 0.4638 - val_loss: 1.4009 - val_accuracy: 0.4900\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4262 - accuracy: 0.4669 - val_loss: 1.3765 - val_accuracy: 0.4975\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 419us/sample - loss: 1.6235 - accuracy: 0.4256 - val_loss: 1.5520 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 116us/sample - loss: 1.5593 - accuracy: 0.4331 - val_loss: 1.5406 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 88us/sample - loss: 1.5630 - accuracy: 0.4331 - val_loss: 1.5492 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 83us/sample - loss: 1.5667 - accuracy: 0.4331 - val_loss: 1.5488 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5655 - accuracy: 0.4331 - val_loss: 1.5365 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5584 - accuracy: 0.4331 - val_loss: 1.5489 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5594 - accuracy: 0.4331 - val_loss: 1.5424 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5583 - accuracy: 0.4331 - val_loss: 1.5591 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5747 - accuracy: 0.4331 - val_loss: 1.5357 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5569 - accuracy: 0.4331 - val_loss: 1.5322 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5651 - accuracy: 0.4331 - val_loss: 1.5345 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5544 - accuracy: 0.4331 - val_loss: 1.5415 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5624 - accuracy: 0.4331 - val_loss: 1.5498 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5565 - accuracy: 0.4331 - val_loss: 1.5382 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5466 - accuracy: 0.4331 - val_loss: 1.5238 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5443 - accuracy: 0.4331 - val_loss: 1.5162 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5358 - accuracy: 0.4331 - val_loss: 1.5018 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5107 - accuracy: 0.4375 - val_loss: 1.4797 - val_accuracy: 0.4825\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4986 - accuracy: 0.4550 - val_loss: 1.4954 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4985 - accuracy: 0.4494 - val_loss: 1.4596 - val_accuracy: 0.4700\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4885 - accuracy: 0.4500 - val_loss: 1.4449 - val_accuracy: 0.4875\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4625 - accuracy: 0.4581 - val_loss: 1.4500 - val_accuracy: 0.4800\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4609 - accuracy: 0.4594 - val_loss: 1.4431 - val_accuracy: 0.4775\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4473 - accuracy: 0.4650 - val_loss: 1.4605 - val_accuracy: 0.4775\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4420 - accuracy: 0.4663 - val_loss: 1.4059 - val_accuracy: 0.4925\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.4249 - accuracy: 0.4700 - val_loss: 1.4125 - val_accuracy: 0.4850\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4113 - accuracy: 0.4712 - val_loss: 1.3897 - val_accuracy: 0.4925\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4075 - accuracy: 0.4725 - val_loss: 1.3788 - val_accuracy: 0.4975\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 135us/sample - loss: 1.4027 - accuracy: 0.4794 - val_loss: 1.3651 - val_accuracy: 0.4925\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 1s 323us/sample - loss: 1.4002 - accuracy: 0.4775 - val_loss: 1.3533 - val_accuracy: 0.5000\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 531us/sample - loss: 1.6305 - accuracy: 0.4238 - val_loss: 1.5520 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 116us/sample - loss: 1.5630 - accuracy: 0.4331 - val_loss: 1.5372 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 88us/sample - loss: 1.5570 - accuracy: 0.4331 - val_loss: 1.5418 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 83us/sample - loss: 1.5647 - accuracy: 0.4331 - val_loss: 1.5378 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5648 - accuracy: 0.4331 - val_loss: 1.5751 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5659 - accuracy: 0.4331 - val_loss: 1.5521 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5559 - accuracy: 0.4331 - val_loss: 1.5385 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5659 - accuracy: 0.4331 - val_loss: 1.5517 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5589 - accuracy: 0.4331 - val_loss: 1.5436 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5575 - accuracy: 0.4331 - val_loss: 1.5326 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5594 - accuracy: 0.4331 - val_loss: 1.5374 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 76us/sample - loss: 1.5539 - accuracy: 0.4331 - val_loss: 1.5679 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5556 - accuracy: 0.4331 - val_loss: 1.5315 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5525 - accuracy: 0.4331 - val_loss: 1.5363 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5656 - accuracy: 0.4331 - val_loss: 1.5351 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5509 - accuracy: 0.4331 - val_loss: 1.5313 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5557 - accuracy: 0.4331 - val_loss: 1.5298 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5533 - accuracy: 0.4331 - val_loss: 1.5436 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5559 - accuracy: 0.4331 - val_loss: 1.5393 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5485 - accuracy: 0.4331 - val_loss: 1.5332 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 102us/sample - loss: 1.5405 - accuracy: 0.4331 - val_loss: 1.5172 - val_accuracy: 0.4500\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5381 - accuracy: 0.4331 - val_loss: 1.5122 - val_accuracy: 0.4500\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5210 - accuracy: 0.4331 - val_loss: 1.4947 - val_accuracy: 0.4500\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5054 - accuracy: 0.4406 - val_loss: 1.4736 - val_accuracy: 0.4750\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4821 - accuracy: 0.4594 - val_loss: 1.4719 - val_accuracy: 0.4800\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 112us/sample - loss: 1.4818 - accuracy: 0.4600 - val_loss: 1.4358 - val_accuracy: 0.4850\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4669 - accuracy: 0.4563 - val_loss: 1.4396 - val_accuracy: 0.4725\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4675 - accuracy: 0.4638 - val_loss: 1.4185 - val_accuracy: 0.4925\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4656 - accuracy: 0.4550 - val_loss: 1.4254 - val_accuracy: 0.4800\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4485 - accuracy: 0.4606 - val_loss: 1.4022 - val_accuracy: 0.4850\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 418us/sample - loss: 1.6212 - accuracy: 0.4194 - val_loss: 1.5563 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 92us/sample - loss: 1.5646 - accuracy: 0.4331 - val_loss: 1.5762 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 109us/sample - loss: 1.5649 - accuracy: 0.4331 - val_loss: 1.5453 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 104us/sample - loss: 1.5591 - accuracy: 0.4331 - val_loss: 1.5389 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 79us/sample - loss: 1.5644 - accuracy: 0.4331 - val_loss: 1.5436 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5676 - accuracy: 0.4331 - val_loss: 1.5328 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5605 - accuracy: 0.4331 - val_loss: 1.5396 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5623 - accuracy: 0.4331 - val_loss: 1.5685 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5604 - accuracy: 0.4331 - val_loss: 1.5333 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 73us/sample - loss: 1.5586 - accuracy: 0.4331 - val_loss: 1.5337 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5597 - accuracy: 0.4331 - val_loss: 1.5315 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5528 - accuracy: 0.4331 - val_loss: 1.5312 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5576 - accuracy: 0.4331 - val_loss: 1.5347 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5584 - accuracy: 0.4331 - val_loss: 1.5477 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5528 - accuracy: 0.4331 - val_loss: 1.5329 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5515 - accuracy: 0.4331 - val_loss: 1.5415 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.5538 - accuracy: 0.4331 - val_loss: 1.5180 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5491 - accuracy: 0.4331 - val_loss: 1.5136 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5268 - accuracy: 0.4331 - val_loss: 1.5259 - val_accuracy: 0.4500\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5192 - accuracy: 0.4350 - val_loss: 1.4818 - val_accuracy: 0.4500\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4970 - accuracy: 0.4444 - val_loss: 1.4884 - val_accuracy: 0.4700\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 111us/sample - loss: 1.4961 - accuracy: 0.4512 - val_loss: 1.4639 - val_accuracy: 0.4775\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4746 - accuracy: 0.4606 - val_loss: 1.4599 - val_accuracy: 0.4800\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.4687 - accuracy: 0.4544 - val_loss: 1.4494 - val_accuracy: 0.4725\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4733 - accuracy: 0.4531 - val_loss: 1.4448 - val_accuracy: 0.4875\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4587 - accuracy: 0.4581 - val_loss: 1.4405 - val_accuracy: 0.4725\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4544 - accuracy: 0.4625 - val_loss: 1.4306 - val_accuracy: 0.4725\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4355 - accuracy: 0.4663 - val_loss: 1.4208 - val_accuracy: 0.4825\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 120us/sample - loss: 1.4285 - accuracy: 0.4694 - val_loss: 1.4011 - val_accuracy: 0.4875\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 120us/sample - loss: 1.4259 - accuracy: 0.4644 - val_loss: 1.3916 - val_accuracy: 0.4900\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 417us/sample - loss: 1.6177 - accuracy: 0.4263 - val_loss: 1.5563 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 115us/sample - loss: 1.5612 - accuracy: 0.4331 - val_loss: 1.5336 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 87us/sample - loss: 1.5596 - accuracy: 0.4331 - val_loss: 1.5399 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 82us/sample - loss: 1.5584 - accuracy: 0.4331 - val_loss: 1.5496 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 79us/sample - loss: 1.5678 - accuracy: 0.4331 - val_loss: 1.5380 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5619 - accuracy: 0.4331 - val_loss: 1.5342 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5670 - accuracy: 0.4331 - val_loss: 1.5428 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5536 - accuracy: 0.4331 - val_loss: 1.5382 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5555 - accuracy: 0.4331 - val_loss: 1.5288 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5508 - accuracy: 0.4331 - val_loss: 1.5330 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5539 - accuracy: 0.4331 - val_loss: 1.5276 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5493 - accuracy: 0.4331 - val_loss: 1.5269 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5465 - accuracy: 0.4331 - val_loss: 1.5086 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5311 - accuracy: 0.4331 - val_loss: 1.5054 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 97us/sample - loss: 1.5102 - accuracy: 0.4344 - val_loss: 1.4894 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5065 - accuracy: 0.4369 - val_loss: 1.4795 - val_accuracy: 0.4850\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4834 - accuracy: 0.4563 - val_loss: 1.4477 - val_accuracy: 0.4825\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4846 - accuracy: 0.4581 - val_loss: 1.4452 - val_accuracy: 0.4850\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 112us/sample - loss: 1.4582 - accuracy: 0.4625 - val_loss: 1.4429 - val_accuracy: 0.4825\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4560 - accuracy: 0.4575 - val_loss: 1.4193 - val_accuracy: 0.4850\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4459 - accuracy: 0.4644 - val_loss: 1.4288 - val_accuracy: 0.4900\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4320 - accuracy: 0.4700 - val_loss: 1.4025 - val_accuracy: 0.4875\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4236 - accuracy: 0.4700 - val_loss: 1.3972 - val_accuracy: 0.4850\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 143us/sample - loss: 1.4029 - accuracy: 0.4781 - val_loss: 1.3891 - val_accuracy: 0.4975\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 145us/sample - loss: 1.4092 - accuracy: 0.4744 - val_loss: 1.3720 - val_accuracy: 0.5025\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 1s 405us/sample - loss: 1.4006 - accuracy: 0.4694 - val_loss: 1.3462 - val_accuracy: 0.5075\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 1s 629us/sample - loss: 1.3685 - accuracy: 0.4894 - val_loss: 1.3190 - val_accuracy: 0.4975\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 2s 1ms/sample - loss: 1.3543 - accuracy: 0.4900 - val_loss: 1.3031 - val_accuracy: 0.5150\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 3s 2ms/sample - loss: 1.3476 - accuracy: 0.4831 - val_loss: 1.2885 - val_accuracy: 0.5175\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 3s 2ms/sample - loss: 1.3229 - accuracy: 0.4981 - val_loss: 1.2771 - val_accuracy: 0.5200\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1600/1600 [==============================] - 1s 417us/sample - loss: 1.6253 - accuracy: 0.4194 - val_loss: 1.5632 - val_accuracy: 0.4500\n",
      "Epoch 2/30\n",
      "1600/1600 [==============================] - 0s 121us/sample - loss: 1.5587 - accuracy: 0.4331 - val_loss: 1.5509 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "1600/1600 [==============================] - 0s 112us/sample - loss: 1.5663 - accuracy: 0.4331 - val_loss: 1.5369 - val_accuracy: 0.4500\n",
      "Epoch 4/30\n",
      "1600/1600 [==============================] - 0s 108us/sample - loss: 1.5671 - accuracy: 0.4331 - val_loss: 1.5339 - val_accuracy: 0.4500\n",
      "Epoch 5/30\n",
      "1600/1600 [==============================] - 0s 82us/sample - loss: 1.5566 - accuracy: 0.4331 - val_loss: 1.5498 - val_accuracy: 0.4500\n",
      "Epoch 6/30\n",
      "1600/1600 [==============================] - 0s 79us/sample - loss: 1.5604 - accuracy: 0.4331 - val_loss: 1.5374 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5604 - accuracy: 0.4331 - val_loss: 1.5362 - val_accuracy: 0.4500\n",
      "Epoch 8/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5658 - accuracy: 0.4331 - val_loss: 1.5424 - val_accuracy: 0.4500\n",
      "Epoch 9/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5608 - accuracy: 0.4331 - val_loss: 1.5370 - val_accuracy: 0.4500\n",
      "Epoch 10/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5544 - accuracy: 0.4331 - val_loss: 1.5360 - val_accuracy: 0.4500\n",
      "Epoch 11/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5554 - accuracy: 0.4331 - val_loss: 1.5389 - val_accuracy: 0.4500\n",
      "Epoch 12/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5451 - accuracy: 0.4331 - val_loss: 1.5263 - val_accuracy: 0.4500\n",
      "Epoch 13/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5546 - accuracy: 0.4331 - val_loss: 1.5423 - val_accuracy: 0.4500\n",
      "Epoch 14/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.5521 - accuracy: 0.4331 - val_loss: 1.5312 - val_accuracy: 0.4500\n",
      "Epoch 15/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5406 - accuracy: 0.4331 - val_loss: 1.5035 - val_accuracy: 0.4500\n",
      "Epoch 16/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.5443 - accuracy: 0.4331 - val_loss: 1.5284 - val_accuracy: 0.4500\n",
      "Epoch 17/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.5251 - accuracy: 0.4331 - val_loss: 1.4941 - val_accuracy: 0.4500\n",
      "Epoch 18/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.5158 - accuracy: 0.4331 - val_loss: 1.4711 - val_accuracy: 0.4500\n",
      "Epoch 19/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.4964 - accuracy: 0.4469 - val_loss: 1.4632 - val_accuracy: 0.4600\n",
      "Epoch 20/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4846 - accuracy: 0.4544 - val_loss: 1.4484 - val_accuracy: 0.4850\n",
      "Epoch 21/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4678 - accuracy: 0.4606 - val_loss: 1.4660 - val_accuracy: 0.4750\n",
      "Epoch 22/30\n",
      "1600/1600 [==============================] - 0s 74us/sample - loss: 1.4696 - accuracy: 0.4656 - val_loss: 1.4537 - val_accuracy: 0.4775\n",
      "Epoch 23/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4302 - accuracy: 0.4675 - val_loss: 1.4199 - val_accuracy: 0.4850\n",
      "Epoch 24/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.4472 - accuracy: 0.4688 - val_loss: 1.4220 - val_accuracy: 0.4875\n",
      "Epoch 25/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.4247 - accuracy: 0.4769 - val_loss: 1.4062 - val_accuracy: 0.4950\n",
      "Epoch 26/30\n",
      "1600/1600 [==============================] - 0s 100us/sample - loss: 1.4108 - accuracy: 0.4762 - val_loss: 1.3836 - val_accuracy: 0.4975\n",
      "Epoch 27/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.3986 - accuracy: 0.4800 - val_loss: 1.3514 - val_accuracy: 0.4925\n",
      "Epoch 28/30\n",
      "1600/1600 [==============================] - 0s 75us/sample - loss: 1.3777 - accuracy: 0.4794 - val_loss: 1.3566 - val_accuracy: 0.5025\n",
      "Epoch 29/30\n",
      "1600/1600 [==============================] - 0s 99us/sample - loss: 1.3695 - accuracy: 0.4844 - val_loss: 1.3305 - val_accuracy: 0.4925\n",
      "Epoch 30/30\n",
      "1600/1600 [==============================] - 0s 98us/sample - loss: 1.3600 - accuracy: 0.4844 - val_loss: 1.3161 - val_accuracy: 0.5125\n"
     ]
    }
   ],
   "source": [
    "# from experiment import run_generic_keras_experiment\n",
    "results_list = []\n",
    "for rep_no in range(reps):\n",
    "    train_idx_dict, test_idx_dict = subsample(meta_data, \"race\", \n",
    "                                              list(filter(lambda x: x != \"Other\", subgroups)), rep_no)\n",
    "\n",
    "    for tsize in train_sizes:\n",
    "        train_idx = []\n",
    "        valid_idx = []\n",
    "        test_idx = []\n",
    "\n",
    "        # adding up all subgroup indices\n",
    "        for group in filter(lambda x: x != \"Other\", subgroups):\n",
    "            train_idx += train_idx_dict[group][:int(tsize * (1 - validation_ratio))]\n",
    "            valid_idx += train_idx_dict[group][int(tsize * (1 - validation_ratio)):int(tsize)]\n",
    "            test_idx += test_idx_dict[group]\n",
    "\n",
    "        try:\n",
    "            # running the experiment\n",
    "            results = run_generic_keras_experiment(\n",
    "                binary_data = X,\n",
    "                meta_data= meta_data,\n",
    "                y_label='y', \n",
    "                z_label=\"race\",\n",
    "                z_values=list(filter(lambda x: x != \"Other\", subgroups)),\n",
    "                clf=clf,\n",
    "                validation=None,\n",
    "                train_valid_test_idx=(train_idx, valid_idx, test_idx),\n",
    "                clf_name=\"ff_keras\",\n",
    "                fit_args=fit_args,\n",
    "                num_batches=35,\n",
    "                synthetic_bins=10,\n",
    "                diagnose_calibration=False,\n",
    "                balanced=False,\n",
    "                y_range=[0, 1, 2, 3, 4, 5],\n",
    "                random_state=None,\n",
    "                save_best_only=True,\n",
    "                checkpoint=checkpoint_path,\n",
    "                compile_parameters=None)\n",
    "            \n",
    "        except:\n",
    "            results = pd.DataFrame([None])\n",
    "\n",
    "        # adding metadata\n",
    "        results = results.assign(tsize=tsize, rep_no=rep_no)\n",
    "\n",
    "        results_list.append(results)\n",
    "        \n",
    "    if rep_no % 3 == 0:\n",
    "        pd.to_pickle(results_list, \"results/subgroup_distro_jan27_temp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_pickle(results_list, \"results/subgroup_distro_jan27.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(train_idx) + len(valid_idx) == len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>vuln_method</th>\n",
       "      <th>batch_no</th>\n",
       "      <th>attacker</th>\n",
       "      <th>target_train_acc</th>\n",
       "      <th>target_test_acc</th>\n",
       "      <th>vuln</th>\n",
       "      <th>support</th>\n",
       "      <th>tsize</th>\n",
       "      <th>rep_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indian</td>\n",
       "      <td>counts</td>\n",
       "      <td>0</td>\n",
       "      <td>discriminating</td>\n",
       "      <td>0.431250</td>\n",
       "      <td>0.427781</td>\n",
       "      <td>False</td>\n",
       "      <td>1877</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indian</td>\n",
       "      <td>counts</td>\n",
       "      <td>0</td>\n",
       "      <td>discriminating</td>\n",
       "      <td>0.431250</td>\n",
       "      <td>0.427781</td>\n",
       "      <td>False</td>\n",
       "      <td>1877</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indian</td>\n",
       "      <td>counts</td>\n",
       "      <td>0</td>\n",
       "      <td>discriminating</td>\n",
       "      <td>0.431250</td>\n",
       "      <td>0.427781</td>\n",
       "      <td>False</td>\n",
       "      <td>1877</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Indian</td>\n",
       "      <td>counts</td>\n",
       "      <td>0</td>\n",
       "      <td>discriminating</td>\n",
       "      <td>0.431250</td>\n",
       "      <td>0.427781</td>\n",
       "      <td>False</td>\n",
       "      <td>1877</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indian</td>\n",
       "      <td>counts</td>\n",
       "      <td>0</td>\n",
       "      <td>discriminating</td>\n",
       "      <td>0.431250</td>\n",
       "      <td>0.427781</td>\n",
       "      <td>False</td>\n",
       "      <td>1877</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49435</th>\n",
       "      <td>Overall</td>\n",
       "      <td>counts</td>\n",
       "      <td>0</td>\n",
       "      <td>regular</td>\n",
       "      <td>0.545521</td>\n",
       "      <td>0.538294</td>\n",
       "      <td>True</td>\n",
       "      <td>12360</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49436</th>\n",
       "      <td>Overall</td>\n",
       "      <td>counts</td>\n",
       "      <td>0</td>\n",
       "      <td>regular</td>\n",
       "      <td>0.545521</td>\n",
       "      <td>0.538294</td>\n",
       "      <td>True</td>\n",
       "      <td>12360</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49437</th>\n",
       "      <td>Overall</td>\n",
       "      <td>counts</td>\n",
       "      <td>0</td>\n",
       "      <td>regular</td>\n",
       "      <td>0.545521</td>\n",
       "      <td>0.538294</td>\n",
       "      <td>True</td>\n",
       "      <td>12360</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49438</th>\n",
       "      <td>Overall</td>\n",
       "      <td>counts</td>\n",
       "      <td>0</td>\n",
       "      <td>regular</td>\n",
       "      <td>0.545521</td>\n",
       "      <td>0.538294</td>\n",
       "      <td>True</td>\n",
       "      <td>12360</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49439</th>\n",
       "      <td>Overall</td>\n",
       "      <td>counts</td>\n",
       "      <td>0</td>\n",
       "      <td>regular</td>\n",
       "      <td>0.545521</td>\n",
       "      <td>0.538294</td>\n",
       "      <td>True</td>\n",
       "      <td>12360</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468320 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subgroup vuln_method  batch_no        attacker  target_train_acc  \\\n",
       "0       Indian      counts         0  discriminating          0.431250   \n",
       "1       Indian      counts         0  discriminating          0.431250   \n",
       "2       Indian      counts         0  discriminating          0.431250   \n",
       "3       Indian      counts         0  discriminating          0.431250   \n",
       "4       Indian      counts         0  discriminating          0.431250   \n",
       "...        ...         ...       ...             ...               ...   \n",
       "49435  Overall      counts         0         regular          0.545521   \n",
       "49436  Overall      counts         0         regular          0.545521   \n",
       "49437  Overall      counts         0         regular          0.545521   \n",
       "49438  Overall      counts         0         regular          0.545521   \n",
       "49439  Overall      counts         0         regular          0.545521   \n",
       "\n",
       "       target_test_acc   vuln  support  tsize  rep_no  \n",
       "0             0.427781  False     1877    200       0  \n",
       "1             0.427781  False     1877    200       0  \n",
       "2             0.427781  False     1877    200       0  \n",
       "3             0.427781  False     1877    200       0  \n",
       "4             0.427781  False     1877    200       0  \n",
       "...                ...    ...      ...    ...     ...  \n",
       "49435         0.538294   True    12360   1717       1  \n",
       "49436         0.538294   True    12360   1717       1  \n",
       "49437         0.538294   True    12360   1717       1  \n",
       "49438         0.538294   True    12360   1717       1  \n",
       "49439         0.538294   True    12360   1717       1  \n",
       "\n",
       "[468320 rows x 10 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(results_list[-12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target_train_acc</th>\n",
       "      <th>overfitting_gap</th>\n",
       "      <th>vuln</th>\n",
       "      <th>max_vuln_disparity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attacker</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>discriminating</th>\n",
       "      <th>1717</th>\n",
       "      <td>0.554079</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>0.564159</td>\n",
       "      <td>0.019094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regular</th>\n",
       "      <th>1717</th>\n",
       "      <td>0.554079</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>0.556715</td>\n",
       "      <td>0.005825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      target_train_acc  overfitting_gap      vuln  \\\n",
       "attacker       model                                                \n",
       "discriminating 1717           0.554079         0.005302  0.564159   \n",
       "regular        1717           0.554079         0.005302  0.556715   \n",
       "\n",
       "                      max_vuln_disparity  \n",
       "attacker       model                      \n",
       "discriminating 1717             0.019094  \n",
       "regular        1717             0.005825  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_disparity(results.rename(dict(tsize=\"model\"), axis=1), renaming_dict=renaming_dict, single_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import max_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "_to_merge = []\n",
    "for results in results_list:\n",
    "    if len(results) == 1:\n",
    "        continue\n",
    "    agg_res = results.groupby([\"subgroup\", \"batch_no\", \"attacker\", \"tsize\"]).agg(\"mean\")\n",
    "    _to_merge.append(agg_res)\n",
    "\n",
    "aggregated_results = pd.concat(_to_merge).groupby([\"subgroup\", \"attacker\", \"tsize\", \"rep_no\"]).agg(\"mean\")\n",
    "# aggregated_results = pd.concat(_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">target_train_acc</th>\n",
       "      <th colspan=\"2\" halign=\"left\">target_test_acc</th>\n",
       "      <th colspan=\"2\" halign=\"left\">vuln</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subgroup</th>\n",
       "      <th>attacker</th>\n",
       "      <th>tsize</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">Asian</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">discriminating</th>\n",
       "      <th>200</th>\n",
       "      <td>0.425151</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.422572</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.914758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.466286</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.462888</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.811121</td>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.495931</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.754288</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.519998</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.682808</td>\n",
       "      <td>0.000771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.544457</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.532944</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.624530</td>\n",
       "      <td>0.001565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.551888</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.560703</td>\n",
       "      <td>0.003573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">regular</th>\n",
       "      <th>200</th>\n",
       "      <td>0.425151</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.422572</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.914758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.466286</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.462888</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.811040</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.495931</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.754125</td>\n",
       "      <td>0.000264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.519998</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.682275</td>\n",
       "      <td>0.000533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.544457</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.532944</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.623234</td>\n",
       "      <td>0.001004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.551888</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.556348</td>\n",
       "      <td>0.001834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">Black</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">discriminating</th>\n",
       "      <th>200</th>\n",
       "      <td>0.425151</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.422572</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.914758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.466286</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.462888</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.811202</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.495931</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.754564</td>\n",
       "      <td>0.000585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.519998</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.682910</td>\n",
       "      <td>0.000816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.544457</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.532944</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.624001</td>\n",
       "      <td>0.001241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.551888</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.560638</td>\n",
       "      <td>0.002737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">regular</th>\n",
       "      <th>200</th>\n",
       "      <td>0.425151</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.422572</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.914758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.466286</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.462888</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.811067</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.495931</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.754100</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.519998</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.682150</td>\n",
       "      <td>0.000117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.544457</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.532944</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.622882</td>\n",
       "      <td>0.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.551888</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.556126</td>\n",
       "      <td>0.001422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">Indian</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">discriminating</th>\n",
       "      <th>200</th>\n",
       "      <td>0.425151</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.422572</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.914758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.466286</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.462888</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.811215</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.495931</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.754426</td>\n",
       "      <td>0.000494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.519998</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.682910</td>\n",
       "      <td>0.000739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.544457</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.532944</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.624323</td>\n",
       "      <td>0.001194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.551888</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.560231</td>\n",
       "      <td>0.002427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">regular</th>\n",
       "      <th>200</th>\n",
       "      <td>0.425151</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.422572</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.914758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.466286</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.462888</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.811040</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.495931</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.754062</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.519998</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.682161</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.544457</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.532944</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.622799</td>\n",
       "      <td>0.000535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.551888</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.556172</td>\n",
       "      <td>0.001477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">Overall</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">discriminating</th>\n",
       "      <th>200</th>\n",
       "      <td>0.425151</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.422572</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.914758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.466286</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.462888</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.811188</td>\n",
       "      <td>0.000144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.495931</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.754433</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.519998</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.682891</td>\n",
       "      <td>0.000402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.544457</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.532944</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.624255</td>\n",
       "      <td>0.000648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.551888</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.560539</td>\n",
       "      <td>0.001466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">regular</th>\n",
       "      <th>200</th>\n",
       "      <td>0.425151</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.422572</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.914758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.466286</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.462888</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.811063</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.495931</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.754113</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.519998</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.682263</td>\n",
       "      <td>0.000162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.544457</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.532944</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.623040</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.551888</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.556385</td>\n",
       "      <td>0.000718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">White</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">discriminating</th>\n",
       "      <th>200</th>\n",
       "      <td>0.425151</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.422572</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.914758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.466286</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.462888</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.811215</td>\n",
       "      <td>0.000379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.495931</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.754451</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.519998</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.682933</td>\n",
       "      <td>0.000944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.544457</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.532944</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.001169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.551888</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.560583</td>\n",
       "      <td>0.002386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">regular</th>\n",
       "      <th>200</th>\n",
       "      <td>0.425151</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.422572</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.914758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.466286</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.462888</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.811107</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.495931</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.754163</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.519998</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.682468</td>\n",
       "      <td>0.000520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0.544457</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.532944</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.623245</td>\n",
       "      <td>0.000718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.551888</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.556893</td>\n",
       "      <td>0.002069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              target_train_acc           target_test_acc  \\\n",
       "                                          mean       std            mean   \n",
       "subgroup attacker       tsize                                              \n",
       "Asian    discriminating 200           0.425151  0.015146        0.422572   \n",
       "                        500           0.466286  0.022476        0.462888   \n",
       "                        700           0.501875  0.017015        0.495931   \n",
       "                        1000          0.529000  0.012686        0.519998   \n",
       "                        1300          0.544457  0.012353        0.532944   \n",
       "                        1717          0.551888  0.010275        0.541197   \n",
       "         regular        200           0.425151  0.015146        0.422572   \n",
       "                        500           0.466286  0.022476        0.462888   \n",
       "                        700           0.501875  0.017015        0.495931   \n",
       "                        1000          0.529000  0.012686        0.519998   \n",
       "                        1300          0.544457  0.012353        0.532944   \n",
       "                        1717          0.551888  0.010275        0.541197   \n",
       "Black    discriminating 200           0.425151  0.015146        0.422572   \n",
       "                        500           0.466286  0.022476        0.462888   \n",
       "                        700           0.501875  0.017015        0.495931   \n",
       "                        1000          0.529000  0.012686        0.519998   \n",
       "                        1300          0.544457  0.012353        0.532944   \n",
       "                        1717          0.551888  0.010275        0.541197   \n",
       "         regular        200           0.425151  0.015146        0.422572   \n",
       "                        500           0.466286  0.022476        0.462888   \n",
       "                        700           0.501875  0.017015        0.495931   \n",
       "                        1000          0.529000  0.012686        0.519998   \n",
       "                        1300          0.544457  0.012353        0.532944   \n",
       "                        1717          0.551888  0.010275        0.541197   \n",
       "Indian   discriminating 200           0.425151  0.015146        0.422572   \n",
       "                        500           0.466286  0.022476        0.462888   \n",
       "                        700           0.501875  0.017015        0.495931   \n",
       "                        1000          0.529000  0.012686        0.519998   \n",
       "                        1300          0.544457  0.012353        0.532944   \n",
       "                        1717          0.551888  0.010275        0.541197   \n",
       "         regular        200           0.425151  0.015146        0.422572   \n",
       "                        500           0.466286  0.022476        0.462888   \n",
       "                        700           0.501875  0.017015        0.495931   \n",
       "                        1000          0.529000  0.012686        0.519998   \n",
       "                        1300          0.544457  0.012353        0.532944   \n",
       "                        1717          0.551888  0.010275        0.541197   \n",
       "Overall  discriminating 200           0.425151  0.015146        0.422572   \n",
       "                        500           0.466286  0.022476        0.462888   \n",
       "                        700           0.501875  0.017015        0.495931   \n",
       "                        1000          0.529000  0.012686        0.519998   \n",
       "                        1300          0.544457  0.012353        0.532944   \n",
       "                        1717          0.551888  0.010275        0.541197   \n",
       "         regular        200           0.425151  0.015146        0.422572   \n",
       "                        500           0.466286  0.022476        0.462888   \n",
       "                        700           0.501875  0.017015        0.495931   \n",
       "                        1000          0.529000  0.012686        0.519998   \n",
       "                        1300          0.544457  0.012353        0.532944   \n",
       "                        1717          0.551888  0.010275        0.541197   \n",
       "White    discriminating 200           0.425151  0.015146        0.422572   \n",
       "                        500           0.466286  0.022476        0.462888   \n",
       "                        700           0.501875  0.017015        0.495931   \n",
       "                        1000          0.529000  0.012686        0.519998   \n",
       "                        1300          0.544457  0.012353        0.532944   \n",
       "                        1717          0.551888  0.010275        0.541197   \n",
       "         regular        200           0.425151  0.015146        0.422572   \n",
       "                        500           0.466286  0.022476        0.462888   \n",
       "                        700           0.501875  0.017015        0.495931   \n",
       "                        1000          0.529000  0.012686        0.519998   \n",
       "                        1300          0.544457  0.012353        0.532944   \n",
       "                        1717          0.551888  0.010275        0.541197   \n",
       "\n",
       "                                             vuln            \n",
       "                                    std      mean       std  \n",
       "subgroup attacker       tsize                                \n",
       "Asian    discriminating 200    0.004626  0.914758  0.000000  \n",
       "                        500    0.019554  0.811121  0.000233  \n",
       "                        700    0.011131  0.754288  0.000308  \n",
       "                        1000   0.009486  0.682808  0.000771  \n",
       "                        1300   0.007817  0.624530  0.001565  \n",
       "                        1717   0.008984  0.560703  0.003573  \n",
       "         regular        200    0.004626  0.914758  0.000000  \n",
       "                        500    0.019554  0.811040  0.000080  \n",
       "                        700    0.011131  0.754125  0.000264  \n",
       "                        1000   0.009486  0.682275  0.000533  \n",
       "                        1300   0.007817  0.623234  0.001004  \n",
       "                        1717   0.008984  0.556348  0.001834  \n",
       "Black    discriminating 200    0.004626  0.914758  0.000000  \n",
       "                        500    0.019554  0.811202  0.000298  \n",
       "                        700    0.011131  0.754564  0.000585  \n",
       "                        1000   0.009486  0.682910  0.000816  \n",
       "                        1300   0.007817  0.624001  0.001241  \n",
       "                        1717   0.008984  0.560638  0.002737  \n",
       "         regular        200    0.004626  0.914758  0.000000  \n",
       "                        500    0.019554  0.811067  0.000080  \n",
       "                        700    0.011131  0.754100  0.000164  \n",
       "                        1000   0.009486  0.682150  0.000117  \n",
       "                        1300   0.007817  0.622882  0.000502  \n",
       "                        1717   0.008984  0.556126  0.001422  \n",
       "Indian   discriminating 200    0.004626  0.914758  0.000000  \n",
       "                        500    0.019554  0.811215  0.000323  \n",
       "                        700    0.011131  0.754426  0.000494  \n",
       "                        1000   0.009486  0.682910  0.000739  \n",
       "                        1300   0.007817  0.624323  0.001194  \n",
       "                        1717   0.008984  0.560231  0.002427  \n",
       "         regular        200    0.004626  0.914758  0.000000  \n",
       "                        500    0.019554  0.811040  0.000080  \n",
       "                        700    0.011131  0.754062  0.000107  \n",
       "                        1000   0.009486  0.682161  0.000215  \n",
       "                        1300   0.007817  0.622799  0.000535  \n",
       "                        1717   0.008984  0.556172  0.001477  \n",
       "Overall  discriminating 200    0.004626  0.914758  0.000000  \n",
       "                        500    0.019554  0.811188  0.000144  \n",
       "                        700    0.011131  0.754433  0.000210  \n",
       "                        1000   0.009486  0.682891  0.000402  \n",
       "                        1300   0.007817  0.624255  0.000648  \n",
       "                        1717   0.008984  0.560539  0.001466  \n",
       "         regular        200    0.004626  0.914758  0.000000  \n",
       "                        500    0.019554  0.811063  0.000072  \n",
       "                        700    0.011131  0.754113  0.000104  \n",
       "                        1000   0.009486  0.682263  0.000162  \n",
       "                        1300   0.007817  0.623040  0.000292  \n",
       "                        1717   0.008984  0.556385  0.000718  \n",
       "White    discriminating 200    0.004626  0.914758  0.000000  \n",
       "                        500    0.019554  0.811215  0.000379  \n",
       "                        700    0.011131  0.754451  0.000449  \n",
       "                        1000   0.009486  0.682933  0.000944  \n",
       "                        1300   0.007817  0.624167  0.001169  \n",
       "                        1717   0.008984  0.560583  0.002386  \n",
       "         regular        200    0.004626  0.914758  0.000000  \n",
       "                        500    0.019554  0.811107  0.000298  \n",
       "                        700    0.011131  0.754163  0.000303  \n",
       "                        1000   0.009486  0.682468  0.000520  \n",
       "                        1300   0.007817  0.623245  0.000718  \n",
       "                        1717   0.008984  0.556893  0.002069  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_results.drop([\"support\"], axis=1).groupby([\"subgroup\", \"attacker\", \"tsize\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fda1e0b2208>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "sns.pointplot(data=aggregated_results.reset_index().query(\"attacker == 'discriminating'\"),\n",
    "                                                          x=\"tsize\", y=\"vuln\", hue=\"subgroup\", ax=ax)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36] *",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
