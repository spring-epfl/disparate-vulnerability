from collections import OrderedDict
from functools import reduce
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from helpers import max_disparity, compute_p_matrices
from plot_utils import plot_stat_heatmaps
from IPython.core.display import display

# model_order_paper = ["LR", "EO LR", "DP LR", "6-NN", "100-NN", "500-NN"]
model_order_paper = ["lr", "fair_lr_eo", "lr_ibm_eps1_dp", "nn_6", "nn_100", "nn_500"]

renaming_dict = OrderedDict(lr="Logistic Regression (LR)",
                            fair_lr_dmpr="Fair LR (Demorgraphic Parity)",
                            fair_lr_eo="Fair LR (Equalized Odds)",
                            nn_6="6-Neuron NN",
                            nn_50="50-Neuron NN",
                            nn_500="500-Neuron NN",
                            nn_100="100-Neuron NN",
                            regular="Regular",
                            discriminating="Discriminating",
                            lr_ibm_eps1_dp="Differentially Private LR (eps=1)",
                            lr_amp_dp="Differentially Private LR (AMP)",
                            lr_psgd_dp="Differentially Private LR (PSGD)",
                            **{"Native American": "Native\nAmerican",
                               "African-American": "African-\nAmerican",
                               "lr_ibm_eps0.1_dp": "Differentially Private LR (eps=0.1)"},
                            )

paper_models = dict(
    lr="LR",
    fair_lr_eo="EO LR",
    lr_ibm_eps1_dp="DP LR",
    nn_6="6-NN",
    nn_100="100-NN",
    nn_500="500-NN")

renaming_dict2 = dict(attacker="Attacker",
                      model="Model",
                      target_train_acc="Train Acc.",
                      overfitting_gap="Overfitting",
                      vuln="Vulnerability",
                      max_vuln_disparity="Max Vuln. Disparity")

model_order = [renaming_dict[model] for model in
               ["lr", "fair_lr_dmpr", "fair_lr_eo", "lr_ibm_eps1_dp", "nn_6", "nn_100", "lr_amp_dp", "lr_psgd_dp"]]


def paper_naming(_input):
    paper_models = dict(
        lr="LR",
        fair_lr_eo="EO LR",
        lr_ibm_eps1_dp="DP LR",
        nn_6="6-NN",
        nn_100="100-NN",
        nn_500="500-NN")

    return {k: v if k not in paper_models.keys() else paper_models[k] for k, v in _input.items()}


def diagnostics(metrics, max_disparity_show=True, latex=False, p_plots=False, plot_p_plots=False, ordered=True, num_shuffles=35):
    def is_max_min(attacker, metric, value):
        if max_values_per_attacker_per_measure[attacker][metric] <= value:
            return "\darkred{{\textbf{{{x:~>5.2f}}}}}".format(x=value)
        elif min_values_per_attacker_per_measure[attacker][metric] >= value:
            return "\darkgreen{{\textbf{{{x:~>5.2f}}}}}".format(x=value)
        else:
            return "{x:~>5.2f}".format(x=value)

    def formatter(x):
        metric = x.name
        if latex:
            if metric not in ['target_train_acc', 'overfitting_gap']:
                    return "{x0},~~{x1}".format(x0=is_max_min(0, metric, x.iloc[1]), x1=is_max_min(1, metric, x.iloc[0]))
            else:
                if metric != 'target_train_acc':
                    return is_max_min(0, metric, x.iloc[0])
                else:
                    return "{x:~>5.2f}".format(x=x.iloc[0])
        else:
            return "{x0}, {x1}".format(x0=x.iloc[1], x1=x.iloc[0])

    fig, ax = plt.subplots()

    max_disparity_df = max_disparity(metrics, renaming_dict=renaming_dict, ax=ax)

    if max_disparity_show:
        display(max_disparity_df)

    max_values_per_attacker_per_measure = reduce(lambda x1, x2: {**x1, **x2},
                                                 (max_disparity_df * 100).round(2).groupby("attacker").apply(
                                                     lambda x: {int(x.name == "discriminating"): x.max(
                                                         axis=0).to_dict()}).tolist())
    min_values_per_attacker_per_measure = reduce(lambda x1, x2: {**x1, **x2},
                                                 (max_disparity_df * 100).round(2).groupby("attacker").apply(
                                                     lambda x: {int(x.name == "discriminating"): x.min(
                                                         axis=0).to_dict()}).tolist())

    aggregators = {met: lambda x: formatter(x) for met in
                   ['target_train_acc', 'overfitting_gap', 'vuln', 'max_vuln_disparity']}


    paper_table = ((max_disparity_df * 100).round(2)
                   .groupby("model").agg(aggregators)
                   )
    if ordered:
        paper_table = paper_table.loc[model_order_paper]

    # Rename table items
    paper_table = (paper_table.reset_index()
                   .replace(paper_models)
                   .rename(renaming_dict2, axis=1)
                   )

    if latex:
        with pd.option_context('display.max_colwidth', 1000):
            print(paper_table.to_latex(escape=False, index=False))
    else:
        display(paper_table)

    discriminating_p_matrices_by_model = None
    regular_p_matrices_by_model = None

    if p_plots:

        subgroups = list(filter(lambda x: x != "Overall", metrics[list(metrics.keys())[0]].subgroup.unique()))
        discriminating_p_matrices_by_model = compute_p_matrices(
            metrics, attacker="discriminating", subgroups=subgroups,
            num_reps=num_shuffles)

        # regular_p_matrices_by_model = compute_p_matrices(
        #     metrics, attacker="regular", subgroups=subgroups,
        #     num_reps=num_shuffles)

        if plot_p_plots:
            fig, axes = plt.subplots(1, 1, figsize=(5, 5), sharex=True, sharey=True)
            cmap = sns.color_palette(["#ECF0F1", "#E74C3C"])
            for ax_id, model_name in enumerate(["ff_keras"]):
                # ax = axes[0][ax_id]
                ax = axes
                # ax.set_title(renaming_dict[model_name])
                metrics = metrics[model_name]
                plot_stat_heatmaps(metrics,
                                   discriminating_p_matrices_by_model[model_name],
                                   subgroups=subgroups,
                                   alpha=0.005, ax=ax,
                                   xticklabels=True,
                                   mode="meta", cmap=cmap)

                # for ax_id, model_name in enumerate(["nn_6", "nn_100", "nn_500"]):
                #     ax = axes[1][ax_id]
                #     metrics = metrics[model_name]
                #     ax.set_title(renaming_dict[model_name])
                #     plot_stat_heatmaps(metrics,
                #                        discriminating_p_matrices_by_model[model_name],
                #                        subgroups=subgroups,
                #                        alpha=0.005, ax=ax,
                #                        xticklabels=True,
                #                        mode="meta", cmap=cmap)
                #     fig.set_tight_layout(tight=True)

            plt.show()

    return paper_table, regular_p_matrices_by_model, discriminating_p_matrices_by_model
