import collections
from experiment import *
# from tqdm.notebook import tqdm

from functools import partial
from tqdm import tqdm as std_tqdm

tqdm = partial(std_tqdm, dynamic_ncols=True)

import pandas as pd
import numpy as np
import tensorflow as tf
from utkface import prepare_utkface


def subsample(meta_data, z_label, subgroups, random_state=None):
    _data = meta_data.loc[meta_data[z_label].isin(subgroups)]
    counts = _data.race.value_counts()
    smallest_subgroup_size = counts.min()
    state = np.random.RandomState(random_state) if random_state is not None else np.random.RandomState()

    train_idx_dict = {}
    test_idx_dict = {}
    for group in subgroups:
        group_idx = state.choice(
            _data.query(f"{z_label} == '{group}'").index, size=int(smallest_subgroup_size)).tolist()

        train_idx_dict[group] = group_idx[:int(smallest_subgroup_size / 2)]
        test_idx_dict[group] = group_idx[int(smallest_subgroup_size / 2):]

    return train_idx_dict, test_idx_dict


def prepare_model():
    clf = tf.keras.Sequential()

    clf.add(
        tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(32, 32, 3)))
    clf.add(tf.keras.layers.MaxPooling2D(pool_size=2))
    clf.add(tf.keras.layers.Dropout(0.3))

    clf.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))
    clf.add(tf.keras.layers.MaxPooling2D(pool_size=2))
    clf.add(tf.keras.layers.Dropout(0.3))

    clf.add(tf.keras.layers.Flatten())
    clf.add(tf.keras.layers.Dense(256, activation='relu'))
    clf.add(tf.keras.layers.Dropout(0.5))
    clf.add(tf.keras.layers.Dense(6, activation='softmax'))

    return clf


def experiment_setting(debug_):
    validation_ratio_ = 0.2
    if not debug_:
        #     train_sizes = [200, 300, 400, 500, 600, 700, 800, 900, 1000, 1200, 1500, 1717]
        train_sizes_ = [250, 500, 750, 1000]
        reps_ = 5
        num_batches_ = 35
    else:
        train_sizes_ = [250]
        reps_ = 1
        num_batches_ = 2

    return validation_ratio_, train_sizes_, reps_, num_batches_


if __name__ == "__main__":

    # parse input
    experiment_label = sys.argv[1]
    try:
        debug = sys.argv[2] == "True"
    except IndexError:
        debug = False

    try:
        start_rep = int(sys.argv[3])
    except IndexError:
        start_rep = 0

    print("Debug: ", debug)
    print("Start Rep: ", start_rep)

    # load data
    X, y, meta_data = prepare_utkface("data/UTKFace/", "data/UTKFace.npz")

    # subsample data
    subgroups = meta_data.race.unique()
    train_idx_dict, test_idx_dict = subsample(meta_data, "race", list(filter(lambda x: x != "Other", subgroups)), 1)

    # prepare keras classifier
    clf = prepare_model()
    checkpoint_path = "models/downsampling_model_2.h5"
    fit_args = dict(batch_size=64, epochs=30)

    # set experiment constants
    filtered_subgroups = list(filter(lambda x: x != "Other", subgroups))
    validation_ratio, train_sizes, reps, num_batches = experiment_setting(debug)

    # Main Loop
    rep_d = tqdm(range(start_rep, reps))
    for rep_no in rep_d:
        rep_d.set_description(f"Rep {rep_no}")

        train_idx_dict, test_idx_dict = subsample(meta_data, "race",
                                                  filtered_subgroups, rep_no)

        tsize_d = tqdm(train_sizes)
        for tsize in tsize_d:
            tsize_d.set_description(f"{tsize}")

            group_d = tqdm(filtered_subgroups)
            for group in group_d:
                group_d.set_description(f"{group}")

                train_idx = []
                valid_idx = []
                test_idx = []

                # adding selected subgroup up to tsize samples
                train_idx += train_idx_dict[group][:int(tsize * (1 - validation_ratio))]
                valid_idx += train_idx_dict[group][int(tsize * (1 - validation_ratio)):int(tsize)]

                # take the |train| + |valid| (= tsize) number of samples from test set
                test_idx += test_idx_dict[group][:tsize]

                for not_group in filtered_subgroups:
                    if not_group != group:
                        # adding other subgroups "completely"
                        validation_size = int(len(train_idx_dict[not_group]) * validation_ratio)
                        valid_idx += train_idx_dict[not_group][:validation_size]
                        train_idx += train_idx_dict[not_group][validation_size:]
                        test_idx += test_idx_dict[not_group]

                try:
                    # running the experiment
                    results = run_generic_keras_experiment(
                        binary_data=X,
                        meta_data=meta_data,
                        y_label='y',
                        z_label="race",
                        z_values=filtered_subgroups,
                        clf=clf,
                        validation=None,
                        train_valid_test_idx=(train_idx, valid_idx, test_idx),
                        clf_name="ff_keras",
                        fit_args=fit_args,
                        num_batches=num_batches,
                        synthetic_bins=10,
                        diagnose_calibration=False,
                        balanced=False,
                        y_range=[0, 1, 2, 3, 4, 5],
                        random_state=None,
                        save_best_only=True,
                        checkpoint=checkpoint_path,
                        compile_parameters=None,
                        output_extended_subgroup_metrics=False)
                except:
                    results = pd.DataFrame([None])

                # adding metadata
                results = results.assign(tsize=tsize, rep_no=rep_no)

                # metrics[(group, tsize)].append(results)
                pd.to_pickle(results, f"results/sample_size_{experiment_label}_{group}_{tsize}_{rep_no}.pkl")
